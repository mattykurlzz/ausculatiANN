{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, normalize\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter, butter, lfilter, ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian, triang, hamming\n",
    "\n",
    "import librosa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\AudioMNIST\\data\\01\\2_01_0.wav\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join('.', 'AudioMNIST', 'data', '01', str(random.randint(0, 9)) + '_01_' + '0' + '.wav')\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь попробуем удалить один канал и прослушать его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0.6249791666666666 48000 29999\n"
     ]
    }
   ],
   "source": [
    "req_len = 0.6\n",
    "\n",
    "data, rate = librosa.load(filename, sr=None, mono=True)\n",
    "\n",
    "if(len(data) > 30_000):\n",
    "  data = data[(len(data) - 30_000) // 2:30_000 + (len(data) - 30_000) // 2]\n",
    "elif (len(data) < 30_000):\n",
    "  data = np.pad(data, ((30_000 - len(data)) // 2))\n",
    "data_filtered = data\n",
    "\n",
    "print(data)\n",
    "print(len(data)/rate, rate, len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 2.08333333e-05 4.16666667e-05 ... 6.24916667e-01\n",
      " 6.24937500e-01 6.24958333e-01] 0.6249791666666666\n"
     ]
    }
   ],
   "source": [
    "t = np.arange (0, len(data)/rate, 1/rate)\n",
    "print(t, len(t)/rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Иван\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:7720: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\AudioMNIST\\data\\01\\2_01_0.wav\n"
     ]
    }
   ],
   "source": [
    "fig, (ax0, ax4, ax5) = plt.subplots(nrows=3, sharex=False)\n",
    "\n",
    "ax0.plot(t, data_filtered)\n",
    "\n",
    "ax4.specgram(data_filtered, Fs=rate, cmap='magma')\n",
    "\n",
    "Pxx = librosa.stft(data, dtype=np.float32, n_fft=1024)\n",
    "Pxx, _ = librosa.magphase(Pxx)\n",
    "Pxx = librosa.feature.melspectrogram(S=Pxx, sr=rate, n_fft=1024)\n",
    "Pxx = librosa.amplitude_to_db(Pxx, ref=np.min)\n",
    "im1 = ax5.imshow(Pxx, origin='lower', cmap='magma', aspect='auto')\n",
    "\n",
    "ax5.axis()\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(20)\n",
    "# ax3.axis(ymin=0, ymax=750)\n",
    "# ax4.axis(ymin=0, ymax=750)\n",
    "# ax5.axis(ymin=0, ymax=750)\n",
    "plt.show()\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.134895\n",
      "(128, 118)\n"
     ]
    }
   ],
   "source": [
    "print(Pxx.max())\n",
    "print(Pxx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 118)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, sharex=True)\n",
    "\n",
    "scalerR = RobustScaler()\n",
    "\n",
    "# Pxx, freqs, bins, im = ax0.specgram(data_filtered, Fs=rate, scale='dB', pad_to=255, mode='psd', scale_by_freq=True, detrend='mean')\n",
    "map1 = ax0.imshow(Pxx, origin='lower', aspect='auto', cmap='magma')\n",
    "\n",
    "Pxx2 = normalize(Pxx, axis=0, norm='l1')\n",
    "map2 = ax1.imshow(Pxx2, origin='lower', aspect='auto', cmap='magma')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "Pxx3 = scaler.fit_transform(Pxx)\n",
    "map3 = ax2.imshow(Pxx3, origin='lower', aspect='auto', cmap='magma')\n",
    "\n",
    "# fig.set_figwidth(80)\n",
    "# fig.set_figheight(20)\n",
    "# ax0.axis(ymin=0, ymax=1)\n",
    "# ax1.axis(ymin=0, ymax=1)\n",
    "fig.set_figheight(20)\n",
    "print(Pxx.shape)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] 128\n",
      "(128, 118)\n"
     ]
    }
   ],
   "source": [
    "Pxx = np.array(Pxx)\n",
    "print(Pxx3, len(Pxx3)) # элемент массива - амплитуды частот\n",
    "print(Pxx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно создавать такую спектрограмму для любого файла и паковать ее в трехмерный массив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinglePatientData:\n",
    "  def __init__(self, name : str, out : int, subfolder : str):\n",
    "    self.contents = name\n",
    "    self.num = out\n",
    "    self.subfolder = subfolder\n",
    "\n",
    "  def getTensor(self, path): # returns 129x1249x5 torch tensor\n",
    "    if (hasattr(self, 'specgrams')):\n",
    "      return self.specgrams\n",
    "    else:\n",
    "      data, rate = librosa.load(os.path.join(path, self.subfolder, self.contents + '.wav'), sr=None, mono=True)\n",
    "\n",
    "      if(len(data) > 30_000):\n",
    "        data = data[(len(data) - 30_000) // 2:30_000 + (len(data) - 30_000) // 2]\n",
    "      elif (len(data) < 30_000):\n",
    "        data = np.pad(data, ((30_000 - len(data)) // 2))\n",
    "\n",
    "      Pxx = librosa.stft(data, dtype=np.float32, n_fft=1024)\n",
    "      Pxx, _ = librosa.magphase(Pxx)\n",
    "      Pxx = librosa.feature.melspectrogram(S=Pxx, sr=rate, n_fft=1024)\n",
    "      Pxx = librosa.amplitude_to_db(Pxx, ref=np.min)\n",
    "\n",
    "      self.specgrams = torch.from_numpy(np.array([Pxx], dtype='float32'))\n",
    "      return self.specgrams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstr(num):\n",
    "  if(num < 10):\n",
    "    return '0' + str(num)\n",
    "  else:\n",
    "    return str(num)\n",
    "\n",
    "def createDataArray():\n",
    "  retArray = []\n",
    "  for m in range(1, 61):\n",
    "    for i in range(10):\n",
    "      for j in range(50):\n",
    "        retArray.append(SinglePatientData(str(i) + '_' + getstr(m) +'_' + str(j), i, getstr(m)))\n",
    "  return retArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = os.path.join('.', 'AudioMNIST', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_01_0 01\n"
     ]
    }
   ],
   "source": [
    "dataArray = createDataArray()\n",
    "\n",
    "print(dataArray[0].contents, dataArray[0].subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 118])\n",
      "5_15_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Иван\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:371: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., :off_start] = fft.rfft(fft_window * y_frames_pre, axis=-2)\n",
      "c:\\Users\\Иван\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:375: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., -off_end:] = fft.rfft(fft_window * y_frames_post, axis=-2)\n",
      "c:\\Users\\Иван\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:387: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataIterator = iter(dataArray)\n",
    "\n",
    "fig, (ax0) = plt.subplots(nrows=1, sharex=True)\n",
    "rnd = random.choice(dataArray)\n",
    "print(rnd.getTensor(foldername)[0].shape)\n",
    "map3 = ax0.imshow(rnd.getTensor(foldername)[0], origin='lower', aspect='auto', cmap='magma')\n",
    "print(rnd.contents)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно приступать к созданию первой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvModel, self).__init__()\n",
    "\n",
    "        conv_layers = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(2,2), stride=(1,1), padding=(2,2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1]\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(2,2), stride=(1,1), padding=(2,2))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2]\n",
    "\n",
    "        # self.conv3 = nn.Conv2d(16, 32, kernel_size=(2,2), stride=(1,1), padding=(1,1))\n",
    "        # self.relu3 = nn.ReLU()\n",
    "        # torch.nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        # self.conv3.bias.data.zero_()\n",
    "        # conv_layers += [self.conv3, self.relu3]\n",
    "\n",
    "        # self.conv4 = nn.Conv2d(32, 64, kernel_size=(2,2), stride=(1,1), padding=(1,1))\n",
    "        # self.relu4 = nn.ReLU()\n",
    "        # torch.nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        # self.conv4.bias.data.zero_()\n",
    "        # conv_layers += [self.conv4, self.relu4]\n",
    "\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        conv_layers += [self.conv5, self.relu5]\n",
    "\n",
    "        self.conv6 = nn.Conv2d(32, 64, kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv6.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        conv_layers += [self.conv6, self.relu6]\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 128, kernel_size=(3,3), stride=(2,2), padding=(1,1))\n",
    "        self.relu7 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv7.weight, a=0.1)\n",
    "        self.conv7.bias.data.zero_()\n",
    "        conv_layers += [self.conv7, self.relu7]\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, kernel_size=(5,5), stride=(4,4), padding=(1,1))\n",
    "        self.relu8 = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv8.weight, a=0.1)\n",
    "        self.conv8.bias.data.zero_()\n",
    "        conv_layers += [self.conv8, self.relu8]\n",
    "\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "        self.lin1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.lin2 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.lin3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.lin = nn.Sequential(*[self.lin1, self.dropout1, self.lin2, self.lin3])\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0])\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовый прогон:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 118])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel()\n",
    "Pxx = next(dataIterator).getTensor(foldername)\n",
    "print(Pxx.shape)\n",
    "out = model(Pxx)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvModel()\n",
    "model = model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(logit, target):\n",
    "    ''' Check if answer is right for training round '''\n",
    "    acc = 0\n",
    "    for i in range(len(logit)):\n",
    "        acc += (1 - abs(logit[i] - target[i])).item()\n",
    "    return acc / len(target)\n",
    "\n",
    "def get_acc_2(logit, target):\n",
    "    ''' Check if answer is right for training round '''\n",
    "    target_2 = target + 1\n",
    "    logit_2 = logit + 1\n",
    "    return ((target_2 - abs(logit_2 - target_2)) / target_2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 4])\n",
      "tensor([[0.1931, 0.1698, 0.1746, 0.2996, 0.1629],\n",
      "        [0.4276, 0.4823, 0.0040, 0.0461, 0.0399],\n",
      "        [0.1158, 0.1799, 0.1041, 0.5606, 0.0396]])\n"
     ]
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.5082 | Train Accuracy: 0.57\n",
      "Epoch: 1 | Loss: 0.9557 | Train Accuracy: 0.66\n",
      "Epoch: 2 | Loss: 0.8162 | Train Accuracy: 0.68\n",
      "Epoch: 3 | Loss: 0.7437 | Train Accuracy: 0.68\n",
      "Epoch: 4 | Loss: 0.6848 | Train Accuracy: 0.69\n",
      "Epoch: 5 | Loss: 0.6513 | Train Accuracy: 0.70\n",
      "Epoch: 6 | Loss: 0.6215 | Train Accuracy: 0.70\n",
      "Epoch: 7 | Loss: 0.5942 | Train Accuracy: 0.70\n",
      "Epoch: 8 | Loss: 0.5751 | Train Accuracy: 0.70\n",
      "Epoch: 9 | Loss: 0.5594 | Train Accuracy: 0.70\n",
      "Epoch: 10 | Loss: 0.5442 | Train Accuracy: 0.71\n",
      "Epoch: 11 | Loss: 0.5284 | Train Accuracy: 0.71\n",
      "Epoch: 12 | Loss: 0.5167 | Train Accuracy: 0.71\n",
      "Epoch: 13 | Loss: 0.5004 | Train Accuracy: 0.71\n",
      "Epoch: 14 | Loss: 0.4934 | Train Accuracy: 0.71\n",
      "Epoch: 15 | Loss: 0.4866 | Train Accuracy: 0.71\n",
      "Epoch: 16 | Loss: 0.4778 | Train Accuracy: 0.71\n",
      "Epoch: 17 | Loss: 0.4660 | Train Accuracy: 0.71\n",
      "Epoch: 18 | Loss: 0.4645 | Train Accuracy: 0.71\n",
      "Epoch: 19 | Loss: 0.4576 | Train Accuracy: 0.71\n",
      "Epoch: 20 | Loss: 0.4472 | Train Accuracy: 0.71\n",
      "Epoch: 21 | Loss: 0.4424 | Train Accuracy: 0.71\n",
      "Epoch: 22 | Loss: 0.4332 | Train Accuracy: 0.71\n",
      "Epoch: 23 | Loss: 0.4338 | Train Accuracy: 0.71\n",
      "Epoch: 24 | Loss: 0.4222 | Train Accuracy: 0.71\n",
      "Epoch: 25 | Loss: 0.4202 | Train Accuracy: 0.71\n",
      "Epoch: 26 | Loss: 0.4160 | Train Accuracy: 0.71\n",
      "Epoch: 27 | Loss: 0.4088 | Train Accuracy: 0.71\n",
      "Epoch: 28 | Loss: 0.4098 | Train Accuracy: 0.71\n",
      "Epoch: 29 | Loss: 0.4038 | Train Accuracy: 0.71\n",
      "Epoch: 30 | Loss: 0.4015 | Train Accuracy: 0.71\n",
      "Epoch: 31 | Loss: 0.3946 | Train Accuracy: 0.71\n",
      "Epoch: 32 | Loss: 0.3926 | Train Accuracy: 0.71\n",
      "Epoch: 33 | Loss: 0.3934 | Train Accuracy: 0.71\n",
      "Epoch: 34 | Loss: 0.3886 | Train Accuracy: 0.71\n",
      "Epoch: 35 | Loss: 0.3856 | Train Accuracy: 0.71\n",
      "Epoch: 36 | Loss: 0.3783 | Train Accuracy: 0.71\n",
      "Epoch: 37 | Loss: 0.3760 | Train Accuracy: 0.71\n",
      "Epoch: 38 | Loss: 0.3744 | Train Accuracy: 0.71\n",
      "Epoch: 39 | Loss: 0.3695 | Train Accuracy: 0.71\n",
      "Epoch: 40 | Loss: 0.3661 | Train Accuracy: 0.71\n",
      "Epoch: 41 | Loss: 0.3669 | Train Accuracy: 0.71\n",
      "Epoch: 42 | Loss: 0.3619 | Train Accuracy: 0.71\n",
      "Epoch: 43 | Loss: 0.3664 | Train Accuracy: 0.71\n",
      "Epoch: 44 | Loss: 0.3589 | Train Accuracy: 0.71\n",
      "Epoch: 45 | Loss: 0.3569 | Train Accuracy: 0.71\n",
      "Epoch: 46 | Loss: 0.3590 | Train Accuracy: 0.71\n",
      "Epoch: 47 | Loss: 0.3576 | Train Accuracy: 0.71\n",
      "Epoch: 48 | Loss: 0.3552 | Train Accuracy: 0.71\n",
      "Epoch: 49 | Loss: 0.3520 | Train Accuracy: 0.71\n",
      "Epoch: 50 | Loss: 0.3507 | Train Accuracy: 0.71\n",
      "Epoch: 51 | Loss: 0.3485 | Train Accuracy: 0.71\n",
      "Epoch: 52 | Loss: 0.3491 | Train Accuracy: 0.71\n",
      "Epoch: 53 | Loss: 0.3447 | Train Accuracy: 0.71\n",
      "Epoch: 54 | Loss: 0.3442 | Train Accuracy: 0.71\n",
      "Epoch: 55 | Loss: 0.3423 | Train Accuracy: 0.71\n",
      "Epoch: 56 | Loss: 0.3422 | Train Accuracy: 0.71\n",
      "Epoch: 57 | Loss: 0.3426 | Train Accuracy: 0.71\n",
      "Epoch: 58 | Loss: 0.3423 | Train Accuracy: 0.71\n",
      "Epoch: 59 | Loss: 0.3387 | Train Accuracy: 0.71\n",
      "Epoch: 60 | Loss: 0.3355 | Train Accuracy: 0.71\n",
      "Epoch: 61 | Loss: 0.3413 | Train Accuracy: 0.71\n",
      "Epoch: 62 | Loss: 0.3379 | Train Accuracy: 0.71\n",
      "Epoch: 63 | Loss: 0.3346 | Train Accuracy: 0.71\n",
      "Epoch: 64 | Loss: 0.3300 | Train Accuracy: 0.71\n",
      "Epoch: 65 | Loss: 0.3372 | Train Accuracy: 0.72\n",
      "Epoch: 66 | Loss: 0.3311 | Train Accuracy: 0.71\n",
      "Epoch: 67 | Loss: 0.3303 | Train Accuracy: 0.71\n",
      "Epoch: 68 | Loss: 0.3319 | Train Accuracy: 0.71\n",
      "Epoch: 69 | Loss: 0.3276 | Train Accuracy: 0.71\n",
      "Epoch: 70 | Loss: 0.3267 | Train Accuracy: 0.71\n",
      "Epoch: 71 | Loss: 0.3261 | Train Accuracy: 0.71\n",
      "Epoch: 72 | Loss: 0.3266 | Train Accuracy: 0.71\n",
      "Epoch: 73 | Loss: 0.3243 | Train Accuracy: 0.71\n",
      "Epoch: 74 | Loss: 0.3220 | Train Accuracy: 0.71\n",
      "Epoch: 75 | Loss: 0.3205 | Train Accuracy: 0.71\n",
      "Epoch: 76 | Loss: 0.3228 | Train Accuracy: 0.71\n",
      "Epoch: 77 | Loss: 0.3202 | Train Accuracy: 0.71\n",
      "Epoch: 78 | Loss: 0.3171 | Train Accuracy: 0.71\n",
      "Epoch: 79 | Loss: 0.3229 | Train Accuracy: 0.71\n",
      "Epoch: 80 | Loss: 0.3223 | Train Accuracy: 0.71\n",
      "Epoch: 81 | Loss: 0.3180 | Train Accuracy: 0.71\n",
      "Epoch: 82 | Loss: 0.3209 | Train Accuracy: 0.71\n",
      "Epoch: 83 | Loss: 0.3174 | Train Accuracy: 0.71\n",
      "Epoch: 84 | Loss: 0.3158 | Train Accuracy: 0.71\n",
      "Epoch: 85 | Loss: 0.3189 | Train Accuracy: 0.71\n",
      "Epoch: 86 | Loss: 0.3170 | Train Accuracy: 0.71\n",
      "Epoch: 87 | Loss: 0.3199 | Train Accuracy: 0.71\n",
      "Epoch: 88 | Loss: 0.3170 | Train Accuracy: 0.71\n",
      "Epoch: 89 | Loss: 0.3136 | Train Accuracy: 0.71\n",
      "Epoch: 90 | Loss: 0.3135 | Train Accuracy: 0.71\n",
      "Epoch: 91 | Loss: 0.3123 | Train Accuracy: 0.71\n",
      "Epoch: 92 | Loss: 0.3108 | Train Accuracy: 0.71\n",
      "Epoch: 93 | Loss: 0.3123 | Train Accuracy: 0.71\n",
      "Epoch: 94 | Loss: 0.3139 | Train Accuracy: 0.71\n",
      "Epoch: 95 | Loss: 0.3124 | Train Accuracy: 0.71\n",
      "Epoch: 96 | Loss: 0.3092 | Train Accuracy: 0.71\n",
      "Epoch: 97 | Loss: 0.3094 | Train Accuracy: 0.72\n",
      "Epoch: 98 | Loss: 0.3091 | Train Accuracy: 0.71\n",
      "Epoch: 99 | Loss: 0.3083 | Train Accuracy: 0.71\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = 0\n",
    "epoch = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    random.shuffle(dataArray)\n",
    "    dataIterator = iter(dataArray)\n",
    "    ## training step\n",
    "    for i, patient in enumerate(dataIterator):\n",
    "        labels = torch.tensor([patient.num], dtype=torch.float32)\n",
    "        # labels = torch.tensor([0 if i != patient.num else 1 for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "        Pxx = patient.getTensor(foldername);\n",
    "\n",
    "        ## forward + backprop + loss\n",
    "        logits = model(Pxx)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## update model params\n",
    "        optimizer.step()\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += get_acc_2(logits, labels)\n",
    "    model.eval()\n",
    "    accuracies += [train_acc / i]\n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
    "          %(epoch, train_running_loss / i, train_acc/i))\n",
    "    \n",
    "ax.plot(accuracies)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.000000, acc: 1.000000\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.100000, acc: 0.900000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.909091\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.916667\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.923077\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.071429, acc: 0.928571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.066667, acc: 0.933333\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.062500, acc: 0.937500\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.058824, acc: 0.941176\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.055556, acc: 0.944444\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.052632, acc: 0.947368\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.050000, acc: 0.950000\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.047619, acc: 0.952381\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.045455, acc: 0.954545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.043478, acc: 0.956522\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.041667, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.080000, acc: 0.940000\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.942308\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.074074, acc: 0.944444\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.071429, acc: 0.946429\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.068966, acc: 0.948276\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.066667, acc: 0.950000\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.064516, acc: 0.951613\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.062500, acc: 0.953125\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.060606, acc: 0.954545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.058824, acc: 0.955882\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.057143, acc: 0.957143\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.055556, acc: 0.958333\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.054054, acc: 0.959459\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.052632, acc: 0.960526\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.051282, acc: 0.961538\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.050000, acc: 0.962500\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.048780, acc: 0.963415\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.047619, acc: 0.964286\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.046512, acc: 0.965116\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.045455, acc: 0.965909\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.044444, acc: 0.966667\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.043478, acc: 0.967391\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.042553, acc: 0.968085\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.041667, acc: 0.968750\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.040816, acc: 0.969388\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.040000, acc: 0.970000\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.039216, acc: 0.970588\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.038462, acc: 0.971154\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.037736, acc: 0.971698\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.055556, acc: 0.970370\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.054545, acc: 0.970909\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.071429, acc: 0.969196\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.070175, acc: 0.969737\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.068966, acc: 0.970259\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.067797, acc: 0.970763\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.967083\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.081967, acc: 0.967623\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.080645, acc: 0.968145\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.079365, acc: 0.968651\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.078125, acc: 0.969141\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.969615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.075758, acc: 0.970076\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.074627, acc: 0.970522\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.073529, acc: 0.970956\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.956884\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.085714, acc: 0.957500\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.084507, acc: 0.958099\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.958681\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.082192, acc: 0.959247\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.081081, acc: 0.959797\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.093333, acc: 0.953667\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.092105, acc: 0.954276\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.954870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.089744, acc: 0.955449\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.088608, acc: 0.956013\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.087500, acc: 0.956562\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.086420, acc: 0.957099\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.085366, acc: 0.957622\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.084337, acc: 0.958133\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.958631\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.094118, acc: 0.953235\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.093023, acc: 0.953779\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.091954, acc: 0.954310\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.954830\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.089888, acc: 0.955337\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.088889, acc: 0.955833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.087912, acc: 0.956319\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.956793\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.086022, acc: 0.957258\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.085106, acc: 0.957713\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.084211, acc: 0.958158\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.093750, acc: 0.948177\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.092784, acc: 0.948711\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091837, acc: 0.949235\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.949747\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090000, acc: 0.950250\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089109, acc: 0.950743\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.088235, acc: 0.951225\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.087379, acc: 0.951699\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.096154, acc: 0.947356\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104762, acc: 0.946905\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103774, acc: 0.947406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102804, acc: 0.947897\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.101852, acc: 0.948380\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.100917, acc: 0.948853\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.100000, acc: 0.949318\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.099099, acc: 0.949775\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.098214, acc: 0.950223\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097345, acc: 0.950664\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.096491, acc: 0.951096\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.095652, acc: 0.951522\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.094828, acc: 0.951940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.094017, acc: 0.952350\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.093220, acc: 0.952754\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.092437, acc: 0.953151\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.091667, acc: 0.953542\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.953926\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.098361, acc: 0.946107\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097561, acc: 0.946545\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.096774, acc: 0.946976\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.096000, acc: 0.947400\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.947817\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.094488, acc: 0.948228\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.093750, acc: 0.948633\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.093023, acc: 0.949031\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.092308, acc: 0.949423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091603, acc: 0.949809\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.098485, acc: 0.949432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097744, acc: 0.949812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.097015, acc: 0.950187\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096296, acc: 0.950556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.095588, acc: 0.950919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.094891, acc: 0.951277\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.094203, acc: 0.951630\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.100719, acc: 0.950779\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.100000, acc: 0.951131\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.099291, acc: 0.951478\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.098592, acc: 0.951819\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097902, acc: 0.952156\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097222, acc: 0.952488\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.096552, acc: 0.952816\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095890, acc: 0.953139\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.953458\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.094595, acc: 0.953773\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.093960, acc: 0.954083\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.093333, acc: 0.954389\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.092715, acc: 0.954691\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.092105, acc: 0.954989\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.091503, acc: 0.955283\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.955574\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.090323, acc: 0.955860\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.089744, acc: 0.956143\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.089172, acc: 0.956423\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.088608, acc: 0.956698\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.088050, acc: 0.956971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.087500, acc: 0.957240\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.957505\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.092593, acc: 0.956886\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.092025, acc: 0.957150\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.091463, acc: 0.957411\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.957670\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.090361, acc: 0.957925\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095808, acc: 0.957321\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.957575\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.094675, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.094118, acc: 0.958074\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.093567, acc: 0.958319\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.093023, acc: 0.958562\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.098266, acc: 0.958159\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097701, acc: 0.958399\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097143, acc: 0.958637\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.096591, acc: 0.958872\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096045, acc: 0.959105\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.095506, acc: 0.959334\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.094972, acc: 0.959561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.094444, acc: 0.959786\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.093923, acc: 0.960008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.093407, acc: 0.960228\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.092896, acc: 0.960445\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.092391, acc: 0.960660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091892, acc: 0.960873\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.091398, acc: 0.961083\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.961291\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.090426, acc: 0.961497\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.089947, acc: 0.961701\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.089474, acc: 0.961903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089005, acc: 0.962102\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.088542, acc: 0.962300\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.088083, acc: 0.962495\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.087629, acc: 0.962688\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.087179, acc: 0.962880\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.086735, acc: 0.963069\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.086294, acc: 0.963256\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.085859, acc: 0.963442\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.085427, acc: 0.963626\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.090000, acc: 0.963252\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.089552, acc: 0.963435\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.089109, acc: 0.963616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.088670, acc: 0.963795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.088235, acc: 0.963973\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.087805, acc: 0.964148\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.087379, acc: 0.964322\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.964495\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.086538, acc: 0.964665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.086124, acc: 0.964834\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.085714, acc: 0.965002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.085308, acc: 0.965168\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.084906, acc: 0.965332\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.084507, acc: 0.965495\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.084112, acc: 0.965656\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.083721, acc: 0.965816\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.965974\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.082949, acc: 0.966131\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.082569, acc: 0.966286\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.082192, acc: 0.966440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.081818, acc: 0.966593\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.081448, acc: 0.966744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.081081, acc: 0.966894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.080717, acc: 0.967042\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.080357, acc: 0.967189\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.080000, acc: 0.967335\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.079646, acc: 0.967480\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.079295, acc: 0.967623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.078947, acc: 0.967765\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.078603, acc: 0.967906\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.078261, acc: 0.968045\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.077922, acc: 0.968184\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.077586, acc: 0.968321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.077253, acc: 0.968457\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.968591\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.076596, acc: 0.968725\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.076271, acc: 0.968858\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.080169, acc: 0.967934\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.079832, acc: 0.968069\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.079498, acc: 0.968202\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.079167, acc: 0.968335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.078838, acc: 0.968466\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.078512, acc: 0.968597\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.078189, acc: 0.968726\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.077869, acc: 0.968854\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.077551, acc: 0.968981\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.077236, acc: 0.969107\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.969232\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.076613, acc: 0.969356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.076305, acc: 0.969480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.076000, acc: 0.969602\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.079681, acc: 0.968395\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.079365, acc: 0.968520\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.079051, acc: 0.968645\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.078740, acc: 0.968768\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.078431, acc: 0.968890\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.078125, acc: 0.969012\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.077821, acc: 0.969133\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.077519, acc: 0.969252\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.081081, acc: 0.968985\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.080769, acc: 0.969104\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.080460, acc: 0.969222\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.080153, acc: 0.969340\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.079848, acc: 0.969457\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.079545, acc: 0.969572\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.079245, acc: 0.969687\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.078947, acc: 0.969801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.078652, acc: 0.969914\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.078358, acc: 0.970026\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.078067, acc: 0.970138\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.077778, acc: 0.970248\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.077491, acc: 0.970358\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.077206, acc: 0.970467\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.970575\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.076642, acc: 0.970683\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.076364, acc: 0.970789\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.076087, acc: 0.970895\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.075812, acc: 0.971000\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.075540, acc: 0.971105\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.075269, acc: 0.971208\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.075000, acc: 0.971311\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.074733, acc: 0.971413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.074468, acc: 0.971514\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.074205, acc: 0.971615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.073944, acc: 0.971715\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.073684, acc: 0.971814\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.073427, acc: 0.971913\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.073171, acc: 0.972011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.072917, acc: 0.972108\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.072664, acc: 0.972204\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.072414, acc: 0.972300\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.072165, acc: 0.972395\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.071918, acc: 0.972490\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.071672, acc: 0.972584\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.071429, acc: 0.972677\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.071186, acc: 0.972770\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.074324, acc: 0.972379\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.074074, acc: 0.972472\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.073826, acc: 0.972564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.073579, acc: 0.972656\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.076667, acc: 0.972377\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.079734, acc: 0.971361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.079470, acc: 0.971456\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.079208, acc: 0.971550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.078947, acc: 0.971644\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.078689, acc: 0.971737\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.078431, acc: 0.971829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.078176, acc: 0.971921\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.077922, acc: 0.972012\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.077670, acc: 0.972103\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.077419, acc: 0.972193\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.080386, acc: 0.969067\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.080128, acc: 0.969166\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.079872, acc: 0.969264\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.079618, acc: 0.969362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.079365, acc: 0.969460\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.079114, acc: 0.969556\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.078864, acc: 0.969652\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.078616, acc: 0.969748\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.078370, acc: 0.969843\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.078125, acc: 0.969937\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.077882, acc: 0.970030\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.077640, acc: 0.970123\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.077399, acc: 0.970216\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.077160, acc: 0.970308\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.076923, acc: 0.970399\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.076687, acc: 0.970490\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.076453, acc: 0.970580\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.076220, acc: 0.970670\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.075988, acc: 0.970759\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.075758, acc: 0.970848\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.075529, acc: 0.970936\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.075301, acc: 0.971023\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.075075, acc: 0.971110\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.077844, acc: 0.970448\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.080597, acc: 0.970205\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.080357, acc: 0.970294\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.080119, acc: 0.970382\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.079882, acc: 0.970469\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.079646, acc: 0.970556\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.079412, acc: 0.970643\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.079179, acc: 0.970729\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.081871, acc: 0.969840\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.081633, acc: 0.969928\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.081395, acc: 0.970015\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.081159, acc: 0.970102\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.080925, acc: 0.970189\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.080692, acc: 0.970275\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.080460, acc: 0.970360\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.080229, acc: 0.970445\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.080000, acc: 0.970529\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.079772, acc: 0.970613\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.079545, acc: 0.970697\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.082153, acc: 0.970465\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.081921, acc: 0.970549\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.081690, acc: 0.970632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.081461, acc: 0.970714\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.081232, acc: 0.970796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.081006, acc: 0.970878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.080780, acc: 0.970959\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.080556, acc: 0.971039\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.083102, acc: 0.970812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.082873, acc: 0.970893\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.082645, acc: 0.970973\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.082418, acc: 0.971052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.082192, acc: 0.971132\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.081967, acc: 0.971211\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.081744, acc: 0.971289\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.081522, acc: 0.971367\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.081301, acc: 0.971445\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.081081, acc: 0.971522\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.080863, acc: 0.971599\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.968987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.083110, acc: 0.969070\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.082888, acc: 0.969153\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.082667, acc: 0.969235\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.085106, acc: 0.969051\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.084881, acc: 0.969133\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.084656, acc: 0.969215\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.084433, acc: 0.969296\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.084211, acc: 0.969377\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.083990, acc: 0.969457\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.083770, acc: 0.969537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.083551, acc: 0.969616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.969696\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.083117, acc: 0.969774\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.082902, acc: 0.969853\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.082687, acc: 0.969930\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.082474, acc: 0.970008\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.082262, acc: 0.970085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.082051, acc: 0.970162\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.081841, acc: 0.970238\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.084184, acc: 0.969039\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.083969, acc: 0.969117\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.083756, acc: 0.969196\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.083544, acc: 0.969274\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.083333, acc: 0.969351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.083123, acc: 0.969428\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.082915, acc: 0.969505\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.082707, acc: 0.969582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.082500, acc: 0.969658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.082294, acc: 0.969733\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.082090, acc: 0.969809\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.081886, acc: 0.969884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.081683, acc: 0.969958\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.081481, acc: 0.970032\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.081281, acc: 0.970106\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.081081, acc: 0.970180\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.080882, acc: 0.970253\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.080685, acc: 0.970325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.080488, acc: 0.970398\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.080292, acc: 0.970470\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.080097, acc: 0.970541\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.079903, acc: 0.970613\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.082126, acc: 0.968268\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.081928, acc: 0.968345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.081731, acc: 0.968421\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.081535, acc: 0.968497\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.081340, acc: 0.968572\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.081146, acc: 0.968647\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.080952, acc: 0.968722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.080760, acc: 0.968796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.080569, acc: 0.968870\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.080378, acc: 0.968943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.080189, acc: 0.969017\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.082353, acc: 0.966737\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.082160, acc: 0.966815\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.081967, acc: 0.966892\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.084112, acc: 0.966736\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.083916, acc: 0.966814\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.083721, acc: 0.966891\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.083527, acc: 0.966968\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.085648, acc: 0.965887\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.087760, acc: 0.963656\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.087558, acc: 0.963740\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.087356, acc: 0.963823\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.087156, acc: 0.963906\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.963989\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.086758, acc: 0.964071\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.088838, acc: 0.963925\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.961734\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.092971, acc: 0.961065\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.092760, acc: 0.961153\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.092551, acc: 0.961241\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.092342, acc: 0.961328\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.092135, acc: 0.961415\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.091928, acc: 0.961502\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.091723, acc: 0.961588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.091518, acc: 0.961674\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.091314, acc: 0.961759\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.091111, acc: 0.961844\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.961929\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.090708, acc: 0.962013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090508, acc: 0.962097\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.090308, acc: 0.962180\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090110, acc: 0.962263\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.089912, acc: 0.962346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.089716, acc: 0.962428\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.089520, acc: 0.962510\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.089325, acc: 0.962592\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089130, acc: 0.962673\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.088937, acc: 0.962754\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.960670\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.090713, acc: 0.960755\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.090517, acc: 0.960840\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.090323, acc: 0.960924\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.090129, acc: 0.961008\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089936, acc: 0.961092\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.091880, acc: 0.959038\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.091684, acc: 0.959125\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.091489, acc: 0.959212\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.091295, acc: 0.959299\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091102, acc: 0.959385\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.959471\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.092827, acc: 0.958853\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.092632, acc: 0.958940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.092437, acc: 0.959026\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.092243, acc: 0.959112\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.092050, acc: 0.959198\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.091858, acc: 0.959283\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.091667, acc: 0.959368\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.091476, acc: 0.959452\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.091286, acc: 0.959536\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.091097, acc: 0.959620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.959703\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090722, acc: 0.959786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.090535, acc: 0.959869\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.090349, acc: 0.959952\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.090164, acc: 0.960034\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089980, acc: 0.960115\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.089796, acc: 0.960197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089613, acc: 0.960278\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.089431, acc: 0.960359\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.089249, acc: 0.960439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.089069, acc: 0.960519\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.088889, acc: 0.960599\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.088710, acc: 0.960678\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.088531, acc: 0.960757\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.088353, acc: 0.960836\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.088176, acc: 0.960915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.088000, acc: 0.960993\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.087824, acc: 0.961071\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.087649, acc: 0.961148\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.087475, acc: 0.961226\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.087302, acc: 0.961302\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.087129, acc: 0.961379\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.086957, acc: 0.961455\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.088757, acc: 0.960545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.088583, acc: 0.960623\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.088409, acc: 0.960700\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.090196, acc: 0.959797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.090020, acc: 0.959876\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.089844, acc: 0.959954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.089669, acc: 0.960032\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.089494, acc: 0.960110\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.089320, acc: 0.960187\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.089147, acc: 0.960264\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.090909, acc: 0.960148\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.090734, acc: 0.960225\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.090559, acc: 0.960301\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.092308, acc: 0.958455\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.094050, acc: 0.958294\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.093870, acc: 0.958374\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.093690, acc: 0.958454\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.095420, acc: 0.958295\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.958374\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.095057, acc: 0.958453\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.094877, acc: 0.958532\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.094697, acc: 0.958611\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.094518, acc: 0.958689\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.094340, acc: 0.958767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.094162, acc: 0.958844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.093985, acc: 0.958922\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.093809, acc: 0.958999\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.093633, acc: 0.959076\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.093458, acc: 0.959152\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.093284, acc: 0.959228\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.093110, acc: 0.959304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.092937, acc: 0.959380\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.092764, acc: 0.959455\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.092593, acc: 0.959530\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.092421, acc: 0.959605\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.092251, acc: 0.959680\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.093923, acc: 0.957912\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.093750, acc: 0.957990\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.093578, acc: 0.958067\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.093407, acc: 0.958144\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.093236, acc: 0.958220\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.093066, acc: 0.958296\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.092896, acc: 0.958372\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.092727, acc: 0.958448\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.092559, acc: 0.958523\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.092391, acc: 0.958599\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.092224, acc: 0.958673\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.092058, acc: 0.958748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091892, acc: 0.958822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.091727, acc: 0.958896\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.093357, acc: 0.958073\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.094982, acc: 0.957252\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.094812, acc: 0.957328\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.094643, acc: 0.957404\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.094474, acc: 0.957480\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.094306, acc: 0.957556\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.094139, acc: 0.957631\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.095745, acc: 0.955933\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095575, acc: 0.956011\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.095406, acc: 0.956089\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.956167\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.095070, acc: 0.956244\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.094903, acc: 0.956321\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.094737, acc: 0.956397\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096322, acc: 0.956279\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.096154, acc: 0.956355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095986, acc: 0.956432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.095819, acc: 0.956508\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095652, acc: 0.956583\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.095486, acc: 0.956659\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.095321, acc: 0.956734\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095156, acc: 0.956809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.094991, acc: 0.956883\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.094828, acc: 0.956957\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.096386, acc: 0.956171\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.096220, acc: 0.956246\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096055, acc: 0.956321\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.095890, acc: 0.956396\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097436, acc: 0.955901\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.097270, acc: 0.955976\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.097104, acc: 0.956051\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.096939, acc: 0.956126\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.096774, acc: 0.956200\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.096610, acc: 0.956275\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.096447, acc: 0.956349\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.097973, acc: 0.954733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.097808, acc: 0.954809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097643, acc: 0.954885\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.097479, acc: 0.954961\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.097315, acc: 0.955037\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097152, acc: 0.955112\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096990, acc: 0.955187\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.096828, acc: 0.955262\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.096667, acc: 0.955337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096506, acc: 0.955411\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096346, acc: 0.955485\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096186, acc: 0.955559\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.096026, acc: 0.955632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095868, acc: 0.955706\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.095710, acc: 0.955779\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.095552, acc: 0.955852\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.095395, acc: 0.955924\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.955997\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.096721, acc: 0.954429\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096563, acc: 0.954504\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.096405, acc: 0.954578\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.096248, acc: 0.954653\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.096091, acc: 0.954726\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.097561, acc: 0.953987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.097403, acc: 0.954062\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.097245, acc: 0.954136\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.097087, acc: 0.954210\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096931, acc: 0.954284\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096774, acc: 0.954358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.096618, acc: 0.954432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.096463, acc: 0.954505\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.097913, acc: 0.954377\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097756, acc: 0.954450\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.097600, acc: 0.954523\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.097444, acc: 0.954596\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097289, acc: 0.954668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097134, acc: 0.954740\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.096979, acc: 0.954812\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096825, acc: 0.954884\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.096672, acc: 0.954956\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.098101, acc: 0.954236\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.097946, acc: 0.954308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.097792, acc: 0.954380\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097638, acc: 0.954452\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097484, acc: 0.954524\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.097331, acc: 0.954595\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.097179, acc: 0.954666\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097027, acc: 0.954737\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.096875, acc: 0.954808\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.096724, acc: 0.954878\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096573, acc: 0.954949\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.096423, acc: 0.955019\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.096273, acc: 0.955088\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096124, acc: 0.955158\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.095975, acc: 0.955228\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.095827, acc: 0.955297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.095679, acc: 0.955366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.095532, acc: 0.955434\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.095385, acc: 0.955503\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.095238, acc: 0.955571\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096626, acc: 0.955469\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096478, acc: 0.955537\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096330, acc: 0.955605\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.097710, acc: 0.955520\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097561, acc: 0.955588\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.097412, acc: 0.955656\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097264, acc: 0.955723\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097117, acc: 0.955790\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096970, acc: 0.955857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.096823, acc: 0.955924\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096677, acc: 0.955991\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.096531, acc: 0.956057\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.096386, acc: 0.956123\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096241, acc: 0.956189\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.096096, acc: 0.956255\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.095952, acc: 0.956321\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.095808, acc: 0.956386\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.095665, acc: 0.956451\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.095522, acc: 0.956516\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.096870, acc: 0.955091\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.098214, acc: 0.954414\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.098068, acc: 0.954481\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.097923, acc: 0.954549\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.097778, acc: 0.954616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.097633, acc: 0.954683\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097489, acc: 0.954750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097345, acc: 0.954817\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.097202, acc: 0.954883\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.097059, acc: 0.954950\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096916, acc: 0.955016\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.096774, acc: 0.955082\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.096633, acc: 0.955148\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096491, acc: 0.955213\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.096350, acc: 0.955279\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096210, acc: 0.955344\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.096070, acc: 0.955409\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.097384, acc: 0.954747\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.098694, acc: 0.954667\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.098551, acc: 0.954733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.098408, acc: 0.954799\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.098266, acc: 0.954864\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.098124, acc: 0.954929\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.097983, acc: 0.954994\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.097842, acc: 0.955059\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.097701, acc: 0.955123\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.097561, acc: 0.955188\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.097421, acc: 0.955252\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.098712, acc: 0.953885\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.098571, acc: 0.953951\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.098431, acc: 0.954017\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.098291, acc: 0.954082\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.098151, acc: 0.954148\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.098011, acc: 0.954213\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.099291, acc: 0.952859\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.099150, acc: 0.952926\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.099010, acc: 0.952993\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.100282, acc: 0.952353\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.100141, acc: 0.952420\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.100000, acc: 0.952487\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.101266, acc: 0.951851\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101124, acc: 0.951918\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.100982, acc: 0.951986\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.100840, acc: 0.952053\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.100699, acc: 0.952120\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.100559, acc: 0.952187\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.101813, acc: 0.950859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101671, acc: 0.950927\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101530, acc: 0.950996\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.101389, acc: 0.951064\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.101248, acc: 0.951132\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102493, acc: 0.950853\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102351, acc: 0.950921\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102210, acc: 0.950989\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102069, acc: 0.951056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101928, acc: 0.951124\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.101788, acc: 0.951191\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.101648, acc: 0.951258\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101509, acc: 0.951325\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102740, acc: 0.950935\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102599, acc: 0.951002\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102459, acc: 0.951069\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102319, acc: 0.951136\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102180, acc: 0.951202\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102041, acc: 0.951269\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.101902, acc: 0.951335\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.103121, acc: 0.950044\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104336, acc: 0.949942\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104195, acc: 0.950010\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104054, acc: 0.950078\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103914, acc: 0.950145\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103774, acc: 0.950212\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103634, acc: 0.950279\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103495, acc: 0.950346\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103356, acc: 0.950413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103217, acc: 0.950479\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103079, acc: 0.950546\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102941, acc: 0.950612\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102804, acc: 0.950678\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.104000, acc: 0.950610\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103862, acc: 0.950676\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103723, acc: 0.950741\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103586, acc: 0.950807\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103448, acc: 0.950872\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103311, acc: 0.950937\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103175, acc: 0.951002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103038, acc: 0.951067\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102902, acc: 0.951131\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102767, acc: 0.951196\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102632, acc: 0.951260\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.103811, acc: 0.951193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103675, acc: 0.951257\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103539, acc: 0.951320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103403, acc: 0.951384\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104575, acc: 0.951012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104439, acc: 0.951076\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104302, acc: 0.951140\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104167, acc: 0.951203\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104031, acc: 0.951267\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103896, acc: 0.951330\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103761, acc: 0.951393\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103627, acc: 0.951456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103493, acc: 0.951519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103359, acc: 0.951582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103226, acc: 0.951644\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103093, acc: 0.951706\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102960, acc: 0.951769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102828, acc: 0.951831\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102696, acc: 0.951892\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102564, acc: 0.951954\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102433, acc: 0.952016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102302, acc: 0.952077\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102171, acc: 0.952138\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102041, acc: 0.952199\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.103185, acc: 0.952133\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103053, acc: 0.952194\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102922, acc: 0.952254\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102792, acc: 0.952315\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102662, acc: 0.952375\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102532, acc: 0.952436\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102402, acc: 0.952496\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102273, acc: 0.952556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102144, acc: 0.952616\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102015, acc: 0.952675\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101887, acc: 0.952735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101759, acc: 0.952794\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102886, acc: 0.952714\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102757, acc: 0.952773\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102628, acc: 0.952832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102500, acc: 0.952891\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102372, acc: 0.952950\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103491, acc: 0.952385\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103362, acc: 0.952445\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103234, acc: 0.952504\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103106, acc: 0.952563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102978, acc: 0.952622\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102850, acc: 0.952680\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102723, acc: 0.952739\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102596, acc: 0.952797\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102469, acc: 0.952856\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102343, acc: 0.952914\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102217, acc: 0.952972\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102091, acc: 0.953030\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101966, acc: 0.953087\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.101840, acc: 0.953145\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.101716, acc: 0.953202\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.102815, acc: 0.952036\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102689, acc: 0.952094\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102564, acc: 0.952153\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102439, acc: 0.952211\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102314, acc: 0.952269\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102190, acc: 0.952327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102066, acc: 0.952385\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101942, acc: 0.952443\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101818, acc: 0.952501\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.101695, acc: 0.952558\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.101572, acc: 0.952616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101449, acc: 0.952673\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.102533, acc: 0.952609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102410, acc: 0.952666\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102286, acc: 0.952723\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102163, acc: 0.952780\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102041, acc: 0.952837\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.103118, acc: 0.952294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102994, acc: 0.952351\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102871, acc: 0.952408\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102748, acc: 0.952465\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102625, acc: 0.952522\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102503, acc: 0.952578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102381, acc: 0.952635\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102259, acc: 0.952691\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102138, acc: 0.952747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102017, acc: 0.952803\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101896, acc: 0.952859\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.101775, acc: 0.952915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.101655, acc: 0.952971\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.101535, acc: 0.953026\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.101415, acc: 0.953081\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101296, acc: 0.953137\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101176, acc: 0.953192\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.101058, acc: 0.953247\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.100939, acc: 0.953302\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.100821, acc: 0.953357\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.100703, acc: 0.953411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.100585, acc: 0.953466\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.100467, acc: 0.953520\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.100350, acc: 0.953574\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.100233, acc: 0.953628\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.100116, acc: 0.953682\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.101163, acc: 0.953155\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101045, acc: 0.953209\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.100928, acc: 0.953263\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.100811, acc: 0.953318\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.101852, acc: 0.953243\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101734, acc: 0.953297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.101617, acc: 0.953351\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102653, acc: 0.952828\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102535, acc: 0.952882\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102417, acc: 0.952937\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102299, acc: 0.952991\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102181, acc: 0.953045\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102064, acc: 0.953099\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101947, acc: 0.953152\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101831, acc: 0.953206\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101714, acc: 0.953259\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.101598, acc: 0.953313\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101482, acc: 0.953366\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101367, acc: 0.953419\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101251, acc: 0.953472\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.101136, acc: 0.953525\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101022, acc: 0.953578\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.100907, acc: 0.953630\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.100793, acc: 0.953683\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.100679, acc: 0.953735\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.100565, acc: 0.953788\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.101580, acc: 0.952711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.101466, acc: 0.952764\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101351, acc: 0.952818\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102362, acc: 0.952710\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102247, acc: 0.952763\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102132, acc: 0.952816\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102018, acc: 0.952869\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.101904, acc: 0.952922\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101790, acc: 0.952974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101676, acc: 0.953027\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101562, acc: 0.953079\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101449, acc: 0.953132\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102450, acc: 0.952998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102336, acc: 0.953051\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103333, acc: 0.952944\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103219, acc: 0.952996\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103104, acc: 0.953048\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102990, acc: 0.953100\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102876, acc: 0.953152\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103867, acc: 0.953081\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103753, acc: 0.953133\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103638, acc: 0.953185\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103524, acc: 0.953236\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103410, acc: 0.953288\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103297, acc: 0.953339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103183, acc: 0.953390\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103070, acc: 0.953441\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102957, acc: 0.953492\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102845, acc: 0.953543\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102732, acc: 0.953594\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102620, acc: 0.953645\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102508, acc: 0.953695\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102397, acc: 0.953746\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102285, acc: 0.953796\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102174, acc: 0.953846\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102063, acc: 0.953896\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101952, acc: 0.953946\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102925, acc: 0.953816\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.102814, acc: 0.953866\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103784, acc: 0.953375\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103672, acc: 0.953425\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103560, acc: 0.953476\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103448, acc: 0.953526\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104413, acc: 0.952499\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104301, acc: 0.952550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104189, acc: 0.952601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104077, acc: 0.952652\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105038, acc: 0.952435\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104925, acc: 0.952486\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104813, acc: 0.952537\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104701, acc: 0.952587\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104589, acc: 0.952638\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104478, acc: 0.952689\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104366, acc: 0.952739\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104255, acc: 0.952789\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104145, acc: 0.952839\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104034, acc: 0.952889\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103924, acc: 0.952939\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103814, acc: 0.952989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103704, acc: 0.953039\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103594, acc: 0.953089\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103485, acc: 0.953138\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103376, acc: 0.953188\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.103267, acc: 0.953237\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103158, acc: 0.953286\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103049, acc: 0.953335\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102941, acc: 0.953384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.102833, acc: 0.953433\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102725, acc: 0.953482\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102618, acc: 0.953531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102510, acc: 0.953579\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102403, acc: 0.953628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102296, acc: 0.953676\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103233, acc: 0.953609\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103125, acc: 0.953657\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103018, acc: 0.953705\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102911, acc: 0.953753\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102804, acc: 0.953801\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102697, acc: 0.953849\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102591, acc: 0.953897\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102484, acc: 0.953945\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102378, acc: 0.953992\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102273, acc: 0.954040\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103199, acc: 0.953571\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103093, acc: 0.953619\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102987, acc: 0.953667\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102881, acc: 0.953715\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102775, acc: 0.953762\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102669, acc: 0.953810\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102564, acc: 0.953857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102459, acc: 0.953904\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.102354, acc: 0.953952\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.102249, acc: 0.953999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102145, acc: 0.954046\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102041, acc: 0.954093\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.101937, acc: 0.954139\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101833, acc: 0.954186\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101729, acc: 0.954233\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101626, acc: 0.954279\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.101523, acc: 0.954326\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102434, acc: 0.954271\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102330, acc: 0.954317\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102227, acc: 0.954363\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102123, acc: 0.954409\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.102020, acc: 0.954455\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.101917, acc: 0.954501\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.101815, acc: 0.954547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.101712, acc: 0.954593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.101610, acc: 0.954639\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.102513, acc: 0.953679\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.102410, acc: 0.953726\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.102307, acc: 0.953772\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104208, acc: 0.951814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104104, acc: 0.951863\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104000, acc: 0.951911\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104895, acc: 0.950960\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104790, acc: 0.951009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104686, acc: 0.951058\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104582, acc: 0.951106\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104478, acc: 0.951155\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104374, acc: 0.951203\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104270, acc: 0.951252\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104167, acc: 0.951300\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105055, acc: 0.951249\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104950, acc: 0.951298\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104847, acc: 0.951346\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104743, acc: 0.951394\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104640, acc: 0.951442\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104536, acc: 0.951490\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104433, acc: 0.951538\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104331, acc: 0.951585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104228, acc: 0.951633\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104126, acc: 0.951680\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104024, acc: 0.951728\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103922, acc: 0.951775\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103820, acc: 0.951822\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103718, acc: 0.951870\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103617, acc: 0.951917\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103516, acc: 0.951964\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103415, acc: 0.952010\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104288, acc: 0.951895\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105161, acc: 0.951844\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105058, acc: 0.951891\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104956, acc: 0.951938\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105825, acc: 0.951661\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105723, acc: 0.951708\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105620, acc: 0.951755\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105518, acc: 0.951801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105416, acc: 0.951848\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105314, acc: 0.951894\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105212, acc: 0.951941\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105111, acc: 0.951987\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105010, acc: 0.952033\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104909, acc: 0.952080\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104808, acc: 0.952126\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105668, acc: 0.952076\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105566, acc: 0.952122\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105465, acc: 0.952168\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105364, acc: 0.952213\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.952259\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106119, acc: 0.952066\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106017, acc: 0.952111\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106870, acc: 0.951680\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106768, acc: 0.951726\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106667, acc: 0.951772\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106565, acc: 0.951818\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106464, acc: 0.951864\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106363, acc: 0.951910\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106262, acc: 0.951955\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107109, acc: 0.951906\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107008, acc: 0.951951\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106906, acc: 0.951997\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106805, acc: 0.952042\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106704, acc: 0.952088\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106604, acc: 0.952133\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106503, acc: 0.952178\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106403, acc: 0.952223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106303, acc: 0.952268\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106203, acc: 0.952313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106103, acc: 0.952357\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106004, acc: 0.952402\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105904, acc: 0.952447\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105805, acc: 0.952491\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105706, acc: 0.952536\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105607, acc: 0.952580\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.106443, acc: 0.951691\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106343, acc: 0.951736\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106244, acc: 0.951781\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106145, acc: 0.951826\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106047, acc: 0.951870\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105948, acc: 0.951915\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105850, acc: 0.951960\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105751, acc: 0.952004\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105653, acc: 0.952049\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105556, acc: 0.952093\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105458, acc: 0.952138\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105360, acc: 0.952182\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.952226\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105166, acc: 0.952270\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105069, acc: 0.952314\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104972, acc: 0.952358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104876, acc: 0.952402\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104779, acc: 0.952446\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104683, acc: 0.952489\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104587, acc: 0.952533\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104491, acc: 0.952576\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104396, acc: 0.952620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104300, acc: 0.952663\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104205, acc: 0.952706\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104110, acc: 0.952750\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104015, acc: 0.952793\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103920, acc: 0.952836\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103825, acc: 0.952879\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103731, acc: 0.952921\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103636, acc: 0.952964\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103542, acc: 0.953007\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103448, acc: 0.953050\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103354, acc: 0.953092\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103261, acc: 0.953135\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103167, acc: 0.953177\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103074, acc: 0.953219\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.102981, acc: 0.953262\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.102888, acc: 0.953304\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.102795, acc: 0.953346\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.102703, acc: 0.953388\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103510, acc: 0.953340\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103417, acc: 0.953382\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103324, acc: 0.953424\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.103232, acc: 0.953466\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103139, acc: 0.953507\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103943, acc: 0.953437\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103850, acc: 0.953479\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103757, acc: 0.953520\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103664, acc: 0.953562\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104464, acc: 0.952710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104371, acc: 0.952753\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104278, acc: 0.952795\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104185, acc: 0.952837\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104093, acc: 0.952879\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104000, acc: 0.952921\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104796, acc: 0.952518\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104703, acc: 0.952561\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104610, acc: 0.952603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104517, acc: 0.952645\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104425, acc: 0.952686\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104332, acc: 0.952728\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105124, acc: 0.952328\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105031, acc: 0.952370\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104938, acc: 0.952412\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105727, acc: 0.952234\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105634, acc: 0.952276\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105541, acc: 0.952318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105448, acc: 0.952360\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105356, acc: 0.952402\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106140, acc: 0.952318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106047, acc: 0.952360\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105954, acc: 0.952402\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105862, acc: 0.952443\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105769, acc: 0.952485\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105677, acc: 0.952527\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105585, acc: 0.952568\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105493, acc: 0.952609\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105401, acc: 0.952651\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105309, acc: 0.952692\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105217, acc: 0.952733\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105126, acc: 0.952774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105035, acc: 0.952815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104944, acc: 0.952856\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104853, acc: 0.952897\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105628, acc: 0.952851\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105536, acc: 0.952892\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105445, acc: 0.952932\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105354, acc: 0.952973\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.953014\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105172, acc: 0.953054\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105082, acc: 0.953095\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104991, acc: 0.953135\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104901, acc: 0.953175\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104811, acc: 0.953216\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104721, acc: 0.953256\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104631, acc: 0.953296\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105398, acc: 0.952907\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105308, acc: 0.952948\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105218, acc: 0.952988\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105128, acc: 0.953028\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105038, acc: 0.953068\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104949, acc: 0.953108\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104859, acc: 0.953148\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104770, acc: 0.953188\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105532, acc: 0.952802\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105442, acc: 0.952843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105353, acc: 0.952883\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.952923\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105174, acc: 0.952963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105085, acc: 0.953002\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104996, acc: 0.953042\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104907, acc: 0.953082\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104818, acc: 0.953122\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104730, acc: 0.953161\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104641, acc: 0.953201\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104553, acc: 0.953240\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104465, acc: 0.953280\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104377, acc: 0.953319\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104289, acc: 0.953358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104202, acc: 0.953397\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104114, acc: 0.953436\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104027, acc: 0.953476\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103940, acc: 0.953515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103853, acc: 0.953553\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.103766, acc: 0.953592\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103679, acc: 0.953631\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103592, acc: 0.953670\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103506, acc: 0.953709\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103420, acc: 0.953747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103333, acc: 0.953786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103247, acc: 0.953824\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.103161, acc: 0.953863\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103076, acc: 0.953901\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.103821, acc: 0.953109\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103734, acc: 0.953148\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103648, acc: 0.953186\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103563, acc: 0.953225\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103477, acc: 0.953264\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103391, acc: 0.953303\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.103306, acc: 0.953341\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.103220, acc: 0.953380\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103960, acc: 0.953143\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.104699, acc: 0.953099\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104613, acc: 0.953138\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104527, acc: 0.953177\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104441, acc: 0.953215\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104355, acc: 0.953254\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104269, acc: 0.953292\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104184, acc: 0.953330\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104098, acc: 0.953368\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104013, acc: 0.953407\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.103928, acc: 0.953445\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.103843, acc: 0.953483\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104575, acc: 0.953112\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104490, acc: 0.953151\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104405, acc: 0.953189\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104319, acc: 0.953227\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104235, acc: 0.953265\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104963, acc: 0.953222\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104878, acc: 0.953260\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104793, acc: 0.953298\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104708, acc: 0.953336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104623, acc: 0.953373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104538, acc: 0.953411\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104453, acc: 0.953449\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104369, acc: 0.953487\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104285, acc: 0.953524\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105008, acc: 0.953481\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104923, acc: 0.953519\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105645, acc: 0.953475\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105560, acc: 0.953513\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105475, acc: 0.953550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105390, acc: 0.953588\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105305, acc: 0.953625\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105221, acc: 0.953662\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105136, acc: 0.953699\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105052, acc: 0.953737\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105769, acc: 0.953694\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105685, acc: 0.953731\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105600, acc: 0.953768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105516, acc: 0.953805\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105431, acc: 0.953841\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105347, acc: 0.953878\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.953915\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105179, acc: 0.953952\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105096, acc: 0.953988\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105012, acc: 0.954025\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104928, acc: 0.954062\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105639, acc: 0.954019\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105556, acc: 0.954055\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105472, acc: 0.954092\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105388, acc: 0.954128\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105305, acc: 0.954164\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105222, acc: 0.954201\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105138, acc: 0.954237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105055, acc: 0.954273\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104972, acc: 0.954309\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104890, acc: 0.954345\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104807, acc: 0.954381\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104724, acc: 0.954417\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104642, acc: 0.954453\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104560, acc: 0.954489\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104478, acc: 0.954524\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105181, acc: 0.954462\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105098, acc: 0.954498\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105016, acc: 0.954533\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104933, acc: 0.954569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104851, acc: 0.954604\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104769, acc: 0.954640\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104688, acc: 0.954675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104606, acc: 0.954711\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104524, acc: 0.954746\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104443, acc: 0.954781\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104361, acc: 0.954817\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.105058, acc: 0.954074\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104977, acc: 0.954109\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104895, acc: 0.954145\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104814, acc: 0.954181\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104732, acc: 0.954216\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104651, acc: 0.954252\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105345, acc: 0.953900\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.953935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105182, acc: 0.953971\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105100, acc: 0.954007\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105792, acc: 0.953656\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106481, acc: 0.953595\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107170, acc: 0.953545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107088, acc: 0.953581\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107005, acc: 0.953617\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106923, acc: 0.953653\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106841, acc: 0.953688\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107527, acc: 0.953647\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107444, acc: 0.953683\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107362, acc: 0.953718\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107280, acc: 0.953754\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107963, acc: 0.953704\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108646, acc: 0.953654\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108563, acc: 0.953690\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108480, acc: 0.953725\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109160, acc: 0.953676\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109077, acc: 0.953711\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108994, acc: 0.953746\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108911, acc: 0.953781\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109589, acc: 0.953741\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109506, acc: 0.953776\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110182, acc: 0.953431\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110099, acc: 0.953466\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110015, acc: 0.953502\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109932, acc: 0.953537\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109848, acc: 0.953572\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109765, acc: 0.953607\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110439, acc: 0.953558\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110355, acc: 0.953593\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111027, acc: 0.953534\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111698, acc: 0.953192\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.112368, acc: 0.953152\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112283, acc: 0.953187\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112199, acc: 0.953222\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.112114, acc: 0.953257\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112030, acc: 0.953292\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111946, acc: 0.953328\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111862, acc: 0.953363\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111778, acc: 0.953398\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111694, acc: 0.953433\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111610, acc: 0.953467\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111527, acc: 0.953502\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111444, acc: 0.953537\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111360, acc: 0.953572\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111277, acc: 0.953606\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111194, acc: 0.953641\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.953676\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111028, acc: 0.953710\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110946, acc: 0.953745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110863, acc: 0.953779\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110781, acc: 0.953813\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110698, acc: 0.953848\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110616, acc: 0.953882\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110534, acc: 0.953916\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110452, acc: 0.953950\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110370, acc: 0.953984\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110289, acc: 0.954018\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110207, acc: 0.954052\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110865, acc: 0.953717\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110783, acc: 0.953751\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110701, acc: 0.953785\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110619, acc: 0.953819\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110538, acc: 0.953853\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110457, acc: 0.953887\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110375, acc: 0.953921\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110294, acc: 0.953955\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110213, acc: 0.953989\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110132, acc: 0.954023\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110051, acc: 0.954056\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109971, acc: 0.954090\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.954124\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109810, acc: 0.954157\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109729, acc: 0.954191\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109649, acc: 0.954224\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109569, acc: 0.954258\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109489, acc: 0.954291\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109409, acc: 0.954325\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109329, acc: 0.954358\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109250, acc: 0.954391\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109170, acc: 0.954424\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109091, acc: 0.954457\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109012, acc: 0.954491\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108932, acc: 0.954524\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108853, acc: 0.954557\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108774, acc: 0.954590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.954622\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108617, acc: 0.954655\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108538, acc: 0.954688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108460, acc: 0.954721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108382, acc: 0.954754\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108303, acc: 0.954786\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108225, acc: 0.954819\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108147, acc: 0.954851\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108069, acc: 0.954884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107991, acc: 0.954916\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107914, acc: 0.954949\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107836, acc: 0.954981\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107759, acc: 0.955014\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107681, acc: 0.955046\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107604, acc: 0.955078\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108244, acc: 0.955039\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108166, acc: 0.955071\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108805, acc: 0.954387\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108727, acc: 0.954420\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108649, acc: 0.954452\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108571, acc: 0.954485\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108494, acc: 0.954517\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108417, acc: 0.954550\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109052, acc: 0.953870\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108974, acc: 0.953902\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108897, acc: 0.953935\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108819, acc: 0.953968\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109453, acc: 0.953930\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.953962\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109297, acc: 0.953995\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109929, acc: 0.953673\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109851, acc: 0.953706\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109773, acc: 0.953739\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109696, acc: 0.953771\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110325, acc: 0.953733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110247, acc: 0.953766\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110169, acc: 0.953799\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110092, acc: 0.953831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110014, acc: 0.953864\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109937, acc: 0.953896\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109859, acc: 0.953929\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110486, acc: 0.953609\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110408, acc: 0.953642\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110330, acc: 0.953675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110253, acc: 0.953707\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110175, acc: 0.953740\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110098, acc: 0.953772\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110021, acc: 0.953804\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109944, acc: 0.953837\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109867, acc: 0.953869\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110490, acc: 0.953552\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110412, acc: 0.953584\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110335, acc: 0.953617\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110258, acc: 0.953649\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110181, acc: 0.953681\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110105, acc: 0.953714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110028, acc: 0.953746\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109951, acc: 0.953778\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109875, acc: 0.953810\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109798, acc: 0.953842\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109722, acc: 0.953874\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109646, acc: 0.953906\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109570, acc: 0.953938\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109494, acc: 0.953970\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109418, acc: 0.954002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109343, acc: 0.954034\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109267, acc: 0.954066\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109191, acc: 0.954097\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109116, acc: 0.954129\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109041, acc: 0.954161\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109655, acc: 0.954116\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109580, acc: 0.954147\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109504, acc: 0.954179\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.110117, acc: 0.953522\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110041, acc: 0.953554\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109966, acc: 0.953586\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.953618\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109815, acc: 0.953650\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109739, acc: 0.953682\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109664, acc: 0.953713\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109589, acc: 0.953745\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109514, acc: 0.953777\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109439, acc: 0.953808\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109364, acc: 0.953840\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109973, acc: 0.953188\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109898, acc: 0.953220\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109823, acc: 0.953252\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109748, acc: 0.953284\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109673, acc: 0.953316\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109598, acc: 0.953348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109524, acc: 0.953379\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109449, acc: 0.953411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.953443\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109301, acc: 0.953474\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109227, acc: 0.953506\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109153, acc: 0.953538\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109079, acc: 0.953569\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109005, acc: 0.953600\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108931, acc: 0.953632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108857, acc: 0.953663\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108784, acc: 0.953694\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109386, acc: 0.953651\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109312, acc: 0.953682\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109238, acc: 0.953713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109164, acc: 0.953744\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109091, acc: 0.953776\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109017, acc: 0.953807\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108944, acc: 0.953838\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108871, acc: 0.953869\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108798, acc: 0.953900\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108725, acc: 0.953931\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109323, acc: 0.953291\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109249, acc: 0.953322\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109846, acc: 0.953287\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109772, acc: 0.953318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109699, acc: 0.953349\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109626, acc: 0.953380\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109552, acc: 0.953411\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109479, acc: 0.953442\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109406, acc: 0.953473\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109333, acc: 0.953504\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109927, acc: 0.953461\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109854, acc: 0.953492\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109780, acc: 0.953523\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109707, acc: 0.953554\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109635, acc: 0.953585\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109562, acc: 0.953616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109489, acc: 0.953647\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109416, acc: 0.953677\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109344, acc: 0.953708\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109272, acc: 0.953739\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109199, acc: 0.953769\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109127, acc: 0.953800\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109055, acc: 0.953831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108983, acc: 0.953861\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108911, acc: 0.953892\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108839, acc: 0.953922\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108767, acc: 0.953952\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.953983\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108624, acc: 0.954013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108553, acc: 0.954043\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108481, acc: 0.954073\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108410, acc: 0.954104\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108339, acc: 0.954134\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108268, acc: 0.954164\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108197, acc: 0.954194\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108126, acc: 0.954224\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108055, acc: 0.954254\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107984, acc: 0.954284\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107914, acc: 0.954314\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107843, acc: 0.954344\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108426, acc: 0.954210\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108355, acc: 0.954240\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108937, acc: 0.954205\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108866, acc: 0.954234\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108795, acc: 0.954264\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108724, acc: 0.954294\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108653, acc: 0.954324\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108583, acc: 0.954353\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108512, acc: 0.954383\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108442, acc: 0.954413\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108371, acc: 0.954442\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108949, acc: 0.954310\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108879, acc: 0.954339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108808, acc: 0.954369\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108738, acc: 0.954398\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109314, acc: 0.954363\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109244, acc: 0.954393\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109173, acc: 0.954422\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109103, acc: 0.954452\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109032, acc: 0.954481\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108962, acc: 0.954510\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108892, acc: 0.954540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108822, acc: 0.954569\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108752, acc: 0.954598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108682, acc: 0.954627\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108612, acc: 0.954657\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108542, acc: 0.954686\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108472, acc: 0.954715\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109044, acc: 0.954680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108974, acc: 0.954709\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108905, acc: 0.954738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108835, acc: 0.954767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108765, acc: 0.954796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.954825\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108626, acc: 0.954853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108557, acc: 0.954882\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108488, acc: 0.954911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108418, acc: 0.954940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108349, acc: 0.954969\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108280, acc: 0.954997\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108211, acc: 0.955026\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108142, acc: 0.955054\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108074, acc: 0.955083\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108005, acc: 0.955112\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107937, acc: 0.955140\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107868, acc: 0.955169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107800, acc: 0.955197\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107731, acc: 0.955225\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108296, acc: 0.955043\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108228, acc: 0.955071\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108792, acc: 0.954783\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108723, acc: 0.954812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108654, acc: 0.954840\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108586, acc: 0.954869\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108517, acc: 0.954897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108449, acc: 0.954926\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108381, acc: 0.954954\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108312, acc: 0.954983\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108244, acc: 0.955011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108176, acc: 0.955039\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108737, acc: 0.955005\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108668, acc: 0.955033\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108600, acc: 0.955061\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108532, acc: 0.955089\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108464, acc: 0.955117\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108396, acc: 0.955146\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108328, acc: 0.955174\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108260, acc: 0.955202\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108193, acc: 0.955230\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108125, acc: 0.955258\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108057, acc: 0.955286\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107990, acc: 0.955314\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108546, acc: 0.955030\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108479, acc: 0.955058\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108411, acc: 0.955086\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108344, acc: 0.955114\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108276, acc: 0.955141\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108209, acc: 0.955169\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108142, acc: 0.955197\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.955122\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108628, acc: 0.955149\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108561, acc: 0.955177\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108493, acc: 0.955205\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108426, acc: 0.955233\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108359, acc: 0.955260\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108292, acc: 0.955288\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108225, acc: 0.955316\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108158, acc: 0.955343\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108091, acc: 0.955371\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108025, acc: 0.955399\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107958, acc: 0.955426\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108508, acc: 0.955299\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108441, acc: 0.955327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108374, acc: 0.955354\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108308, acc: 0.955382\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108241, acc: 0.955409\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108175, acc: 0.955437\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.955464\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108042, acc: 0.955491\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108589, acc: 0.955451\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108522, acc: 0.955478\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108456, acc: 0.955505\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108389, acc: 0.955532\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108323, acc: 0.955560\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108257, acc: 0.955587\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108191, acc: 0.955614\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108125, acc: 0.955641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108059, acc: 0.955668\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107993, acc: 0.955695\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107927, acc: 0.955722\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107861, acc: 0.955749\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108404, acc: 0.955573\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108338, acc: 0.955600\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108273, acc: 0.955627\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108207, acc: 0.955654\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108141, acc: 0.955681\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108075, acc: 0.955708\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108010, acc: 0.955735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107944, acc: 0.955762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107879, acc: 0.955789\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107813, acc: 0.955815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107748, acc: 0.955842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107683, acc: 0.955869\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107618, acc: 0.955896\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107553, acc: 0.955922\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107488, acc: 0.955949\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107423, acc: 0.955975\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107358, acc: 0.956002\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107294, acc: 0.956028\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107229, acc: 0.956055\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107164, acc: 0.956081\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107100, acc: 0.956108\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107035, acc: 0.956134\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106971, acc: 0.956161\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106907, acc: 0.956187\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106843, acc: 0.956213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106779, acc: 0.956239\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106715, acc: 0.956266\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106651, acc: 0.956292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106587, acc: 0.956318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106523, acc: 0.956344\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106459, acc: 0.956370\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106396, acc: 0.956396\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106332, acc: 0.956422\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106269, acc: 0.956448\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106205, acc: 0.956474\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106142, acc: 0.956500\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106079, acc: 0.956526\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106015, acc: 0.956552\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105952, acc: 0.956578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105889, acc: 0.956604\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105826, acc: 0.956630\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105764, acc: 0.956655\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105701, acc: 0.956681\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105638, acc: 0.956707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105575, acc: 0.956733\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105513, acc: 0.956758\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105450, acc: 0.956784\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105388, acc: 0.956809\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105325, acc: 0.956835\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.956861\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105201, acc: 0.956886\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105139, acc: 0.956912\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105077, acc: 0.956937\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105015, acc: 0.956962\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104953, acc: 0.956988\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104891, acc: 0.957013\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104829, acc: 0.957038\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104768, acc: 0.957064\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104706, acc: 0.957089\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104644, acc: 0.957114\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104583, acc: 0.957139\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104521, acc: 0.957165\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104460, acc: 0.957190\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104399, acc: 0.957215\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104338, acc: 0.957240\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104277, acc: 0.957265\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104801, acc: 0.957095\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104740, acc: 0.957120\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104678, acc: 0.957145\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104617, acc: 0.957170\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104556, acc: 0.957195\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104495, acc: 0.957220\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105018, acc: 0.957099\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104956, acc: 0.957124\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104895, acc: 0.957149\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104834, acc: 0.957174\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104773, acc: 0.957199\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104712, acc: 0.957224\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105233, acc: 0.956958\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105752, acc: 0.956919\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105691, acc: 0.956944\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105630, acc: 0.956969\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105568, acc: 0.956993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105507, acc: 0.957018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105446, acc: 0.957043\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105385, acc: 0.957068\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105324, acc: 0.957093\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957118\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105202, acc: 0.957143\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.105719, acc: 0.956590\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105658, acc: 0.956615\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105597, acc: 0.956640\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105536, acc: 0.956665\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105476, acc: 0.956690\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105991, acc: 0.956427\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106505, acc: 0.956394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106444, acc: 0.956419\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106383, acc: 0.956444\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106322, acc: 0.956469\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106261, acc: 0.956494\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106774, acc: 0.956456\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106713, acc: 0.956481\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106651, acc: 0.956506\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106590, acc: 0.956530\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106529, acc: 0.956555\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106468, acc: 0.956580\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106407, acc: 0.956605\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106346, acc: 0.956630\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106286, acc: 0.956655\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106225, acc: 0.956679\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106164, acc: 0.956704\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106104, acc: 0.956729\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106043, acc: 0.956753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105983, acc: 0.956778\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105923, acc: 0.956803\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105862, acc: 0.956827\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105802, acc: 0.956852\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105742, acc: 0.956876\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105682, acc: 0.956901\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105622, acc: 0.956925\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105562, acc: 0.956950\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105502, acc: 0.956974\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105442, acc: 0.956999\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105382, acc: 0.957023\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105323, acc: 0.957047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957072\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105204, acc: 0.957096\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105144, acc: 0.957120\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105085, acc: 0.957144\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105025, acc: 0.957169\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104966, acc: 0.957193\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104907, acc: 0.957217\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105411, acc: 0.956959\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105352, acc: 0.956983\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105293, acc: 0.957008\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105234, acc: 0.957032\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105174, acc: 0.957056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105115, acc: 0.957080\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105056, acc: 0.957104\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104997, acc: 0.957128\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104938, acc: 0.957152\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104879, acc: 0.957176\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104821, acc: 0.957200\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104762, acc: 0.957224\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104703, acc: 0.957248\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104645, acc: 0.957272\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105145, acc: 0.957226\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105087, acc: 0.957250\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105028, acc: 0.957274\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104969, acc: 0.957298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104911, acc: 0.957322\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105410, acc: 0.957290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105351, acc: 0.957314\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105850, acc: 0.957282\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105791, acc: 0.957305\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105732, acc: 0.957329\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105673, acc: 0.957353\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105614, acc: 0.957377\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105556, acc: 0.957400\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105497, acc: 0.957424\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105438, acc: 0.957448\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105380, acc: 0.957471\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105322, acc: 0.957495\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957518\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105205, acc: 0.957542\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105147, acc: 0.957565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105088, acc: 0.957589\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105030, acc: 0.957612\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104972, acc: 0.957636\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104914, acc: 0.957659\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104857, acc: 0.957682\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104799, acc: 0.957706\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104741, acc: 0.957729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104683, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104626, acc: 0.957776\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104568, acc: 0.957799\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105061, acc: 0.957547\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105003, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104945, acc: 0.957594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104887, acc: 0.957617\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104830, acc: 0.957640\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104772, acc: 0.957664\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104715, acc: 0.957687\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104658, acc: 0.957710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104600, acc: 0.957733\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104543, acc: 0.957756\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104486, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.104975, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104918, acc: 0.957771\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104861, acc: 0.957794\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104803, acc: 0.957817\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104746, acc: 0.957840\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104689, acc: 0.957863\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105177, acc: 0.957825\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105120, acc: 0.957848\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105063, acc: 0.957871\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105005, acc: 0.957894\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104948, acc: 0.957917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104891, acc: 0.957940\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104834, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104777, acc: 0.957986\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957941\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105206, acc: 0.957963\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105149, acc: 0.957986\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105092, acc: 0.958009\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105035, acc: 0.958032\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105519, acc: 0.957513\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106003, acc: 0.956995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105946, acc: 0.957019\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105889, acc: 0.957042\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105832, acc: 0.957065\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105774, acc: 0.957088\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105717, acc: 0.957111\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105660, acc: 0.957135\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105603, acc: 0.957158\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105547, acc: 0.957181\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105490, acc: 0.957204\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105433, acc: 0.957227\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105376, acc: 0.957250\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105320, acc: 0.957273\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957296\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105743, acc: 0.957259\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105687, acc: 0.957282\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105630, acc: 0.957305\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105573, acc: 0.957328\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105517, acc: 0.957351\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105460, acc: 0.957373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105404, acc: 0.957396\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105348, acc: 0.957419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105291, acc: 0.957442\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105235, acc: 0.957464\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105179, acc: 0.957487\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105123, acc: 0.957510\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105067, acc: 0.957532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105011, acc: 0.957555\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105487, acc: 0.957311\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105431, acc: 0.957334\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105907, acc: 0.957091\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.106383, acc: 0.957060\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106326, acc: 0.957083\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106270, acc: 0.957106\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106213, acc: 0.957129\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106157, acc: 0.957151\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106101, acc: 0.957174\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106045, acc: 0.957197\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105988, acc: 0.957220\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105932, acc: 0.957242\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105876, acc: 0.957265\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105820, acc: 0.957288\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105764, acc: 0.957310\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105708, acc: 0.957333\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.105652, acc: 0.957355\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105597, acc: 0.957378\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105541, acc: 0.957400\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105485, acc: 0.957423\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105430, acc: 0.957445\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105374, acc: 0.957468\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105319, acc: 0.957490\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957512\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105208, acc: 0.957535\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105152, acc: 0.957557\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105097, acc: 0.957579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105042, acc: 0.957602\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.104987, acc: 0.957624\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104932, acc: 0.957646\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104877, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104822, acc: 0.957690\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104767, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104712, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104657, acc: 0.957757\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104603, acc: 0.957779\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.105071, acc: 0.957278\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105016, acc: 0.957301\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104961, acc: 0.957323\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104906, acc: 0.957345\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105373, acc: 0.957315\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105318, acc: 0.957338\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105263, acc: 0.957360\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105208, acc: 0.957382\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105154, acc: 0.957404\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105619, acc: 0.957166\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105564, acc: 0.957188\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106029, acc: 0.956951\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105974, acc: 0.956973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105919, acc: 0.956996\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105864, acc: 0.957018\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105809, acc: 0.957040\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105754, acc: 0.957062\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105699, acc: 0.957085\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105645, acc: 0.957107\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106108, acc: 0.956957\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106053, acc: 0.956979\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105998, acc: 0.957001\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105943, acc: 0.957023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105888, acc: 0.957045\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105834, acc: 0.957068\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105779, acc: 0.957090\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105725, acc: 0.957112\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105670, acc: 0.957134\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105616, acc: 0.957156\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105561, acc: 0.957178\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105507, acc: 0.957200\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105453, acc: 0.957222\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105398, acc: 0.957244\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105344, acc: 0.957266\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105290, acc: 0.957288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105236, acc: 0.957310\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105182, acc: 0.957332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105128, acc: 0.957354\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105074, acc: 0.957376\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105020, acc: 0.957398\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104967, acc: 0.957419\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.104913, acc: 0.957441\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104859, acc: 0.957463\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104806, acc: 0.957485\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104752, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104699, acc: 0.957528\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.104645, acc: 0.957550\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104592, acc: 0.957571\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104539, acc: 0.957593\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104485, acc: 0.957615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104432, acc: 0.957636\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.104379, acc: 0.957658\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104326, acc: 0.957679\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104273, acc: 0.957701\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104220, acc: 0.957722\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104167, acc: 0.957744\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.104114, acc: 0.957765\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104061, acc: 0.957787\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.104008, acc: 0.957808\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.103955, acc: 0.957830\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.103903, acc: 0.957851\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.103850, acc: 0.957872\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.103797, acc: 0.957894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.103745, acc: 0.957915\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.104198, acc: 0.957683\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.104651, acc: 0.957654\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105104, acc: 0.957507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105051, acc: 0.957529\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.104997, acc: 0.957550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.104945, acc: 0.957572\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.104892, acc: 0.957593\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105343, acc: 0.957551\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105290, acc: 0.957573\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105237, acc: 0.957594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105184, acc: 0.957615\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105131, acc: 0.957637\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105581, acc: 0.957602\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105528, acc: 0.957623\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105475, acc: 0.957645\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105422, acc: 0.957666\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105369, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105817, acc: 0.957658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105764, acc: 0.957680\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105711, acc: 0.957701\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105658, acc: 0.957722\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.105606, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105553, acc: 0.957764\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105500, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105447, acc: 0.957806\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.105395, acc: 0.957827\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.105841, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105788, acc: 0.957807\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106234, acc: 0.957662\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106181, acc: 0.957683\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106129, acc: 0.957704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106076, acc: 0.957725\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106023, acc: 0.957746\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106468, acc: 0.957705\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106415, acc: 0.957726\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106362, acc: 0.957747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106309, acc: 0.957768\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106256, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106203, acc: 0.957810\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106151, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106098, acc: 0.957852\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106046, acc: 0.957873\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105993, acc: 0.957894\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105941, acc: 0.957915\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105888, acc: 0.957935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.105836, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105783, acc: 0.957977\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.105731, acc: 0.957998\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.105679, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.105627, acc: 0.958039\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.105575, acc: 0.958060\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.105523, acc: 0.958081\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.105471, acc: 0.958101\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.105911, acc: 0.958073\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.106352, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106299, acc: 0.957622\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106247, acc: 0.957643\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106195, acc: 0.957663\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106634, acc: 0.957193\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106582, acc: 0.957214\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106529, acc: 0.957235\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106968, acc: 0.957158\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106915, acc: 0.957179\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106863, acc: 0.957200\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106810, acc: 0.957221\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106758, acc: 0.957242\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106706, acc: 0.957263\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106654, acc: 0.957283\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106601, acc: 0.957304\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106549, acc: 0.957325\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106497, acc: 0.957346\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106445, acc: 0.957367\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106393, acc: 0.957388\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106341, acc: 0.957408\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106290, acc: 0.957429\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106238, acc: 0.957450\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106186, acc: 0.957471\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106621, acc: 0.957248\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106569, acc: 0.957269\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106518, acc: 0.957290\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106952, acc: 0.957241\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106900, acc: 0.957262\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106848, acc: 0.957282\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106796, acc: 0.957303\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106744, acc: 0.957324\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106693, acc: 0.957345\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107126, acc: 0.957123\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107074, acc: 0.957144\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107022, acc: 0.957164\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107454, acc: 0.956943\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107402, acc: 0.956964\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107350, acc: 0.956985\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107298, acc: 0.957006\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107246, acc: 0.957026\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107195, acc: 0.957047\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.957068\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107574, acc: 0.956847\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107522, acc: 0.956868\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107952, acc: 0.956841\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107900, acc: 0.956861\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107848, acc: 0.956882\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107796, acc: 0.956903\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107744, acc: 0.956924\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107692, acc: 0.956944\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107641, acc: 0.956965\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107589, acc: 0.956986\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107537, acc: 0.957006\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107486, acc: 0.957027\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107434, acc: 0.957048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107383, acc: 0.957068\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107331, acc: 0.957089\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107280, acc: 0.957109\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107228, acc: 0.957130\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107177, acc: 0.957150\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107126, acc: 0.957171\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107075, acc: 0.957191\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107023, acc: 0.957212\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.107450, acc: 0.956755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107399, acc: 0.956775\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107347, acc: 0.956796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107296, acc: 0.956817\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107245, acc: 0.956837\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107194, acc: 0.956858\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.956878\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107092, acc: 0.956899\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107041, acc: 0.956919\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106990, acc: 0.956940\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106939, acc: 0.956960\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106888, acc: 0.956981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106838, acc: 0.957001\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106787, acc: 0.957022\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106736, acc: 0.957042\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106686, acc: 0.957062\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106635, acc: 0.957083\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106585, acc: 0.957103\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.107008, acc: 0.956650\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106957, acc: 0.956670\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107379, acc: 0.956644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107329, acc: 0.956664\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107278, acc: 0.956685\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107227, acc: 0.956705\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107177, acc: 0.956725\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107126, acc: 0.956746\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107075, acc: 0.956766\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107496, acc: 0.956739\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107446, acc: 0.956760\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107395, acc: 0.956780\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107345, acc: 0.956801\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107765, acc: 0.956774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107714, acc: 0.956794\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107663, acc: 0.956815\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107613, acc: 0.956835\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107562, acc: 0.956855\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107512, acc: 0.956875\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107461, acc: 0.956896\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107411, acc: 0.956916\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107361, acc: 0.956936\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107310, acc: 0.956956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107260, acc: 0.956976\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107210, acc: 0.956996\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107160, acc: 0.957017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107109, acc: 0.957037\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107059, acc: 0.957057\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107009, acc: 0.957077\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106959, acc: 0.957097\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106909, acc: 0.957117\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106860, acc: 0.957137\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106810, acc: 0.957157\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106760, acc: 0.957177\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106710, acc: 0.957197\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106660, acc: 0.957217\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106611, acc: 0.957237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106561, acc: 0.957257\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106512, acc: 0.957276\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106927, acc: 0.957245\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107342, acc: 0.957148\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107292, acc: 0.957168\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107242, acc: 0.957188\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107193, acc: 0.957208\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.957228\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107093, acc: 0.957248\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107044, acc: 0.957268\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106994, acc: 0.957287\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106944, acc: 0.957307\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106895, acc: 0.957327\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106846, acc: 0.957347\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106796, acc: 0.957366\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106747, acc: 0.957386\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106697, acc: 0.957406\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106648, acc: 0.957425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106599, acc: 0.957445\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106550, acc: 0.957465\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106501, acc: 0.957484\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106912, acc: 0.957389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106863, acc: 0.957408\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106814, acc: 0.957428\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106765, acc: 0.957447\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106716, acc: 0.957467\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106667, acc: 0.957487\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106618, acc: 0.957506\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107028, acc: 0.957480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106979, acc: 0.957499\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106930, acc: 0.957519\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107339, acc: 0.957487\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107290, acc: 0.957507\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107241, acc: 0.957526\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107192, acc: 0.957546\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.957565\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107094, acc: 0.957585\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107045, acc: 0.957604\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106996, acc: 0.957623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106947, acc: 0.957643\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106898, acc: 0.957662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106849, acc: 0.957681\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106801, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106752, acc: 0.957720\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107159, acc: 0.957511\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107110, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107062, acc: 0.957550\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107013, acc: 0.957569\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106964, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106915, acc: 0.957608\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106867, acc: 0.957627\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106818, acc: 0.957646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106770, acc: 0.957666\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107175, acc: 0.957634\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107127, acc: 0.957654\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107078, acc: 0.957673\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107483, acc: 0.957239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107434, acc: 0.957258\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107386, acc: 0.957277\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107337, acc: 0.957297\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107741, acc: 0.957090\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107692, acc: 0.957109\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107644, acc: 0.957128\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107595, acc: 0.957148\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107998, acc: 0.957111\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107949, acc: 0.957130\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107901, acc: 0.957149\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108303, acc: 0.957124\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108254, acc: 0.957143\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108206, acc: 0.957162\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108607, acc: 0.956731\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108559, acc: 0.956751\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108510, acc: 0.956770\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108461, acc: 0.956789\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108412, acc: 0.956809\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108813, acc: 0.956379\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108764, acc: 0.956398\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108715, acc: 0.956418\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108666, acc: 0.956437\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108618, acc: 0.956457\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108569, acc: 0.956476\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108520, acc: 0.956496\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108472, acc: 0.956516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108423, acc: 0.956535\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108822, acc: 0.956505\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108774, acc: 0.956524\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108725, acc: 0.956544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108676, acc: 0.956563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108628, acc: 0.956582\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108579, acc: 0.956602\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108531, acc: 0.956621\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108482, acc: 0.956641\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108434, acc: 0.956660\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108385, acc: 0.956679\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108337, acc: 0.956699\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108734, acc: 0.956495\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108686, acc: 0.956514\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108638, acc: 0.956534\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108589, acc: 0.956553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108541, acc: 0.956573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108493, acc: 0.956592\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108444, acc: 0.956611\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108396, acc: 0.956630\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108348, acc: 0.956650\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108300, acc: 0.956669\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108252, acc: 0.956688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108204, acc: 0.956707\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108156, acc: 0.956727\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.956746\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108060, acc: 0.956765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108012, acc: 0.956784\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107965, acc: 0.956803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107917, acc: 0.956822\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107869, acc: 0.956841\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107821, acc: 0.956860\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107774, acc: 0.956879\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107726, acc: 0.956898\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107679, acc: 0.956917\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107631, acc: 0.956936\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108025, acc: 0.956515\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107977, acc: 0.956534\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107930, acc: 0.956553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107882, acc: 0.956572\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107835, acc: 0.956591\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107787, acc: 0.956610\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107740, acc: 0.956629\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108132, acc: 0.956600\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108084, acc: 0.956619\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108037, acc: 0.956638\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108428, acc: 0.956602\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108381, acc: 0.956621\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108772, acc: 0.956596\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108724, acc: 0.956615\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109115, acc: 0.956590\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109067, acc: 0.956609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109019, acc: 0.956628\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108972, acc: 0.956647\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108924, acc: 0.956666\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108876, acc: 0.956685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108829, acc: 0.956704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108781, acc: 0.956723\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108734, acc: 0.956742\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108686, acc: 0.956761\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108639, acc: 0.956780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108591, acc: 0.956798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108544, acc: 0.956817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108497, acc: 0.956836\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108885, acc: 0.956782\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109273, acc: 0.956758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109225, acc: 0.956776\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109178, acc: 0.956795\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109565, acc: 0.956379\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109518, acc: 0.956398\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109470, acc: 0.956417\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109422, acc: 0.956436\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.956455\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109328, acc: 0.956474\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109280, acc: 0.956493\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109233, acc: 0.956512\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109185, acc: 0.956530\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109138, acc: 0.956549\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109091, acc: 0.956568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109044, acc: 0.956587\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108997, acc: 0.956606\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108949, acc: 0.956624\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108902, acc: 0.956643\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108855, acc: 0.956662\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108808, acc: 0.956681\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108761, acc: 0.956699\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108714, acc: 0.956718\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108668, acc: 0.956737\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108621, acc: 0.956755\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108574, acc: 0.956774\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108527, acc: 0.956793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108480, acc: 0.956811\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108434, acc: 0.956830\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108387, acc: 0.956848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108340, acc: 0.956867\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108294, acc: 0.956885\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108247, acc: 0.956904\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108201, acc: 0.956922\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108584, acc: 0.956898\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108537, acc: 0.956916\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108491, acc: 0.956935\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108444, acc: 0.956953\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108398, acc: 0.956972\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108779, acc: 0.956776\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108733, acc: 0.956795\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108686, acc: 0.956813\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109068, acc: 0.956618\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109021, acc: 0.956636\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108974, acc: 0.956655\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108928, acc: 0.956673\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108881, acc: 0.956692\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108835, acc: 0.956710\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108788, acc: 0.956729\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108742, acc: 0.956747\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.956766\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108649, acc: 0.956784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108603, acc: 0.956802\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108557, acc: 0.956821\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108511, acc: 0.956839\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108890, acc: 0.956815\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108844, acc: 0.956833\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108797, acc: 0.956852\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108751, acc: 0.956870\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109130, acc: 0.956846\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109083, acc: 0.956864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109037, acc: 0.956883\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108991, acc: 0.956901\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109368, acc: 0.956877\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109746, acc: 0.956853\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109699, acc: 0.956871\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109653, acc: 0.956889\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109606, acc: 0.956907\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109983, acc: 0.956714\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109937, acc: 0.956732\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.956751\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110266, acc: 0.956685\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110220, acc: 0.956703\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110173, acc: 0.956721\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110549, acc: 0.956599\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110502, acc: 0.956617\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110455, acc: 0.956635\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110409, acc: 0.956654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110362, acc: 0.956672\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110316, acc: 0.956690\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110269, acc: 0.956708\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110223, acc: 0.956726\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110177, acc: 0.956745\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110130, acc: 0.956763\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110084, acc: 0.956781\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110038, acc: 0.956799\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110411, acc: 0.956398\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110365, acc: 0.956416\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110319, acc: 0.956434\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110273, acc: 0.956452\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110226, acc: 0.956471\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110180, acc: 0.956489\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110134, acc: 0.956507\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110088, acc: 0.956525\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110042, acc: 0.956543\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110414, acc: 0.956509\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110368, acc: 0.956528\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110740, acc: 0.956462\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110693, acc: 0.956480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110647, acc: 0.956498\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110601, acc: 0.956517\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110555, acc: 0.956535\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110509, acc: 0.956553\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110463, acc: 0.956571\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110417, acc: 0.956589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110371, acc: 0.956607\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110325, acc: 0.956625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110279, acc: 0.956643\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110649, acc: 0.956620\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110603, acc: 0.956638\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110557, acc: 0.956656\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110926, acc: 0.956622\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111296, acc: 0.956594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111249, acc: 0.956612\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111203, acc: 0.956630\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111157, acc: 0.956648\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.956666\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111065, acc: 0.956684\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111019, acc: 0.956702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110973, acc: 0.956720\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110927, acc: 0.956737\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110881, acc: 0.956755\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110835, acc: 0.956773\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110790, acc: 0.956791\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110744, acc: 0.956809\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.956758\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111065, acc: 0.956776\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111019, acc: 0.956794\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.956812\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110928, acc: 0.956829\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110882, acc: 0.956847\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111248, acc: 0.956659\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111203, acc: 0.956677\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111157, acc: 0.956695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.956712\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111065, acc: 0.956730\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.956748\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.956766\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111339, acc: 0.956742\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111294, acc: 0.956760\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111248, acc: 0.956778\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111202, acc: 0.956796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111157, acc: 0.956813\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.956831\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.956849\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.956866\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110975, acc: 0.956884\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110929, acc: 0.956902\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110884, acc: 0.956919\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110838, acc: 0.956937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110793, acc: 0.956955\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110748, acc: 0.956972\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110703, acc: 0.956990\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.956962\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.956980\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110975, acc: 0.956997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110930, acc: 0.957015\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110885, acc: 0.957032\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110839, acc: 0.957050\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110794, acc: 0.957067\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110749, acc: 0.957085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110704, acc: 0.957102\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110659, acc: 0.957120\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110614, acc: 0.957137\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110569, acc: 0.957154\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110524, acc: 0.957172\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110479, acc: 0.957189\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110434, acc: 0.957207\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110390, acc: 0.957224\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110345, acc: 0.957241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110300, acc: 0.957259\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110255, acc: 0.957276\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110616, acc: 0.957248\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110571, acc: 0.957266\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110526, acc: 0.957283\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110482, acc: 0.957300\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110437, acc: 0.957318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110392, acc: 0.957335\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110348, acc: 0.957352\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110303, acc: 0.957369\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110258, acc: 0.957386\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110214, acc: 0.957404\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110169, acc: 0.957421\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110125, acc: 0.957438\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110081, acc: 0.957455\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110036, acc: 0.957472\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109992, acc: 0.957489\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109948, acc: 0.957507\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109903, acc: 0.957524\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109859, acc: 0.957541\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109815, acc: 0.957558\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109771, acc: 0.957575\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109727, acc: 0.957592\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109683, acc: 0.957609\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109639, acc: 0.957626\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109595, acc: 0.957643\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109551, acc: 0.957660\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109507, acc: 0.957677\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109463, acc: 0.957694\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109419, acc: 0.957711\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109331, acc: 0.957745\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109688, acc: 0.957722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109644, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109600, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109556, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109912, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109868, acc: 0.957756\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.110224, acc: 0.957374\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110180, acc: 0.957391\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110136, acc: 0.957408\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110092, acc: 0.957425\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110048, acc: 0.957442\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110004, acc: 0.957459\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109960, acc: 0.957476\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109916, acc: 0.957493\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109873, acc: 0.957510\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109829, acc: 0.957526\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109785, acc: 0.957543\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109742, acc: 0.957560\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109698, acc: 0.957577\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109654, acc: 0.957594\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109611, acc: 0.957611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109567, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109524, acc: 0.957644\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109480, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109437, acc: 0.957678\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109394, acc: 0.957695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109350, acc: 0.957712\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109307, acc: 0.957728\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109264, acc: 0.957745\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109220, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109573, acc: 0.957722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109529, acc: 0.957739\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109486, acc: 0.957755\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109838, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109795, acc: 0.957591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109751, acc: 0.957608\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110103, acc: 0.957585\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110059, acc: 0.957602\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110016, acc: 0.957619\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109972, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109929, acc: 0.957652\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109886, acc: 0.957669\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109843, acc: 0.957685\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109799, acc: 0.957702\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109756, acc: 0.957719\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109713, acc: 0.957735\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110063, acc: 0.957708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110020, acc: 0.957725\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109976, acc: 0.957742\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109933, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109847, acc: 0.957791\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109804, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109761, acc: 0.957824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109718, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109675, acc: 0.957857\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109632, acc: 0.957874\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109589, acc: 0.957890\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109546, acc: 0.957907\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109503, acc: 0.957923\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109461, acc: 0.957940\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109418, acc: 0.957956\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.957973\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109332, acc: 0.957989\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109290, acc: 0.958005\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109247, acc: 0.958022\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109204, acc: 0.958038\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109162, acc: 0.958055\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109509, acc: 0.958032\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109466, acc: 0.958048\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109424, acc: 0.958065\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109381, acc: 0.958081\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109339, acc: 0.958097\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109296, acc: 0.958114\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109253, acc: 0.958130\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109211, acc: 0.958146\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109169, acc: 0.958162\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109126, acc: 0.958179\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109084, acc: 0.958195\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109042, acc: 0.958211\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108999, acc: 0.958227\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108957, acc: 0.958243\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108915, acc: 0.958260\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108873, acc: 0.958276\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108830, acc: 0.958292\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108788, acc: 0.958308\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108746, acc: 0.958324\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108704, acc: 0.958340\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108662, acc: 0.958357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108620, acc: 0.958373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108578, acc: 0.958389\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108922, acc: 0.958350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108880, acc: 0.958366\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108838, acc: 0.958382\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108796, acc: 0.958398\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108754, acc: 0.958414\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108712, acc: 0.958430\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108671, acc: 0.958446\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108629, acc: 0.958462\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108972, acc: 0.958430\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108930, acc: 0.958446\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108888, acc: 0.958462\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108846, acc: 0.958478\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108804, acc: 0.958494\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108762, acc: 0.958510\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108721, acc: 0.958526\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108679, acc: 0.958541\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108637, acc: 0.958557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108596, acc: 0.958573\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108554, acc: 0.958589\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108512, acc: 0.958605\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108471, acc: 0.958621\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108429, acc: 0.958637\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108388, acc: 0.958653\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108346, acc: 0.958668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108305, acc: 0.958684\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108263, acc: 0.958700\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108222, acc: 0.958716\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108180, acc: 0.958732\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108139, acc: 0.958747\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108480, acc: 0.958381\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108438, acc: 0.958397\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108397, acc: 0.958413\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108356, acc: 0.958429\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108314, acc: 0.958445\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108273, acc: 0.958461\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108232, acc: 0.958476\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108571, acc: 0.958302\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108530, acc: 0.958318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108489, acc: 0.958333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108447, acc: 0.958349\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108406, acc: 0.958365\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108365, acc: 0.958381\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108324, acc: 0.958397\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108283, acc: 0.958413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108242, acc: 0.958428\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108200, acc: 0.958444\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108159, acc: 0.958460\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108498, acc: 0.958438\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108457, acc: 0.958454\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108415, acc: 0.958469\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108374, acc: 0.958485\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108333, acc: 0.958501\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108292, acc: 0.958516\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108251, acc: 0.958532\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108210, acc: 0.958548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108169, acc: 0.958564\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108129, acc: 0.958579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108088, acc: 0.958595\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108047, acc: 0.958611\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108006, acc: 0.958626\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107965, acc: 0.958642\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107925, acc: 0.958657\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107884, acc: 0.958673\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107843, acc: 0.958689\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107802, acc: 0.958704\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108139, acc: 0.958531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108098, acc: 0.958547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108057, acc: 0.958563\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108017, acc: 0.958578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107976, acc: 0.958594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107935, acc: 0.958609\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107895, acc: 0.958625\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107854, acc: 0.958640\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107814, acc: 0.958656\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107773, acc: 0.958671\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107733, acc: 0.958687\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107692, acc: 0.958702\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107652, acc: 0.958718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107612, acc: 0.958733\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107571, acc: 0.958749\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107531, acc: 0.958764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107491, acc: 0.958780\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107450, acc: 0.958795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107410, acc: 0.958811\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107370, acc: 0.958826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107330, acc: 0.958841\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107290, acc: 0.958857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107250, acc: 0.958872\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107210, acc: 0.958888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107170, acc: 0.958903\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107130, acc: 0.958918\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107090, acc: 0.958934\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107050, acc: 0.958949\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107010, acc: 0.958964\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106970, acc: 0.958980\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106930, acc: 0.958995\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106890, acc: 0.959010\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106850, acc: 0.959025\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106811, acc: 0.959041\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106771, acc: 0.959056\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106731, acc: 0.959071\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106691, acc: 0.959086\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106652, acc: 0.959101\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106612, acc: 0.959117\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106573, acc: 0.959132\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106533, acc: 0.959147\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106494, acc: 0.959162\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106454, acc: 0.959177\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106415, acc: 0.959192\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106375, acc: 0.959208\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106336, acc: 0.959223\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.106667, acc: 0.959201\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106627, acc: 0.959216\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106588, acc: 0.959231\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.106918, acc: 0.959209\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106879, acc: 0.959224\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107209, acc: 0.959198\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107169, acc: 0.959213\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107130, acc: 0.959228\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107090, acc: 0.959243\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107051, acc: 0.959258\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107011, acc: 0.959273\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106972, acc: 0.959288\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106932, acc: 0.959303\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106893, acc: 0.959318\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106853, acc: 0.959333\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106814, acc: 0.959348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106775, acc: 0.959363\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106735, acc: 0.959378\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106696, acc: 0.959393\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106657, acc: 0.959408\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106618, acc: 0.959423\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106578, acc: 0.959438\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106907, acc: 0.959392\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106867, acc: 0.959407\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106828, acc: 0.959422\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106789, acc: 0.959436\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106750, acc: 0.959451\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106711, acc: 0.959466\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106672, acc: 0.959481\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106632, acc: 0.959496\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106593, acc: 0.959511\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106554, acc: 0.959526\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106515, acc: 0.959540\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106476, acc: 0.959555\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106437, acc: 0.959570\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106399, acc: 0.959585\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106360, acc: 0.959600\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106321, acc: 0.959614\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106282, acc: 0.959629\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106243, acc: 0.959644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106204, acc: 0.959658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106166, acc: 0.959673\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106127, acc: 0.959688\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106453, acc: 0.959581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106414, acc: 0.959596\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106375, acc: 0.959611\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106336, acc: 0.959625\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106298, acc: 0.959640\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106259, acc: 0.959655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106220, acc: 0.959669\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106545, acc: 0.959320\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106507, acc: 0.959335\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106468, acc: 0.959350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106429, acc: 0.959365\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106391, acc: 0.959379\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106352, acc: 0.959394\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106313, acc: 0.959409\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106275, acc: 0.959424\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106236, acc: 0.959438\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106198, acc: 0.959453\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106159, acc: 0.959468\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106121, acc: 0.959482\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106083, acc: 0.959497\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106044, acc: 0.959512\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106006, acc: 0.959526\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106329, acc: 0.959179\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106291, acc: 0.959194\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.106614, acc: 0.958847\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106575, acc: 0.958862\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106537, acc: 0.958877\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106498, acc: 0.958892\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106821, acc: 0.958787\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.958756\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107104, acc: 0.958771\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107066, acc: 0.958786\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107027, acc: 0.958801\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106988, acc: 0.958816\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106950, acc: 0.958831\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106911, acc: 0.958845\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.106873, acc: 0.958860\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.106835, acc: 0.958875\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106796, acc: 0.958890\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106758, acc: 0.958905\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106719, acc: 0.958919\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106681, acc: 0.958934\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107002, acc: 0.958909\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107322, acc: 0.958565\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107284, acc: 0.958580\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107604, acc: 0.958415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107565, acc: 0.958430\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107527, acc: 0.958445\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107488, acc: 0.958460\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107450, acc: 0.958475\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107411, acc: 0.958490\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107373, acc: 0.958504\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107335, acc: 0.958519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107296, acc: 0.958534\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107258, acc: 0.958549\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107219, acc: 0.958564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107181, acc: 0.958579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107143, acc: 0.958593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107105, acc: 0.958608\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107066, acc: 0.958623\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107028, acc: 0.958638\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106990, acc: 0.958652\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106952, acc: 0.958667\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106914, acc: 0.958682\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.106876, acc: 0.958697\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106838, acc: 0.958711\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106800, acc: 0.958726\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106762, acc: 0.958741\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.106724, acc: 0.958755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106686, acc: 0.958770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106648, acc: 0.958785\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.106610, acc: 0.958799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.106572, acc: 0.958814\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106889, acc: 0.958651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.106851, acc: 0.958666\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.106813, acc: 0.958680\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.106775, acc: 0.958695\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.106738, acc: 0.958710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.106700, acc: 0.958724\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107016, acc: 0.958704\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.107333, acc: 0.958364\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107295, acc: 0.958379\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107257, acc: 0.958393\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107573, acc: 0.958373\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107534, acc: 0.958388\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107850, acc: 0.958363\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107812, acc: 0.958378\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107774, acc: 0.958392\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107736, acc: 0.958407\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107698, acc: 0.958422\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107660, acc: 0.958436\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107622, acc: 0.958451\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107937, acc: 0.958430\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107898, acc: 0.958445\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107860, acc: 0.958460\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107822, acc: 0.958474\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107784, acc: 0.958489\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107746, acc: 0.958504\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107709, acc: 0.958518\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107671, acc: 0.958533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107633, acc: 0.958547\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107595, acc: 0.958562\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107909, acc: 0.958541\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107871, acc: 0.958556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107833, acc: 0.958571\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108146, acc: 0.958546\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.958561\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108070, acc: 0.958575\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108032, acc: 0.958590\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107994, acc: 0.958604\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108307, acc: 0.958443\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108269, acc: 0.958458\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108231, acc: 0.958473\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108193, acc: 0.958487\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108155, acc: 0.958502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108118, acc: 0.958516\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108080, acc: 0.958531\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108042, acc: 0.958545\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108354, acc: 0.958525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108316, acc: 0.958539\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108278, acc: 0.958554\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108240, acc: 0.958568\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108202, acc: 0.958583\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108165, acc: 0.958597\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108127, acc: 0.958612\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108089, acc: 0.958626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108052, acc: 0.958640\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108362, acc: 0.958306\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108325, acc: 0.958321\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108287, acc: 0.958335\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108249, acc: 0.958350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108212, acc: 0.958364\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108522, acc: 0.958340\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108484, acc: 0.958355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108446, acc: 0.958369\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108409, acc: 0.958384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108371, acc: 0.958398\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108333, acc: 0.958413\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108296, acc: 0.958427\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108258, acc: 0.958441\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108221, acc: 0.958456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108183, acc: 0.958470\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108146, acc: 0.958485\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.958499\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108071, acc: 0.958513\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108033, acc: 0.958528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107996, acc: 0.958542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107958, acc: 0.958556\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107921, acc: 0.958571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107884, acc: 0.958585\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107847, acc: 0.958599\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107809, acc: 0.958614\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107772, acc: 0.958628\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107735, acc: 0.958642\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107698, acc: 0.958657\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107660, acc: 0.958671\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107623, acc: 0.958685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107586, acc: 0.958699\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107549, acc: 0.958714\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107512, acc: 0.958728\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107475, acc: 0.958742\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.107782, acc: 0.958722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107745, acc: 0.958736\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107708, acc: 0.958750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107671, acc: 0.958764\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107978, acc: 0.958744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107941, acc: 0.958758\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107904, acc: 0.958773\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107867, acc: 0.958787\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107830, acc: 0.958801\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107793, acc: 0.958815\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108099, acc: 0.958743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108062, acc: 0.958758\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108368, acc: 0.958734\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108673, acc: 0.958633\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108636, acc: 0.958648\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108599, acc: 0.958662\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108562, acc: 0.958676\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108524, acc: 0.958690\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108487, acc: 0.958704\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108450, acc: 0.958718\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108413, acc: 0.958732\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108376, acc: 0.958747\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108339, acc: 0.958761\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108302, acc: 0.958775\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108265, acc: 0.958789\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108228, acc: 0.958803\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108191, acc: 0.958817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108154, acc: 0.958831\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108117, acc: 0.958845\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108080, acc: 0.958859\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108384, acc: 0.958532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108348, acc: 0.958546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108311, acc: 0.958561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108274, acc: 0.958575\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108577, acc: 0.958475\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108540, acc: 0.958489\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108503, acc: 0.958504\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108467, acc: 0.958518\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108430, acc: 0.958532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108393, acc: 0.958546\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108356, acc: 0.958560\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108319, acc: 0.958574\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108282, acc: 0.958588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108246, acc: 0.958602\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108209, acc: 0.958616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108172, acc: 0.958630\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108136, acc: 0.958644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108099, acc: 0.958658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108062, acc: 0.958672\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108026, acc: 0.958686\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107989, acc: 0.958700\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108291, acc: 0.958545\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108254, acc: 0.958559\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108218, acc: 0.958573\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108181, acc: 0.958587\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108145, acc: 0.958601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.958615\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108072, acc: 0.958629\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108035, acc: 0.958643\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108336, acc: 0.958601\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108300, acc: 0.958615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108263, acc: 0.958629\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108564, acc: 0.958530\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108527, acc: 0.958544\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108491, acc: 0.958558\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108454, acc: 0.958572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108418, acc: 0.958586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108381, acc: 0.958600\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108681, acc: 0.958576\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108644, acc: 0.958590\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108608, acc: 0.958604\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108571, acc: 0.958618\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108535, acc: 0.958632\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108498, acc: 0.958646\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108462, acc: 0.958660\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108426, acc: 0.958674\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108389, acc: 0.958688\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108353, acc: 0.958702\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108317, acc: 0.958715\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108280, acc: 0.958729\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108244, acc: 0.958743\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108208, acc: 0.958757\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108171, acc: 0.958771\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108135, acc: 0.958784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108099, acc: 0.958798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108063, acc: 0.958812\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108027, acc: 0.958826\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107991, acc: 0.958840\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107955, acc: 0.958853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107918, acc: 0.958867\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107882, acc: 0.958881\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107846, acc: 0.958895\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107810, acc: 0.958908\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107774, acc: 0.958922\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108072, acc: 0.958602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108036, acc: 0.958616\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108000, acc: 0.958630\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107964, acc: 0.958644\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107928, acc: 0.958657\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107892, acc: 0.958671\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107856, acc: 0.958685\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108153, acc: 0.958588\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108117, acc: 0.958601\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108081, acc: 0.958615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108045, acc: 0.958629\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108009, acc: 0.958643\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107973, acc: 0.958656\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107938, acc: 0.958670\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107902, acc: 0.958684\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107866, acc: 0.958698\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.107830, acc: 0.958711\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.107794, acc: 0.958725\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107759, acc: 0.958739\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107723, acc: 0.958752\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107687, acc: 0.958766\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107652, acc: 0.958780\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.107947, acc: 0.958760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107911, acc: 0.958774\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107876, acc: 0.958788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.107840, acc: 0.958801\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108135, acc: 0.958649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108099, acc: 0.958663\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108394, acc: 0.958644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108358, acc: 0.958657\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108322, acc: 0.958671\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108617, acc: 0.958652\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108581, acc: 0.958665\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108545, acc: 0.958679\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108839, acc: 0.958656\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108803, acc: 0.958670\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108767, acc: 0.958683\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108731, acc: 0.958697\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.958710\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108660, acc: 0.958724\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108624, acc: 0.958738\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108917, acc: 0.958422\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109211, acc: 0.958107\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109175, acc: 0.958121\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109139, acc: 0.958134\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109103, acc: 0.958148\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109067, acc: 0.958162\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109031, acc: 0.958176\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108995, acc: 0.958189\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109288, acc: 0.958039\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109580, acc: 0.957889\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109544, acc: 0.957903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109508, acc: 0.957916\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109472, acc: 0.957930\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109436, acc: 0.957944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109401, acc: 0.957958\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109692, acc: 0.957890\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109656, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109620, acc: 0.957917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109585, acc: 0.957931\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109549, acc: 0.957945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109513, acc: 0.957958\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109477, acc: 0.957972\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109441, acc: 0.957986\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109406, acc: 0.958000\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109370, acc: 0.958013\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109334, acc: 0.958027\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109299, acc: 0.958041\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109263, acc: 0.958054\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109227, acc: 0.958068\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109192, acc: 0.958082\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109156, acc: 0.958095\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109121, acc: 0.958109\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109085, acc: 0.958123\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109049, acc: 0.958136\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109014, acc: 0.958150\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108979, acc: 0.958164\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108943, acc: 0.958177\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108908, acc: 0.958191\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108872, acc: 0.958204\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108837, acc: 0.958218\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108802, acc: 0.958232\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108766, acc: 0.958245\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108731, acc: 0.958259\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.958272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108660, acc: 0.958286\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108625, acc: 0.958299\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108590, acc: 0.958313\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108879, acc: 0.958164\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108844, acc: 0.958178\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108808, acc: 0.958191\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108773, acc: 0.958205\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108738, acc: 0.958218\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108703, acc: 0.958232\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108668, acc: 0.958245\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108956, acc: 0.957936\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108920, acc: 0.957949\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108885, acc: 0.957963\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108850, acc: 0.957976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108815, acc: 0.957990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108780, acc: 0.958003\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108745, acc: 0.958017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108710, acc: 0.958031\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108675, acc: 0.958044\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108640, acc: 0.958058\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108605, acc: 0.958071\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108570, acc: 0.958085\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108535, acc: 0.958098\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109144, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109430, acc: 0.957696\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109395, acc: 0.957710\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109360, acc: 0.957723\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109325, acc: 0.957737\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109290, acc: 0.957750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109254, acc: 0.957764\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109541, acc: 0.957737\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109505, acc: 0.957751\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109470, acc: 0.957765\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109435, acc: 0.957778\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109400, acc: 0.957792\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109365, acc: 0.957805\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109330, acc: 0.957819\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109295, acc: 0.957832\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109580, acc: 0.957686\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109865, acc: 0.957539\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109830, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109795, acc: 0.957566\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109760, acc: 0.957580\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109725, acc: 0.957593\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109690, acc: 0.957607\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109655, acc: 0.957620\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109620, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109585, acc: 0.957647\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109550, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109515, acc: 0.957674\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109480, acc: 0.957688\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109445, acc: 0.957702\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109410, acc: 0.957715\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.957728\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109340, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109305, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109270, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109236, acc: 0.957782\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109201, acc: 0.957796\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109166, acc: 0.957809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109131, acc: 0.957823\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109415, acc: 0.957791\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109380, acc: 0.957804\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109663, acc: 0.957778\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109628, acc: 0.957791\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109593, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109559, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109524, acc: 0.957831\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109489, acc: 0.957845\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109454, acc: 0.957858\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109420, acc: 0.957871\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109385, acc: 0.957885\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109350, acc: 0.957898\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109316, acc: 0.957911\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109598, acc: 0.957893\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109563, acc: 0.957906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109528, acc: 0.957920\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109494, acc: 0.957933\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109459, acc: 0.957946\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109424, acc: 0.957960\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109390, acc: 0.957973\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109671, acc: 0.957947\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109637, acc: 0.957960\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109602, acc: 0.957973\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109567, acc: 0.957987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109533, acc: 0.958000\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109498, acc: 0.958013\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109464, acc: 0.958026\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109429, acc: 0.958040\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109395, acc: 0.958053\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109360, acc: 0.958066\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109326, acc: 0.958079\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109291, acc: 0.958092\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109257, acc: 0.958106\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109223, acc: 0.958119\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109188, acc: 0.958132\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109154, acc: 0.958145\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109119, acc: 0.958158\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109085, acc: 0.958172\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109051, acc: 0.958185\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109017, acc: 0.958198\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108982, acc: 0.958211\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108948, acc: 0.958224\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108914, acc: 0.958237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108880, acc: 0.958250\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108846, acc: 0.958263\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108812, acc: 0.958276\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108777, acc: 0.958290\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108743, acc: 0.958303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108709, acc: 0.958316\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108675, acc: 0.958329\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108641, acc: 0.958342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108607, acc: 0.958355\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108573, acc: 0.958368\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108539, acc: 0.958381\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108818, acc: 0.958238\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108784, acc: 0.958251\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108750, acc: 0.958264\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108716, acc: 0.958277\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108682, acc: 0.958290\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108648, acc: 0.958303\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108926, acc: 0.958160\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108892, acc: 0.958173\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108858, acc: 0.958186\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108824, acc: 0.958199\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108791, acc: 0.958212\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108757, acc: 0.958225\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108723, acc: 0.958238\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108689, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108655, acc: 0.958264\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108932, acc: 0.958121\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108899, acc: 0.958134\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108865, acc: 0.958147\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108831, acc: 0.958160\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108797, acc: 0.958173\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108763, acc: 0.958186\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108729, acc: 0.958199\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108696, acc: 0.958212\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108662, acc: 0.958225\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108628, acc: 0.958238\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108594, acc: 0.958251\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108561, acc: 0.958264\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108527, acc: 0.958277\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108493, acc: 0.958290\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108460, acc: 0.958303\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108426, acc: 0.958316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108393, acc: 0.958329\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108359, acc: 0.958342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108326, acc: 0.958355\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108292, acc: 0.958367\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108259, acc: 0.958380\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108225, acc: 0.958393\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108192, acc: 0.958406\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108158, acc: 0.958419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108125, acc: 0.958432\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108091, acc: 0.958445\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108367, acc: 0.958427\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108333, acc: 0.958439\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108300, acc: 0.958452\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108267, acc: 0.958465\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108233, acc: 0.958478\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108200, acc: 0.958491\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108166, acc: 0.958503\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108133, acc: 0.958516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108100, acc: 0.958529\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108067, acc: 0.958542\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108033, acc: 0.958555\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108000, acc: 0.958567\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.107967, acc: 0.958580\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107934, acc: 0.958593\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108208, acc: 0.958452\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108175, acc: 0.958465\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108141, acc: 0.958477\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.958490\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108075, acc: 0.958503\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108042, acc: 0.958516\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108009, acc: 0.958528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.107975, acc: 0.958541\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.107942, acc: 0.958554\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.108216, acc: 0.958536\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108183, acc: 0.958548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108150, acc: 0.958561\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108116, acc: 0.958574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108083, acc: 0.958587\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108050, acc: 0.958599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108017, acc: 0.958612\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107984, acc: 0.958625\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.107951, acc: 0.958637\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.107918, acc: 0.958650\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.107885, acc: 0.958662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.107852, acc: 0.958675\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108125, acc: 0.958650\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108092, acc: 0.958662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108059, acc: 0.958675\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108026, acc: 0.958687\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108298, acc: 0.958395\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108265, acc: 0.958408\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108232, acc: 0.958420\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108199, acc: 0.958433\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108470, acc: 0.958293\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108437, acc: 0.958306\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.108404, acc: 0.958319\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108371, acc: 0.958331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108338, acc: 0.958344\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108305, acc: 0.958357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108273, acc: 0.958369\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108240, acc: 0.958382\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108207, acc: 0.958395\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108174, acc: 0.958407\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108141, acc: 0.958420\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108108, acc: 0.958433\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.108075, acc: 0.958445\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108042, acc: 0.958458\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108010, acc: 0.958470\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108280, acc: 0.958180\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108247, acc: 0.958192\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108215, acc: 0.958205\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108182, acc: 0.958218\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.108452, acc: 0.958079\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108419, acc: 0.958092\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.108689, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108656, acc: 0.957814\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108623, acc: 0.957827\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108590, acc: 0.957840\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.108558, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.108525, acc: 0.957865\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108492, acc: 0.957878\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.108761, acc: 0.957740\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.108728, acc: 0.957753\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.108998, acc: 0.957732\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108965, acc: 0.957745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.108932, acc: 0.957757\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.108899, acc: 0.957770\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109168, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109135, acc: 0.957765\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109403, acc: 0.957745\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109370, acc: 0.957757\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109639, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109606, acc: 0.957682\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109573, acc: 0.957695\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109540, acc: 0.957708\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109507, acc: 0.957721\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109474, acc: 0.957733\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109441, acc: 0.957746\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109408, acc: 0.957759\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109675, acc: 0.957741\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109643, acc: 0.957754\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109610, acc: 0.957767\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109577, acc: 0.957779\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109544, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109511, acc: 0.957805\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109478, acc: 0.957817\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109445, acc: 0.957830\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109412, acc: 0.957843\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109380, acc: 0.957855\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109347, acc: 0.957868\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109314, acc: 0.957881\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109281, acc: 0.957893\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109249, acc: 0.957906\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109216, acc: 0.957918\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109483, acc: 0.957898\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109450, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109417, acc: 0.957923\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109384, acc: 0.957935\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109650, acc: 0.957873\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109618, acc: 0.957886\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109585, acc: 0.957898\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109552, acc: 0.957911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109520, acc: 0.957924\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109487, acc: 0.957936\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109454, acc: 0.957949\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109422, acc: 0.957961\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109687, acc: 0.957825\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109952, acc: 0.957539\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109920, acc: 0.957552\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109887, acc: 0.957565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109854, acc: 0.957577\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110119, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110086, acc: 0.957573\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110054, acc: 0.957585\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110021, acc: 0.957598\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109988, acc: 0.957611\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109955, acc: 0.957623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109923, acc: 0.957636\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.957648\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109857, acc: 0.957661\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109825, acc: 0.957673\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110089, acc: 0.957538\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110056, acc: 0.957550\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110024, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109991, acc: 0.957575\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110255, acc: 0.957440\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110222, acc: 0.957452\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110190, acc: 0.957465\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110453, acc: 0.957330\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110420, acc: 0.957342\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110388, acc: 0.957355\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110355, acc: 0.957367\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110322, acc: 0.957380\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110290, acc: 0.957393\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110553, acc: 0.957372\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110520, acc: 0.957385\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110487, acc: 0.957398\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110455, acc: 0.957410\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110422, acc: 0.957423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110390, acc: 0.957435\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110357, acc: 0.957448\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110324, acc: 0.957460\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110292, acc: 0.957473\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110259, acc: 0.957486\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110227, acc: 0.957498\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110489, acc: 0.957481\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110457, acc: 0.957494\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110424, acc: 0.957506\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110392, acc: 0.957519\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110359, acc: 0.957531\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110327, acc: 0.957544\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110294, acc: 0.957556\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110262, acc: 0.957569\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110229, acc: 0.957581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110197, acc: 0.957594\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110165, acc: 0.957606\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110132, acc: 0.957618\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110100, acc: 0.957631\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110068, acc: 0.957643\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110035, acc: 0.957656\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110003, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109971, acc: 0.957681\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109938, acc: 0.957693\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109906, acc: 0.957705\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109874, acc: 0.957718\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109842, acc: 0.957730\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109810, acc: 0.957743\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109778, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109745, acc: 0.957767\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109713, acc: 0.957780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109681, acc: 0.957792\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109649, acc: 0.957804\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109909, acc: 0.957780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109877, acc: 0.957792\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109845, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109813, acc: 0.957817\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110073, acc: 0.957793\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110041, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110009, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.110268, acc: 0.957538\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110528, acc: 0.957478\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110496, acc: 0.957490\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110463, acc: 0.957502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110431, acc: 0.957515\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110399, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110658, acc: 0.957394\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110626, acc: 0.957406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110594, acc: 0.957419\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110562, acc: 0.957431\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110529, acc: 0.957444\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110497, acc: 0.957456\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110756, acc: 0.957323\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110724, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110691, acc: 0.957348\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110950, acc: 0.957331\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110918, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110885, acc: 0.957356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110853, acc: 0.957368\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110821, acc: 0.957381\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110789, acc: 0.957393\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110757, acc: 0.957405\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111014, acc: 0.957389\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110982, acc: 0.957401\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110950, acc: 0.957413\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111208, acc: 0.957397\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111175, acc: 0.957409\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111143, acc: 0.957421\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957434\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111079, acc: 0.957446\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111336, acc: 0.957429\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111304, acc: 0.957442\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111272, acc: 0.957454\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111528, acc: 0.957430\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111496, acc: 0.957442\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111753, acc: 0.957423\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111721, acc: 0.957435\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111688, acc: 0.957447\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111656, acc: 0.957460\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111624, acc: 0.957472\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111592, acc: 0.957484\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111848, acc: 0.957460\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111816, acc: 0.957473\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111783, acc: 0.957485\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111751, acc: 0.957497\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111719, acc: 0.957509\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111687, acc: 0.957522\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111655, acc: 0.957534\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111623, acc: 0.957546\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111590, acc: 0.957558\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111558, acc: 0.957570\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111526, acc: 0.957583\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111494, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111749, acc: 0.957578\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111717, acc: 0.957590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111685, acc: 0.957603\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111653, acc: 0.957615\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111621, acc: 0.957627\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111589, acc: 0.957639\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111557, acc: 0.957651\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111525, acc: 0.957663\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111493, acc: 0.957675\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111461, acc: 0.957688\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111429, acc: 0.957700\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111397, acc: 0.957712\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111366, acc: 0.957724\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111334, acc: 0.957736\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111302, acc: 0.957748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111270, acc: 0.957760\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111238, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111206, acc: 0.957784\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111175, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111143, acc: 0.957809\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957821\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111079, acc: 0.957833\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111048, acc: 0.957845\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111016, acc: 0.957857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110984, acc: 0.957869\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110953, acc: 0.957881\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110921, acc: 0.957893\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111174, acc: 0.957869\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111428, acc: 0.957849\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111396, acc: 0.957861\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111364, acc: 0.957873\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111333, acc: 0.957885\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111301, acc: 0.957897\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111269, acc: 0.957909\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111238, acc: 0.957921\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111206, acc: 0.957933\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111174, acc: 0.957945\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111143, acc: 0.957957\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957969\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111080, acc: 0.957981\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111332, acc: 0.957962\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111584, acc: 0.957690\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111553, acc: 0.957702\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111521, acc: 0.957714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111489, acc: 0.957726\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111458, acc: 0.957738\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111426, acc: 0.957750\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111395, acc: 0.957761\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111363, acc: 0.957773\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111615, acc: 0.957644\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111583, acc: 0.957656\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111552, acc: 0.957668\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111520, acc: 0.957680\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111488, acc: 0.957692\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111457, acc: 0.957704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111425, acc: 0.957716\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111394, acc: 0.957728\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111645, acc: 0.957699\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111613, acc: 0.957711\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111582, acc: 0.957723\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111550, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111519, acc: 0.957747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111487, acc: 0.957759\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111456, acc: 0.957771\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111425, acc: 0.957783\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111393, acc: 0.957795\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111362, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111330, acc: 0.957818\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111299, acc: 0.957830\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111268, acc: 0.957842\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111518, acc: 0.957826\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111768, acc: 0.957810\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111737, acc: 0.957821\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111705, acc: 0.957833\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111674, acc: 0.957845\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111642, acc: 0.957857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111611, acc: 0.957869\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111580, acc: 0.957881\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111548, acc: 0.957893\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111517, acc: 0.957904\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111486, acc: 0.957916\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111454, acc: 0.957928\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111423, acc: 0.957940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111392, acc: 0.957952\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111360, acc: 0.957963\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111329, acc: 0.957975\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111578, acc: 0.957959\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111547, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111516, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111485, acc: 0.957994\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111453, acc: 0.958006\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111422, acc: 0.958018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111391, acc: 0.958030\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111360, acc: 0.958041\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111329, acc: 0.958053\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111298, acc: 0.958065\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111266, acc: 0.958077\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111235, acc: 0.958088\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111204, acc: 0.958100\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111173, acc: 0.958112\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111142, acc: 0.958123\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958135\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111080, acc: 0.958147\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111049, acc: 0.958158\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111018, acc: 0.958170\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111266, acc: 0.958042\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111514, acc: 0.958023\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111483, acc: 0.958035\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111452, acc: 0.958046\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111421, acc: 0.958058\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111390, acc: 0.958070\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111359, acc: 0.958081\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111328, acc: 0.958093\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111297, acc: 0.958105\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111266, acc: 0.958116\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111235, acc: 0.958128\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111204, acc: 0.958140\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111173, acc: 0.958151\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111142, acc: 0.958163\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958175\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111080, acc: 0.958186\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111049, acc: 0.958198\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111019, acc: 0.958209\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110988, acc: 0.958221\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110957, acc: 0.958233\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111204, acc: 0.958106\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111173, acc: 0.958117\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111142, acc: 0.958129\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958140\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111080, acc: 0.958152\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111050, acc: 0.958164\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111296, acc: 0.957898\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111265, acc: 0.957910\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111234, acc: 0.957922\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111480, acc: 0.957902\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111449, acc: 0.957914\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111418, acc: 0.957926\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111664, acc: 0.957910\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111633, acc: 0.957921\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111602, acc: 0.957933\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111571, acc: 0.957945\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111541, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111510, acc: 0.957968\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111755, acc: 0.957703\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111724, acc: 0.957715\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111693, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111663, acc: 0.957738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111632, acc: 0.957750\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111601, acc: 0.957762\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111570, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111540, acc: 0.957785\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111509, acc: 0.957797\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111478, acc: 0.957808\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111447, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111417, acc: 0.957831\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111386, acc: 0.957843\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111356, acc: 0.957855\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111600, acc: 0.957775\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111569, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111538, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111508, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111477, acc: 0.957821\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111447, acc: 0.957833\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111416, acc: 0.957844\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111385, acc: 0.957856\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111355, acc: 0.957867\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111324, acc: 0.957879\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111294, acc: 0.957890\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111263, acc: 0.957902\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111233, acc: 0.957913\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111202, acc: 0.957925\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111172, acc: 0.957937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111142, acc: 0.957948\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957960\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111081, acc: 0.957971\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111050, acc: 0.957983\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.957994\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110990, acc: 0.958006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110959, acc: 0.958017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110929, acc: 0.958028\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110899, acc: 0.958040\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110868, acc: 0.958051\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110838, acc: 0.958063\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110808, acc: 0.958074\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110778, acc: 0.958086\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110747, acc: 0.958097\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110717, acc: 0.958109\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110687, acc: 0.958120\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110657, acc: 0.958131\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110627, acc: 0.958143\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110597, acc: 0.958154\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110566, acc: 0.958166\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110536, acc: 0.958177\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110778, acc: 0.958154\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110748, acc: 0.958166\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110718, acc: 0.958177\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110688, acc: 0.958189\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110658, acc: 0.958200\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110628, acc: 0.958211\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110598, acc: 0.958223\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110568, acc: 0.958234\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110538, acc: 0.958245\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110508, acc: 0.958257\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110478, acc: 0.958268\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110448, acc: 0.958279\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110418, acc: 0.958291\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110388, acc: 0.958302\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110358, acc: 0.958313\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110599, acc: 0.958297\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110840, acc: 0.958038\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110810, acc: 0.958049\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110780, acc: 0.958060\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110750, acc: 0.958072\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110720, acc: 0.958083\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110690, acc: 0.958094\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110660, acc: 0.958106\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110630, acc: 0.958117\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110600, acc: 0.958128\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110570, acc: 0.958140\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110541, acc: 0.958151\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110781, acc: 0.958129\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110751, acc: 0.958140\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110721, acc: 0.958151\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110691, acc: 0.958163\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110661, acc: 0.958174\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110631, acc: 0.958185\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110602, acc: 0.958196\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110572, acc: 0.958208\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110542, acc: 0.958219\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110512, acc: 0.958230\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110482, acc: 0.958241\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110722, acc: 0.958226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110692, acc: 0.958237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110662, acc: 0.958248\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110902, acc: 0.958170\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110872, acc: 0.958181\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110842, acc: 0.958192\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110812, acc: 0.958204\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110782, acc: 0.958215\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110753, acc: 0.958226\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110723, acc: 0.958237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110693, acc: 0.958248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110663, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110634, acc: 0.958271\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110604, acc: 0.958282\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110574, acc: 0.958293\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110545, acc: 0.958304\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110515, acc: 0.958316\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110485, acc: 0.958327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110456, acc: 0.958338\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110426, acc: 0.958349\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110665, acc: 0.958226\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110635, acc: 0.958238\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110605, acc: 0.958249\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110576, acc: 0.958260\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110546, acc: 0.958271\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110784, acc: 0.958229\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110754, acc: 0.958240\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110725, acc: 0.958251\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110695, acc: 0.958262\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110666, acc: 0.958273\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110636, acc: 0.958285\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110606, acc: 0.958296\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110577, acc: 0.958307\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110547, acc: 0.958318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110518, acc: 0.958329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110488, acc: 0.958340\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110459, acc: 0.958351\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110429, acc: 0.958362\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110400, acc: 0.958374\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110371, acc: 0.958385\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110341, acc: 0.958396\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110312, acc: 0.958407\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110282, acc: 0.958418\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110253, acc: 0.958429\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110224, acc: 0.958440\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110460, acc: 0.958424\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110431, acc: 0.958436\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110668, acc: 0.958413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110638, acc: 0.958424\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110609, acc: 0.958435\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110579, acc: 0.958447\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110550, acc: 0.958458\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110521, acc: 0.958469\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110491, acc: 0.958480\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110462, acc: 0.958491\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110698, acc: 0.958449\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110669, acc: 0.958460\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110639, acc: 0.958471\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110610, acc: 0.958482\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110581, acc: 0.958493\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110551, acc: 0.958504\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110787, acc: 0.958488\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110758, acc: 0.958499\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110728, acc: 0.958510\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110964, acc: 0.958256\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110935, acc: 0.958267\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110905, acc: 0.958278\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110876, acc: 0.958289\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110847, acc: 0.958300\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110817, acc: 0.958312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110788, acc: 0.958323\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111023, acc: 0.958307\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110994, acc: 0.958318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110964, acc: 0.958329\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110935, acc: 0.958340\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110906, acc: 0.958351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110876, acc: 0.958362\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958109\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111082, acc: 0.958120\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111052, acc: 0.958131\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111023, acc: 0.958142\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110994, acc: 0.958153\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110965, acc: 0.958164\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110935, acc: 0.958175\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110906, acc: 0.958186\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110877, acc: 0.958197\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110848, acc: 0.958208\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110819, acc: 0.958219\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110789, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110760, acc: 0.958241\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110731, acc: 0.958252\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110702, acc: 0.958263\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110673, acc: 0.958274\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110644, acc: 0.958285\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110615, acc: 0.958296\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110586, acc: 0.958307\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110819, acc: 0.958187\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110790, acc: 0.958198\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110761, acc: 0.958209\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110732, acc: 0.958220\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110703, acc: 0.958231\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110674, acc: 0.958242\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110907, acc: 0.958122\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.958002\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111082, acc: 0.958024\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111053, acc: 0.958035\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111024, acc: 0.958046\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110995, acc: 0.958056\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110966, acc: 0.958067\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110937, acc: 0.958078\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110908, acc: 0.958089\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.958035\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958046\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111082, acc: 0.958057\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111053, acc: 0.958068\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111024, acc: 0.958079\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110995, acc: 0.958090\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110966, acc: 0.958101\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111198, acc: 0.957981\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111169, acc: 0.957992\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.958003\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958014\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111343, acc: 0.957764\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111314, acc: 0.957775\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111285, acc: 0.957786\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111256, acc: 0.957797\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111487, acc: 0.957782\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111458, acc: 0.957793\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111429, acc: 0.957804\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111400, acc: 0.957815\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111371, acc: 0.957826\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111342, acc: 0.957837\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111313, acc: 0.957848\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111284, acc: 0.957859\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111256, acc: 0.957870\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111227, acc: 0.957881\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111198, acc: 0.957892\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111169, acc: 0.957903\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.957914\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957925\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111082, acc: 0.957936\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111053, acc: 0.957946\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111025, acc: 0.957957\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110996, acc: 0.957968\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110967, acc: 0.957979\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110938, acc: 0.957990\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111169, acc: 0.957975\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.957986\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111082, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111054, acc: 0.958019\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111284, acc: 0.958001\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111255, acc: 0.958012\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111226, acc: 0.958022\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111197, acc: 0.958033\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111169, acc: 0.958044\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111140, acc: 0.958055\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111370, acc: 0.958034\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111341, acc: 0.958044\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111312, acc: 0.958055\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111283, acc: 0.958066\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111255, acc: 0.958077\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111226, acc: 0.958088\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111455, acc: 0.958073\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111426, acc: 0.958083\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111655, acc: 0.957965\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111627, acc: 0.957976\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111598, acc: 0.957987\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111569, acc: 0.957998\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111540, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111512, acc: 0.958019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111483, acc: 0.958030\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111712, acc: 0.958004\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111683, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111654, acc: 0.958026\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111626, acc: 0.958037\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111597, acc: 0.958048\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111568, acc: 0.958058\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111539, acc: 0.958069\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111768, acc: 0.958054\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111739, acc: 0.958065\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111710, acc: 0.958076\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111682, acc: 0.958086\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111653, acc: 0.958097\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111624, acc: 0.958108\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111596, acc: 0.958119\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111567, acc: 0.958129\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111538, acc: 0.958140\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111510, acc: 0.958151\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111738, acc: 0.958110\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111709, acc: 0.958121\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111680, acc: 0.958132\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111652, acc: 0.958143\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111879, acc: 0.958025\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111851, acc: 0.958036\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111822, acc: 0.958047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111793, acc: 0.958058\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.112020, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111992, acc: 0.957823\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.112219, acc: 0.957806\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112190, acc: 0.957816\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112161, acc: 0.957827\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112133, acc: 0.957838\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.112104, acc: 0.957849\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112076, acc: 0.957860\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.112047, acc: 0.957870\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112018, acc: 0.957881\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111990, acc: 0.957892\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111961, acc: 0.957903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111933, acc: 0.957913\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111904, acc: 0.957924\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111876, acc: 0.957935\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111847, acc: 0.957945\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111819, acc: 0.957956\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111790, acc: 0.957967\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111762, acc: 0.957978\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111733, acc: 0.957988\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111705, acc: 0.957999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111676, acc: 0.958010\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111648, acc: 0.958020\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111620, acc: 0.958031\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111845, acc: 0.958016\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111817, acc: 0.958027\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111789, acc: 0.958038\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111760, acc: 0.958048\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111732, acc: 0.958059\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111703, acc: 0.958069\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111675, acc: 0.958080\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111647, acc: 0.958091\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111618, acc: 0.958101\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111590, acc: 0.958112\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111562, acc: 0.958123\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111534, acc: 0.958133\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111505, acc: 0.958144\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111477, acc: 0.958154\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111449, acc: 0.958165\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111421, acc: 0.958176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111392, acc: 0.958186\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111364, acc: 0.958197\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111589, acc: 0.958081\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111561, acc: 0.958092\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111533, acc: 0.958102\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111504, acc: 0.958113\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111476, acc: 0.958123\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111448, acc: 0.958134\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111420, acc: 0.958144\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111392, acc: 0.958155\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111364, acc: 0.958166\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111336, acc: 0.958176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111307, acc: 0.958187\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111532, acc: 0.958134\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111504, acc: 0.958145\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111728, acc: 0.958119\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111699, acc: 0.958130\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111671, acc: 0.958140\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111643, acc: 0.958151\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111615, acc: 0.958161\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111587, acc: 0.958172\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111811, acc: 0.958155\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111782, acc: 0.958165\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111754, acc: 0.958176\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111726, acc: 0.958186\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111698, acc: 0.958197\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111670, acc: 0.958207\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111642, acc: 0.958218\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111865, acc: 0.958102\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111837, acc: 0.958113\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111809, acc: 0.958124\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111781, acc: 0.958134\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111753, acc: 0.958145\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111725, acc: 0.958155\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111697, acc: 0.958166\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111669, acc: 0.958176\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111641, acc: 0.958187\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111613, acc: 0.958197\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111836, acc: 0.958172\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111807, acc: 0.958182\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111779, acc: 0.958193\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111751, acc: 0.958203\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111723, acc: 0.958214\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111695, acc: 0.958224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111668, acc: 0.958235\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111890, acc: 0.958182\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111862, acc: 0.958193\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111834, acc: 0.958203\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111806, acc: 0.958214\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111778, acc: 0.958224\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111750, acc: 0.958235\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111722, acc: 0.958245\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111944, acc: 0.958231\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111916, acc: 0.958241\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111888, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111860, acc: 0.958262\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112082, acc: 0.958222\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112054, acc: 0.958233\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112026, acc: 0.958243\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111998, acc: 0.958254\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111970, acc: 0.958264\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111942, acc: 0.958274\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111914, acc: 0.958285\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111886, acc: 0.958295\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111858, acc: 0.958306\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111831, acc: 0.958316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111803, acc: 0.958326\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111775, acc: 0.958337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111747, acc: 0.958347\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111719, acc: 0.958358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111692, acc: 0.958368\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111664, acc: 0.958378\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111636, acc: 0.958389\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111608, acc: 0.958399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111581, acc: 0.958409\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111553, acc: 0.958420\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111525, acc: 0.958430\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111497, acc: 0.958440\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111470, acc: 0.958451\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111442, acc: 0.958461\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111414, acc: 0.958471\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111387, acc: 0.958481\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111359, acc: 0.958492\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111332, acc: 0.958502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111304, acc: 0.958512\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111276, acc: 0.958523\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111249, acc: 0.958533\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111221, acc: 0.958543\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111194, acc: 0.958553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111166, acc: 0.958564\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111139, acc: 0.958574\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958584\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111084, acc: 0.958594\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111056, acc: 0.958605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111029, acc: 0.958615\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111001, acc: 0.958625\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.958635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110946, acc: 0.958646\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110919, acc: 0.958656\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110892, acc: 0.958666\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110864, acc: 0.958676\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110837, acc: 0.958686\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110809, acc: 0.958697\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110782, acc: 0.958707\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111001, acc: 0.958594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.958604\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110947, acc: 0.958614\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110919, acc: 0.958624\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110892, acc: 0.958635\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111084, acc: 0.958630\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111056, acc: 0.958640\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111029, acc: 0.958651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111002, acc: 0.958661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.958671\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110947, acc: 0.958681\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111166, acc: 0.958568\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111138, acc: 0.958579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958589\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111330, acc: 0.958353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111302, acc: 0.958363\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111275, acc: 0.958374\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111248, acc: 0.958384\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111466, acc: 0.958363\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111438, acc: 0.958374\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111411, acc: 0.958384\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111384, acc: 0.958394\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111356, acc: 0.958404\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111329, acc: 0.958414\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111302, acc: 0.958425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111275, acc: 0.958435\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111247, acc: 0.958445\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111220, acc: 0.958455\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111438, acc: 0.958441\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111410, acc: 0.958451\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111383, acc: 0.958461\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111356, acc: 0.958471\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111329, acc: 0.958481\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111301, acc: 0.958492\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111274, acc: 0.958502\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111491, acc: 0.958267\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111464, acc: 0.958278\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111437, acc: 0.958288\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111410, acc: 0.958298\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111383, acc: 0.958308\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111355, acc: 0.958318\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111328, acc: 0.958329\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111301, acc: 0.958339\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111274, acc: 0.958349\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111247, acc: 0.958359\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111463, acc: 0.958339\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111436, acc: 0.958349\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111409, acc: 0.958359\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111382, acc: 0.958369\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111355, acc: 0.958379\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111571, acc: 0.958365\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111544, acc: 0.958375\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111760, acc: 0.958355\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111733, acc: 0.958365\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111706, acc: 0.958375\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111679, acc: 0.958385\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111652, acc: 0.958395\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111625, acc: 0.958406\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111597, acc: 0.958416\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111570, acc: 0.958426\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111786, acc: 0.958314\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111759, acc: 0.958325\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111732, acc: 0.958335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111705, acc: 0.958345\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111678, acc: 0.958355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111650, acc: 0.958365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111623, acc: 0.958375\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111596, acc: 0.958385\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111569, acc: 0.958395\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111542, acc: 0.958405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111515, acc: 0.958415\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111488, acc: 0.958426\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111461, acc: 0.958436\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111434, acc: 0.958446\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111407, acc: 0.958456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111380, acc: 0.958466\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111353, acc: 0.958476\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111326, acc: 0.958486\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111299, acc: 0.958496\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111272, acc: 0.958506\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111245, acc: 0.958516\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111219, acc: 0.958526\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111192, acc: 0.958536\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111165, acc: 0.958546\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111138, acc: 0.958556\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958566\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111084, acc: 0.958576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111057, acc: 0.958586\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111031, acc: 0.958596\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111004, acc: 0.958606\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110977, acc: 0.958616\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110950, acc: 0.958626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110924, acc: 0.958636\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110897, acc: 0.958646\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110870, acc: 0.958656\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111084, acc: 0.958546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111058, acc: 0.958556\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111272, acc: 0.958445\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111245, acc: 0.958455\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111218, acc: 0.958465\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111432, acc: 0.958355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111405, acc: 0.958365\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111378, acc: 0.958375\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111352, acc: 0.958385\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111325, acc: 0.958395\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111298, acc: 0.958405\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111271, acc: 0.958415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111245, acc: 0.958425\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111218, acc: 0.958435\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111191, acc: 0.958445\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111164, acc: 0.958455\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111138, acc: 0.958465\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958475\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111084, acc: 0.958485\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111058, acc: 0.958495\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111031, acc: 0.958505\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111005, acc: 0.958514\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110978, acc: 0.958524\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110951, acc: 0.958534\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110925, acc: 0.958544\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110898, acc: 0.958554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110872, acc: 0.958564\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110845, acc: 0.958574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110819, acc: 0.958584\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110792, acc: 0.958594\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110766, acc: 0.958604\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110739, acc: 0.958614\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110713, acc: 0.958624\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110925, acc: 0.958514\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110899, acc: 0.958524\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110872, acc: 0.958534\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110846, acc: 0.958544\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110819, acc: 0.958554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110793, acc: 0.958563\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110766, acc: 0.958573\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110740, acc: 0.958583\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110713, acc: 0.958593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110687, acc: 0.958603\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110661, acc: 0.958613\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110634, acc: 0.958623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110608, acc: 0.958633\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110582, acc: 0.958642\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110555, acc: 0.958652\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110529, acc: 0.958662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110503, acc: 0.958672\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110476, acc: 0.958682\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110450, acc: 0.958692\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110424, acc: 0.958702\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110635, acc: 0.958592\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110609, acc: 0.958602\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110583, acc: 0.958612\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110556, acc: 0.958622\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110768, acc: 0.958608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110741, acc: 0.958618\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110953, acc: 0.958390\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110926, acc: 0.958400\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110900, acc: 0.958410\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958301\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111085, acc: 0.958311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111058, acc: 0.958321\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111032, acc: 0.958331\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111006, acc: 0.958341\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110979, acc: 0.958350\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110953, acc: 0.958360\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110927, acc: 0.958370\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110900, acc: 0.958380\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110874, acc: 0.958390\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110848, acc: 0.958400\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110822, acc: 0.958410\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110795, acc: 0.958419\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110769, acc: 0.958429\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110743, acc: 0.958439\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110717, acc: 0.958449\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110691, acc: 0.958459\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110664, acc: 0.958469\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110638, acc: 0.958478\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110612, acc: 0.958488\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110586, acc: 0.958498\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110560, acc: 0.958508\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110534, acc: 0.958518\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110508, acc: 0.958527\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110482, acc: 0.958537\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110456, acc: 0.958547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110429, acc: 0.958557\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110403, acc: 0.958567\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110377, acc: 0.958576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110351, acc: 0.958586\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110325, acc: 0.958596\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110299, acc: 0.958606\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110273, acc: 0.958615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110247, acc: 0.958625\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110457, acc: 0.958399\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110431, acc: 0.958409\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110405, acc: 0.958419\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110379, acc: 0.958429\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110353, acc: 0.958439\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110327, acc: 0.958448\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110301, acc: 0.958458\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110275, acc: 0.958468\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110249, acc: 0.958478\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110223, acc: 0.958487\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110197, acc: 0.958497\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110171, acc: 0.958507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110146, acc: 0.958517\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110120, acc: 0.958526\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110329, acc: 0.958513\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110303, acc: 0.958522\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110277, acc: 0.958532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110251, acc: 0.958542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110225, acc: 0.958552\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110199, acc: 0.958561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110173, acc: 0.958571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110148, acc: 0.958581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110122, acc: 0.958590\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110096, acc: 0.958600\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110070, acc: 0.958610\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110279, acc: 0.958596\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110253, acc: 0.958606\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110227, acc: 0.958615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110201, acc: 0.958625\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110175, acc: 0.958635\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110384, acc: 0.958619\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110358, acc: 0.958628\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110332, acc: 0.958638\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110306, acc: 0.958648\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110514, acc: 0.958540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110488, acc: 0.958550\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110462, acc: 0.958560\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110437, acc: 0.958569\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110411, acc: 0.958579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110385, acc: 0.958589\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110359, acc: 0.958598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110334, acc: 0.958608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110308, acc: 0.958618\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110282, acc: 0.958627\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110256, acc: 0.958637\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110231, acc: 0.958647\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110205, acc: 0.958656\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110179, acc: 0.958666\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110154, acc: 0.958676\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110128, acc: 0.958685\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110102, acc: 0.958695\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110310, acc: 0.958681\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110284, acc: 0.958691\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110258, acc: 0.958700\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110233, acc: 0.958710\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110207, acc: 0.958720\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110181, acc: 0.958729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110156, acc: 0.958739\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110130, acc: 0.958748\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110105, acc: 0.958758\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110079, acc: 0.958767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110053, acc: 0.958777\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110028, acc: 0.958787\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110002, acc: 0.958796\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109977, acc: 0.958806\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109951, acc: 0.958815\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109926, acc: 0.958825\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109900, acc: 0.958834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109875, acc: 0.958844\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110081, acc: 0.958796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110056, acc: 0.958805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110030, acc: 0.958815\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110005, acc: 0.958824\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109979, acc: 0.958834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109954, acc: 0.958843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109928, acc: 0.958853\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109903, acc: 0.958862\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109877, acc: 0.958872\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109852, acc: 0.958881\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109827, acc: 0.958891\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109801, acc: 0.958900\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109776, acc: 0.958910\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109750, acc: 0.958919\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109725, acc: 0.958929\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109700, acc: 0.958938\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109674, acc: 0.958948\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109649, acc: 0.958957\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109855, acc: 0.958736\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109829, acc: 0.958745\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109804, acc: 0.958755\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110009, acc: 0.958649\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109984, acc: 0.958659\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109959, acc: 0.958668\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109933, acc: 0.958678\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110138, acc: 0.958662\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110113, acc: 0.958671\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110088, acc: 0.958681\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110062, acc: 0.958690\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110037, acc: 0.958700\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110012, acc: 0.958709\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109986, acc: 0.958719\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109961, acc: 0.958728\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109936, acc: 0.958738\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109910, acc: 0.958747\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109885, acc: 0.958757\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109860, acc: 0.958766\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109835, acc: 0.958776\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109809, acc: 0.958785\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110014, acc: 0.958718\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109989, acc: 0.958728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109963, acc: 0.958737\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110168, acc: 0.958724\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110142, acc: 0.958733\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110117, acc: 0.958742\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110092, acc: 0.958752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110066, acc: 0.958761\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110041, acc: 0.958771\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110016, acc: 0.958780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109991, acc: 0.958790\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109966, acc: 0.958799\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109940, acc: 0.958809\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109915, acc: 0.958818\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109890, acc: 0.958827\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109865, acc: 0.958837\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109840, acc: 0.958846\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109815, acc: 0.958856\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109790, acc: 0.958865\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109764, acc: 0.958875\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109739, acc: 0.958884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109714, acc: 0.958893\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109689, acc: 0.958903\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109664, acc: 0.958912\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109639, acc: 0.958922\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109614, acc: 0.958931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109589, acc: 0.958940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109564, acc: 0.958950\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109539, acc: 0.958959\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109514, acc: 0.958968\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109489, acc: 0.958978\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109464, acc: 0.958987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109439, acc: 0.958996\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109414, acc: 0.959006\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109389, acc: 0.959015\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109364, acc: 0.959024\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109339, acc: 0.959034\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109315, acc: 0.959043\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109290, acc: 0.959052\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109265, acc: 0.959062\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109240, acc: 0.959071\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109443, acc: 0.959058\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109418, acc: 0.959067\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109393, acc: 0.959076\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109368, acc: 0.959086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109343, acc: 0.959095\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109318, acc: 0.959104\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109293, acc: 0.959113\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109269, acc: 0.959123\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109244, acc: 0.959132\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109219, acc: 0.959141\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109194, acc: 0.959151\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109169, acc: 0.959160\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109145, acc: 0.959169\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109120, acc: 0.959178\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109095, acc: 0.959188\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109070, acc: 0.959197\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109046, acc: 0.959206\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109021, acc: 0.959215\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109223, acc: 0.958998\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109198, acc: 0.959007\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109173, acc: 0.959017\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109149, acc: 0.959026\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109124, acc: 0.959035\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109099, acc: 0.959044\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109074, acc: 0.959054\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109050, acc: 0.959063\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109025, acc: 0.959072\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109000, acc: 0.959081\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.108976, acc: 0.959091\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109177, acc: 0.959077\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109153, acc: 0.959087\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109128, acc: 0.959096\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109103, acc: 0.959105\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109079, acc: 0.959114\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109280, acc: 0.958898\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.109481, acc: 0.958794\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109456, acc: 0.958804\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109431, acc: 0.958813\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109407, acc: 0.958822\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109382, acc: 0.958831\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109357, acc: 0.958841\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109333, acc: 0.958850\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109308, acc: 0.958859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109283, acc: 0.958868\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109259, acc: 0.958878\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109234, acc: 0.958887\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109210, acc: 0.958896\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109185, acc: 0.958906\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109160, acc: 0.958915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109136, acc: 0.958924\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109111, acc: 0.958933\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109312, acc: 0.958920\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109512, acc: 0.958907\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109487, acc: 0.958916\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109463, acc: 0.958925\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109438, acc: 0.958934\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109414, acc: 0.958944\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109389, acc: 0.958953\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109814, acc: 0.958738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109789, acc: 0.958747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109764, acc: 0.958756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109740, acc: 0.958765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109715, acc: 0.958775\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109690, acc: 0.958784\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109666, acc: 0.958793\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109641, acc: 0.958802\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109617, acc: 0.958812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109592, acc: 0.958821\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109568, acc: 0.958830\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109543, acc: 0.958839\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109518, acc: 0.958848\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109494, acc: 0.958858\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109693, acc: 0.958842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109669, acc: 0.958851\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109644, acc: 0.958860\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109620, acc: 0.958870\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109595, acc: 0.958879\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109571, acc: 0.958888\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109546, acc: 0.958897\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109522, acc: 0.958906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109497, acc: 0.958916\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109473, acc: 0.958925\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109448, acc: 0.958934\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109424, acc: 0.958943\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109399, acc: 0.958952\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109375, acc: 0.958961\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109351, acc: 0.958971\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109326, acc: 0.958980\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109302, acc: 0.958989\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109277, acc: 0.958998\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109253, acc: 0.959007\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109229, acc: 0.959016\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109204, acc: 0.959025\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109180, acc: 0.959035\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109156, acc: 0.959044\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109131, acc: 0.959053\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.109107, acc: 0.959062\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109083, acc: 0.959071\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109059, acc: 0.959080\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109034, acc: 0.959089\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109010, acc: 0.959098\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109208, acc: 0.959076\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109184, acc: 0.959085\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109160, acc: 0.959094\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109358, acc: 0.959081\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109333, acc: 0.959090\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109309, acc: 0.959099\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109285, acc: 0.959108\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109260, acc: 0.959117\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109236, acc: 0.959126\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109212, acc: 0.959135\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109188, acc: 0.959144\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109164, acc: 0.959153\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109139, acc: 0.959162\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109337, acc: 0.959149\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109313, acc: 0.959158\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109288, acc: 0.959167\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109264, acc: 0.959176\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109462, acc: 0.959075\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.109659, acc: 0.958862\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.109635, acc: 0.958871\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109610, acc: 0.958881\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109586, acc: 0.958890\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109562, acc: 0.958899\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.109759, acc: 0.958797\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.109956, acc: 0.958784\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109931, acc: 0.958793\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.109907, acc: 0.958802\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.109883, acc: 0.958811\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.109859, acc: 0.958821\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.109834, acc: 0.958830\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.109810, acc: 0.958839\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109786, acc: 0.958848\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109761, acc: 0.958857\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109958, acc: 0.958838\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.109934, acc: 0.958848\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110130, acc: 0.958832\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110106, acc: 0.958841\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110082, acc: 0.958850\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110057, acc: 0.958859\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110254, acc: 0.958758\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110229, acc: 0.958767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110205, acc: 0.958776\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110181, acc: 0.958785\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110156, acc: 0.958795\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110132, acc: 0.958804\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110108, acc: 0.958813\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110084, acc: 0.958822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110059, acc: 0.958831\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110255, acc: 0.958815\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110231, acc: 0.958824\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110207, acc: 0.958834\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110183, acc: 0.958843\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110378, acc: 0.958830\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110354, acc: 0.958839\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110330, acc: 0.958848\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110305, acc: 0.958857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110281, acc: 0.958866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110257, acc: 0.958875\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110233, acc: 0.958884\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110428, acc: 0.958871\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110404, acc: 0.958880\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110380, acc: 0.958889\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110355, acc: 0.958898\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110331, acc: 0.958907\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110307, acc: 0.958916\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110283, acc: 0.958925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110259, acc: 0.958934\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110234, acc: 0.958943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110210, acc: 0.958952\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110186, acc: 0.958961\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110162, acc: 0.958970\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110138, acc: 0.958979\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110114, acc: 0.958988\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110090, acc: 0.958997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110066, acc: 0.959006\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110042, acc: 0.959015\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110236, acc: 0.959002\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110431, acc: 0.958938\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110407, acc: 0.958947\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110383, acc: 0.958956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110358, acc: 0.958965\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110334, acc: 0.958974\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110310, acc: 0.958983\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110286, acc: 0.958992\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110262, acc: 0.959001\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110238, acc: 0.959010\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110214, acc: 0.959019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110190, acc: 0.959028\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110166, acc: 0.959037\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110142, acc: 0.959046\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110336, acc: 0.958836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110312, acc: 0.958845\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110288, acc: 0.958854\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110264, acc: 0.958863\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110240, acc: 0.958872\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110216, acc: 0.958881\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110192, acc: 0.958890\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110168, acc: 0.958899\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110144, acc: 0.958908\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110120, acc: 0.958917\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110096, acc: 0.958926\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110072, acc: 0.958935\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110048, acc: 0.958944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110024, acc: 0.958953\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110217, acc: 0.958744\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110411, acc: 0.958729\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110387, acc: 0.958738\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110363, acc: 0.958747\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110339, acc: 0.958756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110315, acc: 0.958765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110291, acc: 0.958774\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110267, acc: 0.958783\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110243, acc: 0.958792\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110219, acc: 0.958801\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110412, acc: 0.958701\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110388, acc: 0.958710\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110581, acc: 0.958502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110557, acc: 0.958511\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110533, acc: 0.958520\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110509, acc: 0.958529\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110485, acc: 0.958538\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110461, acc: 0.958547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110437, acc: 0.958556\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110630, acc: 0.958541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110606, acc: 0.958550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110582, acc: 0.958559\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110558, acc: 0.958568\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110534, acc: 0.958577\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110727, acc: 0.958564\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110703, acc: 0.958573\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110895, acc: 0.958561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110871, acc: 0.958570\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110847, acc: 0.958579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110823, acc: 0.958588\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110799, acc: 0.958596\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110991, acc: 0.958584\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110967, acc: 0.958593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110943, acc: 0.958602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110919, acc: 0.958611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110895, acc: 0.958620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110871, acc: 0.958628\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110848, acc: 0.958637\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110824, acc: 0.958646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110800, acc: 0.958655\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110776, acc: 0.958664\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110752, acc: 0.958673\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110728, acc: 0.958682\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110704, acc: 0.958691\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110680, acc: 0.958700\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110657, acc: 0.958709\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110633, acc: 0.958718\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110609, acc: 0.958726\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110585, acc: 0.958735\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110561, acc: 0.958744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110538, acc: 0.958753\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110514, acc: 0.958762\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110490, acc: 0.958771\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110466, acc: 0.958780\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110443, acc: 0.958788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110419, acc: 0.958797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110395, acc: 0.958806\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110371, acc: 0.958815\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.110562, acc: 0.958609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110539, acc: 0.958618\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110515, acc: 0.958627\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110491, acc: 0.958636\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110468, acc: 0.958645\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110444, acc: 0.958654\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110420, acc: 0.958662\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110397, acc: 0.958671\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110587, acc: 0.958656\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110564, acc: 0.958665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110540, acc: 0.958674\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110516, acc: 0.958683\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110493, acc: 0.958692\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110469, acc: 0.958701\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110445, acc: 0.958709\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110422, acc: 0.958718\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110398, acc: 0.958727\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110374, acc: 0.958736\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110351, acc: 0.958745\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110327, acc: 0.958754\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110304, acc: 0.958762\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110280, acc: 0.958771\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110256, acc: 0.958780\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110233, acc: 0.958789\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110209, acc: 0.958798\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110186, acc: 0.958806\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110162, acc: 0.958815\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110139, acc: 0.958824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110115, acc: 0.958833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110092, acc: 0.958842\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110068, acc: 0.958850\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110045, acc: 0.958859\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110021, acc: 0.958868\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.109998, acc: 0.958877\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110188, acc: 0.958862\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110164, acc: 0.958871\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110141, acc: 0.958879\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110117, acc: 0.958888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110094, acc: 0.958897\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110070, acc: 0.958906\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110047, acc: 0.958914\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110023, acc: 0.958923\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110213, acc: 0.958825\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110402, acc: 0.958811\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110379, acc: 0.958819\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110355, acc: 0.958828\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110332, acc: 0.958837\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110521, acc: 0.958819\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110497, acc: 0.958828\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110686, acc: 0.958813\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110875, acc: 0.958751\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110852, acc: 0.958760\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110828, acc: 0.958768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110805, acc: 0.958777\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110781, acc: 0.958786\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110757, acc: 0.958795\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110734, acc: 0.958803\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110710, acc: 0.958812\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110687, acc: 0.958821\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110664, acc: 0.958830\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110852, acc: 0.958815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110829, acc: 0.958823\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110805, acc: 0.958832\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110782, acc: 0.958841\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110758, acc: 0.958850\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110735, acc: 0.958858\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110711, acc: 0.958867\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110688, acc: 0.958876\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110664, acc: 0.958884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110641, acc: 0.958893\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110618, acc: 0.958902\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110594, acc: 0.958910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110571, acc: 0.958919\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110547, acc: 0.958928\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110524, acc: 0.958937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110501, acc: 0.958945\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110477, acc: 0.958954\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110454, acc: 0.958963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110431, acc: 0.958971\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110407, acc: 0.958980\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110595, acc: 0.958967\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110572, acc: 0.958976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110549, acc: 0.958985\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110525, acc: 0.958993\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110713, acc: 0.958981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110689, acc: 0.958990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110666, acc: 0.958998\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110643, acc: 0.959007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110619, acc: 0.959016\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110596, acc: 0.959024\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110573, acc: 0.959033\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110550, acc: 0.959041\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110526, acc: 0.959050\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110503, acc: 0.959059\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110480, acc: 0.959067\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110457, acc: 0.959076\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110433, acc: 0.959084\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110410, acc: 0.959093\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110387, acc: 0.959102\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110364, acc: 0.959110\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.110551, acc: 0.958909\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110738, acc: 0.958896\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110924, acc: 0.958800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110901, acc: 0.958809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110878, acc: 0.958817\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110855, acc: 0.958826\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111041, acc: 0.958814\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111228, acc: 0.958612\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111204, acc: 0.958621\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111391, acc: 0.958525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111367, acc: 0.958533\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111344, acc: 0.958542\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111321, acc: 0.958551\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111507, acc: 0.958539\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111484, acc: 0.958547\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111460, acc: 0.958556\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111646, acc: 0.958538\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111623, acc: 0.958547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111600, acc: 0.958556\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111576, acc: 0.958565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111553, acc: 0.958573\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111739, acc: 0.958512\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111715, acc: 0.958521\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111692, acc: 0.958529\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111669, acc: 0.958538\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111854, acc: 0.958442\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111831, acc: 0.958451\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111808, acc: 0.958460\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111784, acc: 0.958468\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111970, acc: 0.958454\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111947, acc: 0.958462\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111923, acc: 0.958471\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111900, acc: 0.958480\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111876, acc: 0.958488\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111853, acc: 0.958497\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112038, acc: 0.958401\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112224, acc: 0.958389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112200, acc: 0.958398\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112177, acc: 0.958407\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112153, acc: 0.958415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112130, acc: 0.958424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112107, acc: 0.958433\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112083, acc: 0.958441\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112060, acc: 0.958450\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112037, acc: 0.958459\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112013, acc: 0.958467\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111990, acc: 0.958476\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111967, acc: 0.958485\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111943, acc: 0.958493\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111920, acc: 0.958502\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111897, acc: 0.958510\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111874, acc: 0.958519\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111850, acc: 0.958528\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111827, acc: 0.958536\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111804, acc: 0.958545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111781, acc: 0.958554\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111757, acc: 0.958562\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111734, acc: 0.958571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111711, acc: 0.958579\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111688, acc: 0.958588\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111665, acc: 0.958597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111641, acc: 0.958605\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111618, acc: 0.958614\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111595, acc: 0.958622\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111572, acc: 0.958631\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111549, acc: 0.958639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111526, acc: 0.958648\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111503, acc: 0.958657\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111479, acc: 0.958665\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111456, acc: 0.958674\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111433, acc: 0.958682\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111410, acc: 0.958691\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111387, acc: 0.958699\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111364, acc: 0.958708\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111341, acc: 0.958717\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111525, acc: 0.958702\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111502, acc: 0.958711\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111479, acc: 0.958719\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111456, acc: 0.958728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111433, acc: 0.958736\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111410, acc: 0.958745\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111387, acc: 0.958753\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111570, acc: 0.958741\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111547, acc: 0.958750\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111524, acc: 0.958758\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111501, acc: 0.958767\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111478, acc: 0.958775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111455, acc: 0.958784\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111432, acc: 0.958792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111409, acc: 0.958801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111386, acc: 0.958809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111363, acc: 0.958818\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111340, acc: 0.958826\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111317, acc: 0.958835\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111294, acc: 0.958843\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111271, acc: 0.958852\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111248, acc: 0.958860\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111226, acc: 0.958869\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111203, acc: 0.958877\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111180, acc: 0.958886\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111157, acc: 0.958894\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111134, acc: 0.958902\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958911\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111088, acc: 0.958919\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111065, acc: 0.958928\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111043, acc: 0.958936\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.958945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110997, acc: 0.958953\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.958962\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110951, acc: 0.958970\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110929, acc: 0.958978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110906, acc: 0.958987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110883, acc: 0.958995\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110860, acc: 0.959004\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110837, acc: 0.959012\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110815, acc: 0.959021\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110792, acc: 0.959029\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110974, acc: 0.959015\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110952, acc: 0.959023\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110929, acc: 0.959031\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110906, acc: 0.959040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110883, acc: 0.959048\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110861, acc: 0.959057\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110838, acc: 0.959065\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110815, acc: 0.959073\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110793, acc: 0.959082\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110975, acc: 0.958988\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110952, acc: 0.958996\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110929, acc: 0.959005\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110906, acc: 0.959013\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110884, acc: 0.959021\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110861, acc: 0.959030\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110838, acc: 0.959038\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110816, acc: 0.959046\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.110998, acc: 0.959034\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110975, acc: 0.959043\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110952, acc: 0.959051\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110930, acc: 0.959059\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110907, acc: 0.959068\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111088, acc: 0.958872\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.958880\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111043, acc: 0.958889\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111020, acc: 0.958897\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111202, acc: 0.958885\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111179, acc: 0.958894\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111156, acc: 0.958902\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111134, acc: 0.958910\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111315, acc: 0.958898\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111292, acc: 0.958907\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111270, acc: 0.958915\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111247, acc: 0.958923\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111224, acc: 0.958932\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111202, acc: 0.958940\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111179, acc: 0.958948\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111156, acc: 0.958957\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111134, acc: 0.958965\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.958974\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111089, acc: 0.958982\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.958990\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111043, acc: 0.958999\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111021, acc: 0.959007\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110998, acc: 0.959015\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110976, acc: 0.959024\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110953, acc: 0.959032\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110931, acc: 0.959040\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110908, acc: 0.959049\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110885, acc: 0.959057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110863, acc: 0.959065\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111043, acc: 0.959051\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111021, acc: 0.959059\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110998, acc: 0.959068\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110976, acc: 0.959076\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110953, acc: 0.959084\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110931, acc: 0.959092\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110908, acc: 0.959101\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110886, acc: 0.959109\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110863, acc: 0.959117\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110841, acc: 0.959126\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111021, acc: 0.959109\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110999, acc: 0.959117\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.110976, acc: 0.959125\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110954, acc: 0.959133\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110931, acc: 0.959142\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110909, acc: 0.959150\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110886, acc: 0.959158\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110864, acc: 0.959166\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.110841, acc: 0.959175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110819, acc: 0.959183\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110999, acc: 0.959090\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.110976, acc: 0.959098\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111156, acc: 0.959039\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111134, acc: 0.959048\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111111, acc: 0.959056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111089, acc: 0.959064\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.959072\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111044, acc: 0.959081\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111021, acc: 0.959089\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110999, acc: 0.959097\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.110977, acc: 0.959105\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110954, acc: 0.959114\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.110932, acc: 0.959122\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110909, acc: 0.959130\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110887, acc: 0.959138\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.110865, acc: 0.959147\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.110842, acc: 0.959155\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110820, acc: 0.959163\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110798, acc: 0.959171\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.110775, acc: 0.959180\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.110954, acc: 0.959168\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.110932, acc: 0.959176\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.110910, acc: 0.959184\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111089, acc: 0.958991\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111066, acc: 0.958999\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111245, acc: 0.958907\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111223, acc: 0.958915\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111200, acc: 0.958924\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.111379, acc: 0.958731\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111357, acc: 0.958739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111334, acc: 0.958747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111312, acc: 0.958756\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111491, acc: 0.958744\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111468, acc: 0.958752\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111446, acc: 0.958760\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111624, acc: 0.958746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111602, acc: 0.958755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111579, acc: 0.958763\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111557, acc: 0.958771\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111535, acc: 0.958779\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111512, acc: 0.958788\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111490, acc: 0.958796\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111468, acc: 0.958804\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111646, acc: 0.958612\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111623, acc: 0.958620\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111601, acc: 0.958629\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111579, acc: 0.958637\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111556, acc: 0.958645\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111534, acc: 0.958653\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111512, acc: 0.958662\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111489, acc: 0.958670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111467, acc: 0.958678\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111445, acc: 0.958687\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111422, acc: 0.958695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111400, acc: 0.958703\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111378, acc: 0.958711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111355, acc: 0.958720\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111333, acc: 0.958728\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111311, acc: 0.958736\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111489, acc: 0.958644\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111666, acc: 0.958633\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111843, acc: 0.958541\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111821, acc: 0.958549\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111799, acc: 0.958558\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111776, acc: 0.958566\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111954, acc: 0.958474\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.112131, acc: 0.958283\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112109, acc: 0.958292\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112086, acc: 0.958300\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112064, acc: 0.958308\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112041, acc: 0.958316\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112019, acc: 0.958325\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111997, acc: 0.958333\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111974, acc: 0.958341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111952, acc: 0.958350\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111930, acc: 0.958358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111908, acc: 0.958366\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111885, acc: 0.958375\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111863, acc: 0.958383\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111841, acc: 0.958391\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111819, acc: 0.958399\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111796, acc: 0.958408\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111774, acc: 0.958416\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111752, acc: 0.958424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111730, acc: 0.958432\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111707, acc: 0.958441\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111685, acc: 0.958449\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111663, acc: 0.958457\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111641, acc: 0.958466\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111619, acc: 0.958474\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111597, acc: 0.958482\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111574, acc: 0.958490\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111552, acc: 0.958499\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.111530, acc: 0.958507\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111706, acc: 0.958493\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111684, acc: 0.958501\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.111662, acc: 0.958509\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111640, acc: 0.958518\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111618, acc: 0.958526\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111596, acc: 0.958534\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111574, acc: 0.958542\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111551, acc: 0.958550\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.111529, acc: 0.958559\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111507, acc: 0.958567\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111683, acc: 0.958553\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111859, acc: 0.958539\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111837, acc: 0.958548\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111815, acc: 0.958556\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111793, acc: 0.958564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111771, acc: 0.958572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111748, acc: 0.958580\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111726, acc: 0.958589\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.111902, acc: 0.958577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111880, acc: 0.958585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111858, acc: 0.958593\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112033, acc: 0.958573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112011, acc: 0.958581\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111989, acc: 0.958590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111967, acc: 0.958598\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.111945, acc: 0.958606\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111923, acc: 0.958614\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.111901, acc: 0.958622\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111878, acc: 0.958630\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.111856, acc: 0.958639\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.111834, acc: 0.958647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.111812, acc: 0.958655\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.111790, acc: 0.958663\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.111965, acc: 0.958606\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.111943, acc: 0.958614\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112118, acc: 0.958425\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112096, acc: 0.958433\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112271, acc: 0.958417\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.112249, acc: 0.958425\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112227, acc: 0.958433\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112402, acc: 0.958343\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112379, acc: 0.958351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112357, acc: 0.958359\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112335, acc: 0.958367\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.112510, acc: 0.958356\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.112488, acc: 0.958364\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112466, acc: 0.958372\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112443, acc: 0.958380\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112618, acc: 0.958192\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112596, acc: 0.958200\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112770, acc: 0.958189\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112748, acc: 0.958197\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112726, acc: 0.958205\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112704, acc: 0.958213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.112682, acc: 0.958222\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112659, acc: 0.958230\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112834, acc: 0.958173\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112811, acc: 0.958181\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112789, acc: 0.958189\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112767, acc: 0.958197\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112745, acc: 0.958205\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112723, acc: 0.958214\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.112701, acc: 0.958222\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112679, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112657, acc: 0.958238\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112635, acc: 0.958246\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112613, acc: 0.958255\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112591, acc: 0.958263\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112569, acc: 0.958271\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112742, acc: 0.958259\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112720, acc: 0.958268\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.112698, acc: 0.958276\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.112676, acc: 0.958284\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112654, acc: 0.958292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112632, acc: 0.958300\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112610, acc: 0.958308\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112588, acc: 0.958317\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.112761, acc: 0.958305\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112739, acc: 0.958313\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.112717, acc: 0.958321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.112695, acc: 0.958330\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112673, acc: 0.958338\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.112651, acc: 0.958346\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.112629, acc: 0.958354\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.112607, acc: 0.958362\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112585, acc: 0.958370\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.112563, acc: 0.958378\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.112541, acc: 0.958387\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112910, acc: 0.958005\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.112888, acc: 0.958013\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113060, acc: 0.957997\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113038, acc: 0.958005\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113016, acc: 0.958013\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113189, acc: 0.957924\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113362, acc: 0.957910\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113340, acc: 0.957919\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113318, acc: 0.957927\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113296, acc: 0.957935\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113274, acc: 0.957943\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113252, acc: 0.957951\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113424, acc: 0.957765\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113402, acc: 0.957773\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113380, acc: 0.957781\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113552, acc: 0.957595\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113530, acc: 0.957603\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113508, acc: 0.957612\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.957531\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.957539\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.957547\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.957556\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.957544\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.957553\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.957561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.957569\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.957489\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957400\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957408\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957397\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957386\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957394\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957402\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957411\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957427\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957435\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957444\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957452\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.957460\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957449\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957457\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957465\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957474\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957482\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957490\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957498\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957487\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957495\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957504\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957512\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.957520\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957528\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957545\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957553\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957376\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957385\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957393\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957401\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957409\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957417\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957426\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957434\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957442\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957258\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957266\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957274\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957282\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957291\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957299\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957307\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957315\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957324\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.957332\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957340\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957356\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.957381\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957389\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.957397\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957405\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.957414\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.957422\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957238\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957246\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957063\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957071\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957079\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957049\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957038\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957055\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957063\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.956880\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.956869\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.956877\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.956886\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.956894\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114870, acc: 0.956883\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.956891\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.956899\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.956908\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.956916\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.956924\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.956932\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.956941\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.956949\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.956957\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.956965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.956973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.956982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.956990\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.956998\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957014\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957023\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957031\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.956944\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.956952\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.956960\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.956941\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.956949\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.956934\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.956942\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.956931\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.956939\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.956947\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.956956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.956964\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.956972\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.956790\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.956798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.956807\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.956815\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.956728\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956736\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.956745\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115348, acc: 0.956732\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956740\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.956748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.956756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.956764\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.956773\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115406, acc: 0.956591\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956600\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.956608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.956616\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115319, acc: 0.956624\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.956632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115275, acc: 0.956641\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.956649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.956657\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.956665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.956673\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.956682\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.956690\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.956698\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.956706\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956714\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.956723\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956731\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.956739\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.956747\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.956755\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.956763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.956772\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.956780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.956788\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.956796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.956804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.956812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.956820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.956829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956837\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.956845\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.956853\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.956842\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.956850\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.956859\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.956867\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.956875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.956883\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.956891\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.956899\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.956907\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.956727\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.956735\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.956744\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.956658\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.956666\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.956674\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.956682\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.956690\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.956698\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.956707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.956715\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.956723\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.956731\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.956739\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.956755\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.956763\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.956771\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.956780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.956788\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.956796\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.956785\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.956793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.956801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.956809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.956817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.956826\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.956834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.956842\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.956756\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.956764\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.956772\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.956781\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.956789\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.956797\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.956805\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.956813\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.956821\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.956829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.956837\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.956845\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.956853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.956861\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.956869\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.956877\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.956867\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.956875\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.956883\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.956891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.956899\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.956814\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.956822\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.956737\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.956745\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.956753\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.956761\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.956746\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.956754\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.956762\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.956749\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.956757\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.956765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.956773\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.956781\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.956789\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.956797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.956805\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.956813\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.956801\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.956809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.956817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.956825\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.956833\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.956841\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.956849\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.956857\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.956865\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.956873\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.956881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.956889\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.956897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.956905\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.956913\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.956921\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.956929\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.956937\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.956945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.956953\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.956868\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.956876\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.956884\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.956892\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.956900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.956908\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.956731\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.956739\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.956747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.956755\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.956763\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.956771\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.956779\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.956766\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.956774\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114902, acc: 0.956782\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.956790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.956798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.956806\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.956630\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.956638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.956646\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.956654\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.956639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.956647\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.956655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.956663\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.956671\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.956679\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.956687\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.956695\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.956703\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.956711\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.956719\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.956727\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.956735\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.956742\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.956567\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.956575\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.956583\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.956591\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.956599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.956607\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.956615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.956622\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.956630\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.956638\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.956646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956654\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.956571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.956579\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.956587\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.956594\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.956602\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.956590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.956598\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.956606\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.956614\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.956622\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.956630\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.956638\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.956554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.956562\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.956570\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.956578\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.956568\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.956576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.956584\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.956591\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.956508\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.956516\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.956524\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.956532\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.956540\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.956548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.956556\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956564\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.956571\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.956488\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956496\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.956504\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.956512\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.956520\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.956528\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.956536\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.956544\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.956552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.956560\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.956567\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.956575\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.956583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.956591\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.956599\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.956581\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.956589\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.956597\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.956605\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.956590\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.956598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.956606\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.956593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.956601\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.956609\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.956617\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.956625\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.956633\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.956641\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.956648\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.956656\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.956664\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.956672\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.956680\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.956688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956695\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.956703\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.956711\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.956719\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.956727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.956735\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.956743\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.956750\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.956668\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.956676\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.956683\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.956691\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.956699\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.956707\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.956715\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.956723\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.956730\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.956648\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.956630\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.956638\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.956646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.956653\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.956661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.956669\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956677\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.956685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.956692\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.956700\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.956708\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.956716\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.956634\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.956641\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.956649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.956657\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.956665\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.956673\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.956680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.956688\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.956696\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.956704\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.956712\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.956719\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.956727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.956735\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.956743\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.956750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.956758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.956766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.956774\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.956782\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.956789\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.956797\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.956625\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.956633\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.956641\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.956649\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.956477\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.956485\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.956493\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.956500\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.956508\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.956426\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.956434\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.956420\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.956428\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.956435\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.956443\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.956451\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.956459\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.956466\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.956474\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.956303\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.956311\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.956319\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.956327\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.956334\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.956342\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.956350\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.956358\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.956366\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.956373\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.956381\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.956389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.956397\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.956405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.956412\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.956420\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.956428\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.956436\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.956443\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.956451\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.956459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956467\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.956474\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.956482\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.956490\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.956498\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.956505\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.956513\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.956432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.956440\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.956447\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.956366\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.956374\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.956382\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.956390\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.956397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.956405\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.956413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.956421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.956428\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.956411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.956418\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.956426\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114838, acc: 0.956434\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.956442\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.956449\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.956457\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956465\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.956472\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.956480\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.956488\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.956496\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.956503\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.956511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.956519\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.956526\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.956534\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.956542\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.956530\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.956538\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.956545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.956553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.956561\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.956480\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.956488\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.956470\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114902, acc: 0.956478\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.956485\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.956493\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.956483\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.956491\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.956410\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.956418\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.956426\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.956433\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.956423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956431\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115208, acc: 0.956439\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.956446\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.956454\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.956462\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.956469\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.956477\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.956485\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.956492\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.956412\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956420\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.956427\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.956435\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.956443\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.956450\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115276, acc: 0.956433\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.956441\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.956448\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.956456\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.956464\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115351, acc: 0.956442\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.956450\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.956457\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.956465\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.956472\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.956480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956488\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.956495\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.956503\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.956511\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.956518\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.956526\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.956534\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.956541\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.956549\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.956556\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.956545\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.956552\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.956560\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.956567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.956575\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.956583\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.956590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.956598\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.956605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.956613\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.956621\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.956628\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.956636\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.956643\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.956651\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.956659\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.956666\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.956674\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114840, acc: 0.956681\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.956689\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.956696\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.956704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.956712\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.956719\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.956727\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.956734\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114854, acc: 0.956567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.956575\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.956582\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.956590\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.956598\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.956613\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.956620\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.956628\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.956635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.956643\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.956650\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.956658\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.956666\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.956673\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.956681\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.956688\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.956696\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.956703\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.956711\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.956718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.956726\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.956733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.956741\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.956748\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.956756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.956763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.956771\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.956778\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.956786\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.956794\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.956801\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.956809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.956816\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.956804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.956812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.956819\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.956827\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.956834\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.956842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.956849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.956857\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.956864\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.956872\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114152, acc: 0.956879\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.956887\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.956894\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.956901\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.956822\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.956830\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.956837\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.956787\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.956795\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.956802\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.956810\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.956817\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.956803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.956810\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.956818\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.956825\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.956833\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.956840\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.956848\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.956855\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.956862\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.956870\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.956858\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.956866\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.956873\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.956881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.956888\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.956874\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.956881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.956889\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.956896\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.956904\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.956911\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.956746\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.956754\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.956761\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.956769\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.956776\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.956783\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.956791\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.956798\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.956806\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.956813\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.956821\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.956828\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.956835\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.956843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.956850\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.956840\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.956848\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.956855\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.956863\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.956853\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.956860\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.956868\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.956875\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.956861\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.956868\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.956876\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.956866\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.956874\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.956710\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.956717\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.956724\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.956732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.956739\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.956747\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.956754\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.956761\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.956769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.956776\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.956784\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.956791\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.956798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.956806\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.956794\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.956802\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.956723\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.956731\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.956738\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.956746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.956753\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.956760\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.956775\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.956783\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.956790\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114846, acc: 0.956776\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.956783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.956791\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.956798\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.956788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.956796\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.956803\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.956793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.956801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.956808\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.956816\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.956653\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.956660\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.956667\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956675\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.956625\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956463\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.956470\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115345, acc: 0.956477\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956485\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.956492\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115457, acc: 0.956330\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115437, acc: 0.956337\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.956320\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115568, acc: 0.956327\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115548, acc: 0.956335\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115698, acc: 0.956172\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.956180\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.956187\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115809, acc: 0.956178\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115789, acc: 0.956185\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.956193\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115750, acc: 0.956200\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115731, acc: 0.956208\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115881, acc: 0.956130\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115861, acc: 0.956138\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115841, acc: 0.956145\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115822, acc: 0.956152\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115802, acc: 0.956160\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.956150\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115932, acc: 0.956158\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115913, acc: 0.956165\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115893, acc: 0.956173\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115873, acc: 0.956180\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115854, acc: 0.956187\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115834, acc: 0.956195\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115814, acc: 0.956202\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115795, acc: 0.956210\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115775, acc: 0.956217\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115925, acc: 0.956208\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115905, acc: 0.956215\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115886, acc: 0.956222\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115866, acc: 0.956230\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115846, acc: 0.956237\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115827, acc: 0.956245\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115807, acc: 0.956252\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115788, acc: 0.956259\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115937, acc: 0.956250\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115918, acc: 0.956257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115898, acc: 0.956265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115878, acc: 0.956272\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115859, acc: 0.956280\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115839, acc: 0.956287\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115820, acc: 0.956294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115800, acc: 0.956302\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115781, acc: 0.956309\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115930, acc: 0.956298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115910, acc: 0.956305\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115891, acc: 0.956312\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115871, acc: 0.956320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115852, acc: 0.956327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115832, acc: 0.956334\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115813, acc: 0.956342\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115793, acc: 0.956349\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115774, acc: 0.956357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115754, acc: 0.956364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115735, acc: 0.956371\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.956379\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.956386\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115844, acc: 0.956309\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115825, acc: 0.956316\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115805, acc: 0.956324\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115786, acc: 0.956331\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115935, acc: 0.956320\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115915, acc: 0.956327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115896, acc: 0.956335\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115876, acc: 0.956342\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115857, acc: 0.956349\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115837, acc: 0.956357\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115818, acc: 0.956364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115798, acc: 0.956371\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115779, acc: 0.956379\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115759, acc: 0.956386\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.956393\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.956401\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115701, acc: 0.956408\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115682, acc: 0.956415\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.956422\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.956430\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.956437\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115772, acc: 0.956426\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115752, acc: 0.956433\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115733, acc: 0.956440\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115714, acc: 0.956448\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115862, acc: 0.956371\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115842, acc: 0.956378\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115823, acc: 0.956386\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115804, acc: 0.956393\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115784, acc: 0.956400\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115765, acc: 0.956408\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115913, acc: 0.956331\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115893, acc: 0.956339\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116042, acc: 0.956327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116022, acc: 0.956335\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116003, acc: 0.956342\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115983, acc: 0.956349\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.956357\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115944, acc: 0.956364\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115925, acc: 0.956371\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.956378\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115886, acc: 0.956386\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115867, acc: 0.956393\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115848, acc: 0.956400\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115828, acc: 0.956408\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115809, acc: 0.956415\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115789, acc: 0.956422\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.956429\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115751, acc: 0.956437\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115731, acc: 0.956444\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115712, acc: 0.956451\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115860, acc: 0.956438\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115840, acc: 0.956445\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115821, acc: 0.956452\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115802, acc: 0.956459\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115782, acc: 0.956467\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115930, acc: 0.956307\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116077, acc: 0.956298\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116058, acc: 0.956305\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116039, acc: 0.956312\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116019, acc: 0.956320\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116000, acc: 0.956327\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.956334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.956341\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115942, acc: 0.956349\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115923, acc: 0.956356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115903, acc: 0.956363\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115884, acc: 0.956371\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115865, acc: 0.956378\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115846, acc: 0.956385\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115826, acc: 0.956392\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115807, acc: 0.956400\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115788, acc: 0.956407\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115768, acc: 0.956414\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115749, acc: 0.956421\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.956429\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115877, acc: 0.956353\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116024, acc: 0.956194\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116005, acc: 0.956201\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115985, acc: 0.956208\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115966, acc: 0.956216\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115947, acc: 0.956223\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115928, acc: 0.956230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115908, acc: 0.956237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115889, acc: 0.956245\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115870, acc: 0.956252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115851, acc: 0.956259\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115997, acc: 0.956250\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115978, acc: 0.956257\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115959, acc: 0.956264\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.956272\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115920, acc: 0.956279\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115901, acc: 0.956286\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115882, acc: 0.956293\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115863, acc: 0.956301\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115844, acc: 0.956308\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115824, acc: 0.956315\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115805, acc: 0.956322\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115786, acc: 0.956330\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115767, acc: 0.956337\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115748, acc: 0.956344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.956351\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.956358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.956366\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.956373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115652, acc: 0.956380\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115633, acc: 0.956387\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115614, acc: 0.956395\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115595, acc: 0.956402\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.956409\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115556, acc: 0.956416\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.956423\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.956431\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.956438\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.956445\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.956452\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115607, acc: 0.956441\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115753, acc: 0.956432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.956439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.956446\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.956453\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.956460\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.956468\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.956475\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.956482\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115765, acc: 0.956473\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.956480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.956487\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115708, acc: 0.956494\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115689, acc: 0.956501\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115670, acc: 0.956509\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.956516\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.956523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.956530\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.956537\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.956544\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115556, acc: 0.956552\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.956559\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.956566\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.956573\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.956580\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.956587\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.956594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115423, acc: 0.956602\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.956609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956616\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115366, acc: 0.956623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.956630\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.956473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.956480\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115454, acc: 0.956487\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.956412\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115580, acc: 0.956419\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115561, acc: 0.956427\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115542, acc: 0.956434\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115523, acc: 0.956441\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.956448\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115486, acc: 0.956455\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115467, acc: 0.956462\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115448, acc: 0.956469\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115429, acc: 0.956477\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115410, acc: 0.956484\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.956491\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115372, acc: 0.956498\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115353, acc: 0.956505\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.956512\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115315, acc: 0.956519\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.956526\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115441, acc: 0.956515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115422, acc: 0.956522\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.956530\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115366, acc: 0.956544\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.956551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.956558\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115309, acc: 0.956565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.956572\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.956579\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.956586\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.956594\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.956601\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.956608\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.956615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.956622\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.956629\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.956636\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.956643\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956650\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.956657\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.956644\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.956651\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.956658\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.956665\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.956672\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.956679\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.956686\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.956693\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.956701\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.956708\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.956715\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.956722\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.956729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.956736\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.956725\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.956732\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.956739\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.956746\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.956753\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.956760\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.956767\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.956611\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.956618\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.956625\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.956633\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.956640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.956647\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.956654\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.956661\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.956668\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.956675\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.956601\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.956608\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.956622\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.956629\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.956636\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.956589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.956596\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.956603\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.956610\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.956617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.956624\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.956631\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956638\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.956645\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.956652\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.956659\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.956666\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.956673\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.956680\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.956687\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.956694\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.956701\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.956708\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.956715\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.956722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.956729\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.956736\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956743\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.956750\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.956757\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.956764\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.956771\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.956778\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.956785\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.956792\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.956781\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.956788\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.956795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.956802\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.956791\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.956798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.956805\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.956812\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.956658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.956665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.956672\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114889, acc: 0.956679\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.956605\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.956612\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.956619\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.956610\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.956617\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.956624\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.956631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.956638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.956645\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.956652\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.956659\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.956666\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.956673\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.956680\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.956687\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.956694\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.956701\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.956708\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.956714\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.956721\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.956711\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.956717\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.956724\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.956731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.956738\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.956745\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.956672\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.956679\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.956686\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.956693\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.956700\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.956707\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.956714\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.956721\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.956728\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114867, acc: 0.956734\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.956741\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.956748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.956755\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114954, acc: 0.956744\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.956751\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.956758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.956765\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.956772\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.956779\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.956786\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.956793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.956800\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.956807\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.956814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.956820\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.956827\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.956834\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.956841\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.956848\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.956855\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.956862\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.956869\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.956876\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.956882\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.956869\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.956717\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.956724\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.956731\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.956737\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.956744\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.956731\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.956738\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.956745\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.956752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.956759\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.956766\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.956773\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.956780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.956786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.956793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956800\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.956807\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.956814\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.956821\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.956828\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.956781\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.956788\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.956795\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.956802\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.956730\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.956736\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.956743\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.956750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.956757\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.956764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.956771\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.956778\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.956784\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.956791\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.956798\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.956805\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.956796\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.956803\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.956810\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.956817\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.956823\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956830\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.956837\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.956844\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.956851\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.956858\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.956864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.956871\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.956878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.956885\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.956892\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.956899\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.956888\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.956895\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.956901\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.956892\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.956899\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.956827\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.956834\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.956841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.956847\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.956854\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.956861\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.956868\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.956875\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.956882\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.956888\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.956895\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.956902\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.956909\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.956916\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.956844\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.956850\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.956857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.956864\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.956871\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956878\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.956884\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.956891\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.956898\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.956905\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.956911\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.956761\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.956768\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.956735\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.956742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.956749\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114870, acc: 0.956755\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.956762\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.956769\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.956776\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.956783\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.956789\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.956796\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.956803\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.956810\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.956801\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.956729\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.956736\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.956743\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.956750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.956756\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.956763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.956770\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.956777\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.956761\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.956768\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.956696\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.956703\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.956710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956717\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.956723\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.956573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.956580\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.956587\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.956594\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.956601\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.956607\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.956614\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.956621\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.956628\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.956635\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.956641\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.956648\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.956655\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.956662\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.956512\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.956519\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.956526\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.956532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956539\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.956546\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.956553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114982, acc: 0.956560\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.956551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.956558\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.956564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.956571\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.956578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.956585\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.956552\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.956559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.956566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.956573\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.956580\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956586\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115217, acc: 0.956515\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.956522\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.956529\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115319, acc: 0.956518\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.956525\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.956532\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.956538\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115403, acc: 0.956506\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956513\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.956520\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.956504\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.956496\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115780, acc: 0.956347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115762, acc: 0.956354\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115744, acc: 0.956360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115726, acc: 0.956367\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115708, acc: 0.956374\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.956381\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115672, acc: 0.956387\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115654, acc: 0.956394\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.956401\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.956408\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115600, acc: 0.956415\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115737, acc: 0.956344\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115719, acc: 0.956350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115701, acc: 0.956357\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.956364\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.956371\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.956378\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.956384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.956391\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.956398\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115576, acc: 0.956405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115558, acc: 0.956411\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115540, acc: 0.956418\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115522, acc: 0.956425\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.956354\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.956361\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.956368\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.956374\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.956381\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.956388\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.956395\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.956401\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115516, acc: 0.956408\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.956415\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.956422\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.956428\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.956435\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.956442\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115408, acc: 0.956449\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.956455\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.956462\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.956469\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.956476\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115319, acc: 0.956482\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.956489\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.956496\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.956502\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.956494\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.956346\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115676, acc: 0.956301\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115658, acc: 0.956308\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.956315\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115622, acc: 0.956322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.956328\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115586, acc: 0.956335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.956342\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.956348\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.956355\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.956362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.956369\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115479, acc: 0.956375\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115616, acc: 0.956367\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115598, acc: 0.956373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115580, acc: 0.956380\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.956387\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.956394\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.956400\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.956407\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.956414\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.956420\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115456, acc: 0.956427\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115438, acc: 0.956434\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115420, acc: 0.956441\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.956447\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.956461\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.956467\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.956474\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.956481\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115450, acc: 0.956334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115432, acc: 0.956340\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115414, acc: 0.956347\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.956354\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.956361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.956367\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.956374\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956381\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.956387\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.956394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115272, acc: 0.956401\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.956407\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.956414\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.956421\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.956434\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.956441\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.956396\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115438, acc: 0.956326\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115420, acc: 0.956333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.956340\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956347\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.956353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.956360\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.956367\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.956373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.956380\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.956387\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.956393\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.956323\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.956330\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.956337\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.956343\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.956357\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.956364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.956370\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.956377\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.956384\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.956390\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956397\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.956404\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.956410\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.956417\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.956271\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.956277\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.956284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.956291\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.956297\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.956304\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.956311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.956317\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.956309\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.956316\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.956322\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.956310\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.956316\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115514, acc: 0.956304\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115496, acc: 0.956311\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.956317\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.956324\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115443, acc: 0.956331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.956337\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115408, acc: 0.956344\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115390, acc: 0.956351\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.956357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.956364\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115338, acc: 0.956371\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.956377\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.956384\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.956390\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.956397\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.956404\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.956410\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.956417\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.956424\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.956430\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.956437\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.956443\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.956450\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.956457\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.956463\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.956470\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.956477\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.956483\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.956490\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.956496\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.956503\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.956510\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.956516\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.956523\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.956529\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.956460\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.956467\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.956473\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.956480\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.956487\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.956493\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.956500\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.956506\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.956513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.956519\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.956526\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.956533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.956539\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.956546\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.956552\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.956559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.956565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.956572\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.956579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.956585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.956592\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.956598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.956605\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.956536\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.956526\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.956532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.956539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.956545\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.956530\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.956537\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.956543\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.956550\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.956557\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.956563\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.956494\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.956501\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.956507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.956514\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.956520\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.956527\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.956534\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.956540\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.956547\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.956553\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.956560\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.956416\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.956347\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.956354\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.956360\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.956367\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.956373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.956380\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.956386\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.956393\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.956400\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.956406\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.956413\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.956419\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.956426\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.956432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.956439\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.956430\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.956422\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.956428\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.956435\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.956442\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.956448\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.956455\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956461\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.956468\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.956458\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.956464\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.956321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115292, acc: 0.956327\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115425, acc: 0.956259\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115408, acc: 0.956265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115390, acc: 0.956272\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.956278\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115356, acc: 0.956285\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115339, acc: 0.956292\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.956298\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.956305\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.956311\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115269, acc: 0.956318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.956324\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.956331\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.956321\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115350, acc: 0.956327\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115333, acc: 0.956334\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.956340\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.956347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115281, acc: 0.956353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.956360\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115247, acc: 0.956366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.956379\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.956386\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.956392\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.956399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.956406\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.956412\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.956419\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.956425\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.956432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.956438\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.956445\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.956451\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.956458\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.956464\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.956454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.956460\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.956467\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.956473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.956480\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.956486\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.956493\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.956499\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.956506\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.956497\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.956504\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.956361\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.956368\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.956374\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.956381\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.956387\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.956394\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.956400\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.956407\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.956413\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.956405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.956411\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.956418\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.956424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.956431\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.956437\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.956444\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.956450\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.956457\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.956463\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.956470\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956476\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.956483\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.956489\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.956496\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.956502\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.956508\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.956515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.956521\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.956528\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.956534\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.956541\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.956547\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.956554\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.956560\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.956566\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.956573\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.956579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.956586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.956592\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.956599\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.956605\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.956611\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.956603\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.956609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.956616\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.956622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.956629\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.956635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.956642\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.956648\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.956654\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.956587\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.956593\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.956600\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.956606\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.956613\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.956619\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.956625\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.956632\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.956638\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.956645\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.956651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.956657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.956664\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.956670\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.956677\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.956683\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.956689\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.956696\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.956702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.956708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.956715\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.956721\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.956711\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.956718\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.956724\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.956730\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.956737\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.956743\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.956749\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.956756\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.956762\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.956769\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.956775\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.956781\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.956788\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.956794\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.956800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.956807\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.956813\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.956819\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.956826\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.956832\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.956838\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.956845\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.956851\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.956857\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.956864\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.956854\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.956860\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.956845\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.956852\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.956858\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.956864\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.956871\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.956877\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.956883\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.956890\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.956896\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.956902\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.956909\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.956915\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.956921\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.956928\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.956934\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.956940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114152, acc: 0.956946\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.956953\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.956959\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.956965\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.956972\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.956978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.956984\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.956991\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.956997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957003\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.957009\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957016\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.957022\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957028\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.956961\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.956953\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.956959\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.956966\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.956972\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.956978\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.956985\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.956991\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.956997\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957003\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957010\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957016\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.957022\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957028\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957035\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.957041\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957047\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.957053\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957060\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.957066\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.957056\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.957062\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.957069\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.957075\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957081\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.957087\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957094\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.957100\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957106\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.957112\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.957119\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.957125\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.957131\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.957137\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.957143\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.957150\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.957156\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.957162\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.957168\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.957175\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957036\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957006\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.957012\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.957018\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.957014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957021\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.957019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.957025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957031\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957021\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957027\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957034\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957040\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957046\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.957052\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957058\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957065\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.957071\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957077\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.957083\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.957090\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.957096\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957030\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.957036\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957042\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957054\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957061\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957067\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957044\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957050\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957057\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957063\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957069\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957075\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957045\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957058\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957048\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957054\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957060\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957066\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957073\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957079\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957085\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957091\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957097\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957104\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957110\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957116\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957122\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957062\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957069\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957060\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957067\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957073\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957079\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957085\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957091\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957026\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957032\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.956990\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.956996\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957002\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957009\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957015\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957021\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957013\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957019\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957031\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957037\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957044\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957050\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957056\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957062\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.956925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.956931\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.956865\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.956871\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.956878\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.956884\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.956890\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.956753\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.956759\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.956765\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.956771\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.956763\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.956626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.956632\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.956495\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.956502\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.956508\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.956514\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.956520\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.956526\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.956533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.956539\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.956497\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.956504\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.956510\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.956516\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.956522\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.956529\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.956535\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.956541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.956547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.956553\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.956560\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.956566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.956572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.956578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.956584\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.956591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.956597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.956603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114902, acc: 0.956609\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.956615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.956622\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.956628\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.956616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.956622\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.956628\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.956635\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.956570\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.956576\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.956582\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.956588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.956594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.956601\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.956607\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956599\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.956605\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.956611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.956617\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.956623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.956630\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.956636\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.956626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.956632\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.956638\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.956645\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.956651\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.956657\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.956663\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.956653\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.956645\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.956652\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.956658\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.956664\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.956670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.956676\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.956682\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.956688\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115275, acc: 0.956624\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.956630\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.956636\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.956642\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.956648\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.956654\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.956661\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.956667\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.956602\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.956608\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.956614\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.956620\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.956627\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.956633\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.956639\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.956645\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.956651\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115281, acc: 0.956586\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.956593\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.956599\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.956605\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.956611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.956617\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.956623\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.956629\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.956636\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.956642\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.956648\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.956654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.956660\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.956666\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.956531\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.956537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.956543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.956549\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.956556\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.956562\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.956568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.956574\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.956557\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.956534\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.956541\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.956547\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.956553\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.956559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.956565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.956571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.956577\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.956583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.956590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.956596\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.956602\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.956608\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.956473\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.956479\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.956485\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115211, acc: 0.956492\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.956482\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.956488\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.956494\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.956500\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.956507\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.956513\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.956519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.956525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.956531\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.956537\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.956543\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.956549\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.956555\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.956562\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.956568\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.956560\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115217, acc: 0.956566\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.956572\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.956578\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.956584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.956590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.956596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.956602\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.956609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.956615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.956621\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.956627\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.956617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.956623\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.956629\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.956636\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.956571\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.956578\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.956584\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.956590\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.956582\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115315, acc: 0.956588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.956594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.956600\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.956606\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.956612\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.956618\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.956624\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956630\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.956637\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.956643\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.956649\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.956655\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.956645\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.956651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956657\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.956663\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.956669\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.956676\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.956682\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.956688\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.956694\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.956700\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.956706\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.956712\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.956578\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.956584\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.956590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.956596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.956602\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.956609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.956615\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.956621\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.956627\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.956493\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956430\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.956436\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.956442\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.956448\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.956454\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.956460\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956466\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.956472\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.956478\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.956484\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.956490\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115149, acc: 0.956496\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.956502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.956508\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.956515\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.956521\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.956527\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.956533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.956539\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.956545\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.956537\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.956529\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.956535\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.956541\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.956547\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.956553\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.956559\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.956565\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.956502\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.956508\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.956514\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.956520\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.956526\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.956532\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.956538\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.956544\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115305, acc: 0.956536\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.956542\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.956548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115257, acc: 0.956554\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.956546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.956553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.956559\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.956565\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115315, acc: 0.956571\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115299, acc: 0.956577\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.956583\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115406, acc: 0.956575\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115390, acc: 0.956581\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.956587\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115358, acc: 0.956593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115342, acc: 0.956599\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.956611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.956617\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.956623\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.956629\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.956635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.956641\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.956647\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.956653\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.956659\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.956665\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.956671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.956677\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.956683\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.956689\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.956626\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.956614\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.956620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.956626\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.956632\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.956638\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.956644\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115390, acc: 0.956635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.956641\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115358, acc: 0.956647\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115342, acc: 0.956653\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.956659\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.956665\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115432, acc: 0.956602\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115416, acc: 0.956608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115401, acc: 0.956614\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.956620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.956626\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115353, acc: 0.956632\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.956638\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.956644\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115305, acc: 0.956650\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.956656\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.956662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115258, acc: 0.956668\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.956674\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.956680\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.956686\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.956692\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.956697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.956703\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.956709\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.956715\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.956721\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.956727\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.956733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.956739\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.956745\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.956751\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.956757\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.956763\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.956769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.956775\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.956781\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.956787\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.956793\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.956799\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.956804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.956810\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.956816\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114846, acc: 0.956822\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.956828\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.956834\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.956840\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.956846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.956852\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.956858\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.956850\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.956856\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.956862\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.956868\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.956874\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.956880\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.956885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.956891\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.956897\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.956903\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.956909\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.956915\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.956921\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.956927\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.956933\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.956939\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.956944\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.956950\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.956956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.956962\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.956951\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.956957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.956963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.956969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.956974\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.956980\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.956986\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.956992\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.956998\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957004\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957010\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957016\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957021\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957027\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957033\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957025\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957031\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957037\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957043\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957049\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957055\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957060\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957066\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957072\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957078\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957084\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957090\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957096\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957101\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957107\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957113\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957119\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957125\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957131\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957136\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957142\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957148\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957154\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957160\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957166\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957171\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957177\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.957183\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957053\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957044\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957050\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957055\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957061\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957067\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957056\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957062\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957068\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957073\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957079\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957085\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957091\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957097\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957102\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957063\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957001\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.956993\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.956999\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957005\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957011\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957023\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957006\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.956992\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.956998\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957004\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.957010\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.957002\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957008\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957014\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957006\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.956998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957004\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957010\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.956971\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.956976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.956982\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.956988\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.956994\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957005\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957029\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957034\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957040\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957033\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.956971\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115281, acc: 0.956977\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.956982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.956988\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.956994\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957000\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957006\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957017\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957023\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957029\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957035\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957040\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957046\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957052\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957058\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957069\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957075\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957081\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957087\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.957093\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957031\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957043\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957054\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957060\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957066\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957071\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957077\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957083\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957089\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957095\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957100\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957091\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957097\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957103\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.957108\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.957114\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.957120\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957126\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957098\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957104\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957109\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957115\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.957121\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957127\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957132\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.957138\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957144\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957150\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957088\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957094\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957100\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957106\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957111\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957117\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957123\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957129\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957134\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957140\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957146\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957152\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957157\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957163\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957169\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957180\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957186\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957192\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957197\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957203\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957209\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957215\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957220\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957159\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957165\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957171\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957176\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957182\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.957171\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957163\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957169\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957175\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957181\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957186\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957192\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957153\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957159\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.957165\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957170\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957182\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957188\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957193\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957186\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957191\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957064\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957070\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957075\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957081\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957087\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957092\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957098\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957104\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957110\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957115\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.957121\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957127\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957119\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957125\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957130\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957136\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957142\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957148\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957153\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957159\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957165\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957170\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957176\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957182\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114889, acc: 0.957187\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957199\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957204\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957210\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957216\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957221\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957227\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957233\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957239\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957244\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957250\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957256\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957261\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957267\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957273\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957278\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957284\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957290\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957295\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957306\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957299\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957290\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957282\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957294\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957299\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.957305\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.957311\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957316\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.957322\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957333\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957339\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957212\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957201\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957207\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957213\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957218\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957224\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.957230\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957235\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957241\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957247\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114867, acc: 0.957252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957258\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957263\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957269\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957275\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957280\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957260\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957265\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957271\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.957277\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957282\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957288\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957293\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957299\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957305\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957310\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957316\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.957289\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957294\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957300\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.957292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957298\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957304\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957309\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957296\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957302\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957307\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957313\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.957318\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957330\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957335\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957341\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.957347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.957352\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957363\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.957369\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957375\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957249\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957254\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957260\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957266\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957271\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.957277\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957282\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957288\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957299\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957305\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957310\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957316\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957322\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957327\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957338\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957344\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957361\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957366\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957372\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957378\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957383\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957389\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957394\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957400\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957392\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957398\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957403\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957409\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957420\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957426\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957431\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957437\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957442\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957448\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957454\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957465\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957470\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957459\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957438\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957425\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957430\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957436\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957442\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957447\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957453\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957458\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957464\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957469\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957475\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957480\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957486\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957491\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957432\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957437\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957443\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957318\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957324\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957329\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957319\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957330\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.957270\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957276\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957281\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957287\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957304\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114854, acc: 0.957309\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957298\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114954, acc: 0.957304\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957309\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957315\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.957343\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.957348\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957354\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957359\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957365\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957370\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957376\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957381\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957387\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957393\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957398\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957274\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957279\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957285\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957276\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957282\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957287\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.957293\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957304\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957309\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957320\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957326\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957331\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957348\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957354\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.957343\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957354\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957327\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.957338\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957344\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957349\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957355\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957360\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957366\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957371\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957377\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957382\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957388\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957393\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957399\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957404\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957410\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957415\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957426\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957432\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957437\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957443\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957448\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957454\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957459\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957465\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957341\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957352\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957358\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957363\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957304\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957310\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957315\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957321\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957326\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957332\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957343\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957354\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957359\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957365\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957370\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957376\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957381\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957387\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957392\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957398\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957403\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957409\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957414\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957419\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957425\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957430\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957436\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957423\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957428\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957434\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957439\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957381\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957386\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957392\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957403\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957408\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957413\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957419\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957424\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957430\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957435\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957441\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957446\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957439\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957426\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957431\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957437\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957430\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957435\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957440\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957446\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957451\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957329\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957340\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957345\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957351\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957362\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957358\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957364\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957375\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957380\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957385\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957391\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957396\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957402\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957407\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957418\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957429\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957307\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957312\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957318\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957323\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957329\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957334\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957345\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957361\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957367\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957372\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957378\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957383\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957388\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957394\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957399\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957405\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957410\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957416\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957357\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957349\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957354\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957359\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957365\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957370\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957376\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957381\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957387\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957392\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957397\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957403\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957408\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957414\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957405\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957410\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957416\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957421\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957414\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957419\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957425\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957430\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957435\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957441\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957446\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957452\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957457\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957450\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957455\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957461\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957466\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957471\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957477\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957482\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957488\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957493\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957498\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957504\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957509\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957451\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957457\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957462\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957467\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957473\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957478\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957483\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957489\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957494\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957500\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957505\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957510\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957516\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957521\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957526\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957532\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957537\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957416\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957422\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957427\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957432\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957311\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957317\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957196\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957201\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957207\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957212\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957217\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957223\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957228\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957234\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957239\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957244\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957250\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957255\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957261\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957266\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957271\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957277\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957282\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957288\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957272\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957277\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957283\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957288\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957279\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957285\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957296\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957301\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957264\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957270\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957263\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957268\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957273\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957279\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957284\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957289\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957295\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957300\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957306\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957311\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957301\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957306\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957311\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957254\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957259\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957270\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957275\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957281\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957272\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957277\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957283\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957294\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957299\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957310\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957320\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957326\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957331\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957328\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957333\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957338\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957281\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957287\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957297\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957308\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957313\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957319\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957324\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957329\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957335\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957309\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957314\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957319\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957325\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957346\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957367\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957373\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957378\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957383\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957389\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957394\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957405\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957410\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957415\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957421\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957426\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957431\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957412\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957417\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957422\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957427\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957433\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957438\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957443\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957449\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957454\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957459\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957465\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957470\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957475\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957481\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957486\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957491\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957496\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957507\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957512\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957518\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957528\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957533\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957526\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957532\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957542\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957547\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957553\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957558\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957532\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957537\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957543\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957548\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957559\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957567\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957578\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957583\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957588\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957581\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957592\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957607\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957613\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957618\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957623\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957628\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957634\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957644\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957649\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957655\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957660\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957665\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957547\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957568\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957578\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957594\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957604\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957610\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957615\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957625\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957630\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957636\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114268, acc: 0.957641\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957651\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957644\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957637\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957642\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957524\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957529\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957535\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957540\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957545\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957550\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957556\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957561\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957566\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957568\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957573\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957579\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957571\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957577\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957587\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957531\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957536\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957541\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957546\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957552\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957562\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957567\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957573\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957522\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957527\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957532\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957542\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957548\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957553\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957558\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957569\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957579\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957572\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957587\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957598\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957603\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957608\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957614\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957619\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957624\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957629\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957639\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957645\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957650\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957655\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957676\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957681\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957686\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957630\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957641\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957646\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957639\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957644\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957649\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957654\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957659\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957665\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957670\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957680\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957685\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957690\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957634\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957645\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957650\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957655\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957660\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957666\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957676\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957681\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957686\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957691\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957697\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957702\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.957707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957712\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957717\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957722\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957728\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957738\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957743\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.957748\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957769\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.957774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957779\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957744\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.957754\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114115, acc: 0.957759\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.957774\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957780\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.957785\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957790\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957795\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.957805\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957795\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.957800\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.957816\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957821\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957826\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.957836\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.957841\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957730\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.957680\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957685\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957690\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957695\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.957700\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.957705\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957716\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957721\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.957726\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957731\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.957736\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.957741\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957746\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957751\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957757\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957767\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957656\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957661\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957653\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957658\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.957663\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957655\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957665\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.957670\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957663\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957608\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957568\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957573\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957578\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957583\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957588\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957598\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957604\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957548\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957559\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957564\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957569\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957574\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957579\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957584\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957594\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957600\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957593\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957598\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957608\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957613\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957606\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957611\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957621\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957627\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957632\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957642\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957647\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957652\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957667\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957672\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957678\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957683\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957688\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957678\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957683\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957688\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957693\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957698\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957703\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957708\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957658\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957663\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957669\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957674\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957679\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957684\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957689\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957694\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957709\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957719\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957724\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957718\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957723\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957693\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957698\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957703\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957708\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957713\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957718\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957693\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957703\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957713\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957719\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957724\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957729\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957709\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957719\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957724\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957714\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957679\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957684\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957689\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957694\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957700\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957691\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957696\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957701\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957712\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957717\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957722\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957727\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957732\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957737\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957682\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957687\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957692\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957697\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957672\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957678\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957683\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957688\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957693\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957703\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957708\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957713\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957718\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957723\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957728\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957733\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957743\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957634\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957639\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957644\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957649\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957654\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957659\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957664\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957669\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957674\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957679\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957684\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957690\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957700\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957705\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957710\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957715\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957720\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957725\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957730\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957740\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957745\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957750\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957770\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957775\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957780\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957795\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957800\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.957805\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.957810\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957798\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957808\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957813\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957818\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957823\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957828\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.957833\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.957838\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957843\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957848\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.957858\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957863\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957829\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957834\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957839\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114086, acc: 0.957844\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.957849\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957854\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.957859\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957869\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957874\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957879\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957884\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.957889\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.957893\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957898\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957849\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.957842\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957847\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957841\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957851\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957856\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957861\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.957870\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957875\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.957880\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957874\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957879\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.957884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957889\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957893\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957898\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.957903\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957908\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957913\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957918\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957923\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.957928\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957933\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.957938\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957943\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957948\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.957953\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957958\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.957963\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957968\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.957973\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957978\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957983\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.957988\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.957993\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.957998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958003\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958022\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.958014\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958019\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.958024\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957990\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.957995\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.958000\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958005\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.958010\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958019\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958024\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114010, acc: 0.957971\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113997, acc: 0.957975\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.957980\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.957985\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957990\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957971\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.957964\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957969\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957974\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957979\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957971\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957858\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957863\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114292, acc: 0.957868\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957878\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957888\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957893\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957898\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957903\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.957913\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957918\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957922\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.957927\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957932\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.957937\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957884\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.957889\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957893\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957898\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.957908\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.957913\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114115, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957916\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957921\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957926\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957931\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957928\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957933\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957938\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957943\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957935\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957939\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114292, acc: 0.957944\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957936\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957941\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957951\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957956\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957961\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957966\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957961\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957971\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957976\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957981\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957986\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957990\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957995\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.958000\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957889\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957893\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957898\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957908\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957900\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957905\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957897\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957902\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957907\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957858\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957863\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957868\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957873\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957878\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957825\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957830\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957796\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.957793\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957786\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957801\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957806\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114870, acc: 0.957810\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.957815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957820\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957825\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957830\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957835\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957832\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957837\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.957842\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957847\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957735\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957740\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957745\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114889, acc: 0.957760\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114863, acc: 0.957770\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957775\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957773\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957778\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957782\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957787\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957792\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.957797\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.957807\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957812\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957817\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957827\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957831\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957836\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957841\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957833\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957838\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.957848\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957858\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957862\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957867\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957872\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957877\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957882\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957887\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957776\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957781\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957786\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.957733\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957726\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.957731\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.957736\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957741\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957746\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957751\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957756\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957760\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.957765\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114870, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.957775\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957780\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957785\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957795\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957799\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957809\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957814\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957819\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957833\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957838\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957848\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957853\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957858\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957863\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957867\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957872\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957877\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957882\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957887\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957892\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957781\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957786\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957777\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957782\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957791\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957801\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957806\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957816\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957820\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957814\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957819\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957824\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957828\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957833\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957838\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957843\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957853\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957845\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957849\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957854\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957859\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957869\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957874\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957879\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957888\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957893\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957898\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957907\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957798\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957807\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957817\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957822\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957809\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957813\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957709\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957723\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957733\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957738\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957742\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957747\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957752\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957757\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957724\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957733\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957738\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957705\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957710\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957715\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957719\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957724\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957729\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957734\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957739\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957744\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957753\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957763\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957768\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957772\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957777\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957782\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957801\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957806\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957811\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957702\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957695\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957700\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957705\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957710\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957719\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957724\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957615\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957630\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957634\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957639\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957649\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957654\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957663\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957668\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957673\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957678\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957683\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957687\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957692\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957697\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957707\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957711\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957716\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957721\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957726\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957735\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957740\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957745\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957750\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957755\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957759\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957769\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957774\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957779\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957783\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957788\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957782\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957786\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957801\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957806\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957791\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957796\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957801\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957815\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957825\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957830\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957834\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.957839\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957844\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957837\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957786\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957779\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957737\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957742\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957746\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957751\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957749\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957754\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957759\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957764\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957769\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957778\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957783\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957797\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957802\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957802\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957807\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957812\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957816\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957821\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957831\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957835\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957840\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957845\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957850\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957854\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957869\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957873\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957878\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957883\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957892\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957897\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957902\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957907\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957911\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957916\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957921\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957926\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957930\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957935\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957940\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957944\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957949\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957954\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957959\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957968\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957973\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957978\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114096, acc: 0.957982\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957987\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.957992\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.957996\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958001\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.958006\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958011\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.958020\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.958025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958029\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.958034\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958044\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958053\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957946\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957950\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957955\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.957960\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.957964\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.957969\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.957974\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957983\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.957993\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.957997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958002\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958007\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958016\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958021\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958026\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958035\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958040\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958035\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958040\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958022\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958027\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.958032\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958036\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958041\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958046\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957999\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.958004\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.958008\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958013\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957906\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.957911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.957915\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957909\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.957914\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957918\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957928\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957933\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957937\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.957947\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.957951\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957919\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957923\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.957933\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957938\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.957942\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.957952\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.957956\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957961\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957966\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.957970\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.957975\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957924\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.957929\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957933\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113971, acc: 0.957938\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957943\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.957952\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.957957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.957962\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.957966\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957971\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.957976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.957980\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.957985\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.957990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.957994\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.957999\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958004\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958008\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958013\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958022\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958027\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958032\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958041\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958046\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.958055\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958060\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958069\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.957963\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.957967\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.957972\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.957977\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.957981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.957986\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.957990\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.957995\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958000\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958004\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958009\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958018\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113551, acc: 0.958023\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113538, acc: 0.958028\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958032\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.957982\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.957986\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.957991\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.957996\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958000\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958005\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.957973\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.957977\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.957982\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.957976\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.957980\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.957985\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.957978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.957988\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.957992\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.957997\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958002\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958006\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958016\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958020\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958025\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958030\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.958034\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958039\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958043\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958048\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.958053\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.958057\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958062\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958021\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.958025\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958030\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958035\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958039\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.957989\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.957998\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958003\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.957952\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.957957\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.957851\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.957856\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.957860\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.957755\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.957759\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.957769\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.957778\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.957783\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.957797\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.957801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.957806\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.957810\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.957820\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.957774\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.957779\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957783\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.957788\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.957738\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.957742\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.957747\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.957752\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.957756\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.957761\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.957770\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.957775\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.957779\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.957784\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.957793\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.957798\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.957803\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.957807\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.957812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.957816\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.957785\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.957780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.957789\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.957794\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.957805\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.957810\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.957814\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957801\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957810\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.957814\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.957819\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.957824\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.957828\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.957833\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.957842\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.957847\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957851\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.957856\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.957861\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.957865\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.957815\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.957820\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.957824\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.957829\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.957834\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.957838\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.957847\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.957852\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.957857\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.957861\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.957866\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.957870\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.957875\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.957867\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.957872\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.957877\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.957881\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.957886\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.957891\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.957900\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.957904\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.957909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.957913\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.957918\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.957927\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.957932\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.957936\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.957941\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.957946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.957950\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.957955\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.957948\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.957953\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.957958\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.957962\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.957967\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.957935\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.957940\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.957944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.957949\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.957953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.957958\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.957967\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.957972\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.957976\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.957981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.957985\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.957990\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.957995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113530, acc: 0.957999\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113518, acc: 0.958004\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.958008\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.958013\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958017\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113468, acc: 0.958022\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113456, acc: 0.958027\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113444, acc: 0.958031\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113540, acc: 0.957981\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.957986\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.957990\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.957995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.958000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.958004\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113466, acc: 0.958009\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113454, acc: 0.958013\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113441, acc: 0.958018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113429, acc: 0.958022\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958016\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.957966\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.957971\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.957976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.957980\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.957974\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.957978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.957983\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.957879\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.957888\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.957893\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.957897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.957902\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.957906\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.957911\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.957915\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.957920\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.957925\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.957929\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.957934\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.957938\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.957943\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.957947\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.957940\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.957944\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.957889\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.957893\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.957844\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.957848\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.957853\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957857\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.957862\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.957866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.957871\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.957875\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.957880\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.957885\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.957889\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.957894\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.957898\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957907\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.957912\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.957813\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.957817\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957822\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.957831\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.957836\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.957840\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.957845\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.957849\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.957777\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.957787\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.957737\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.957742\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.957746\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957751\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.957755\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.957760\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.957769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957774\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.957778\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.957783\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.957787\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.957792\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957760\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.957765\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957770\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957779\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.957747\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.957752\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.957756\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.957761\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.957712\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957705\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957715\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957719\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.957724\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957728\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957737\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957746\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957751\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957769\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957766\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957771\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.957775\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957780\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957784\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.957789\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.957793\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.957798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.957803\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957807\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.957812\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.957816\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.957821\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957825\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.957830\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.957834\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.957827\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957831\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.957787\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957791\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.957796\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.957747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.957751\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.957760\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957774\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957778\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.957792\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957797\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.957801\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.957806\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.957810\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.957815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.957819\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.957824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957828\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.957833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.957837\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.957846\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.957797\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.957802\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.957811\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.957815\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.957824\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.957722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957726\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.957731\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957740\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957744\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.957736\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113971, acc: 0.957740\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.957749\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957754\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957763\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.957767\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.957772\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.957776\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.957781\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.957785\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957773\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957742\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.957746\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957751\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957755\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957653\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957658\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957662\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957667\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957676\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957680\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.957685\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957636\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957641\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957645\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957650\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114152, acc: 0.957654\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.957659\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114128, acc: 0.957663\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114115, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.957672\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.957677\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957666\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957670\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.957675\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.957679\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957675\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957680\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957684\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.957689\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957693\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957698\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957702\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957707\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957605\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957609\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957607\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957611\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957616\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.957625\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957629\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957527\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957532\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957536\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957541\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957545\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957550\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957555\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957559\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957564\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957568\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957573\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957577\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957582\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957598\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957602\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957607\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.957594\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957598\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957607\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957612\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957621\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957630\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957586\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957590\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957577\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957586\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957591\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957595\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957600\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957604\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.957609\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957613\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957618\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957626\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957631\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957635\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957640\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.957644\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957649\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957653\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.957667\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.957671\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957676\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.957680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.957685\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957689\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957694\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957702\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957707\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957716\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957720\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957725\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.957729\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.957734\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957726\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957735\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.957740\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957744\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.957749\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.957753\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.957766\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.957771\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.957775\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.957780\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.957784\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.957793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.957798\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.957802\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.957806\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.957815\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.957829\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.957821\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.957826\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.957830\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.957835\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.957839\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.957844\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.957848\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.957852\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.957857\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.957861\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.957866\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.957870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.957874\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.957879\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.957783\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.957787\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.957801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.957805\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.957809\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.957814\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.957818\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.957823\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.957827\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.957831\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.957836\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.957840\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.957792\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.957793\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.957797\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.957801\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.957780\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.957784\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.957793\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.957802\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.957804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.957809\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.957813\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.957818\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.957822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.957827\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.957835\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.957840\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.957844\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.957849\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.957853\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.957857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.957866\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.957871\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.957875\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.957879\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.957884\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.957888\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.957893\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.957887\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.957891\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.957895\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.957900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.957904\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.957909\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.957913\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.957917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.957922\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.957926\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.957930\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.957935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113556, acc: 0.957939\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.957944\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.957948\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.957952\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113508, acc: 0.957957\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113497, acc: 0.957961\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.957966\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113473, acc: 0.957970\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113461, acc: 0.957974\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113449, acc: 0.957979\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113437, acc: 0.957983\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113426, acc: 0.957987\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113414, acc: 0.957992\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113506, acc: 0.957892\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113494, acc: 0.957896\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113483, acc: 0.957901\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.957905\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113459, acc: 0.957910\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113447, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113435, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113424, acc: 0.957923\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113412, acc: 0.957927\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113400, acc: 0.957931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113388, acc: 0.957936\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113376, acc: 0.957940\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113365, acc: 0.957945\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113353, acc: 0.957949\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113341, acc: 0.957953\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113329, acc: 0.957958\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113317, acc: 0.957962\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113306, acc: 0.957966\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113294, acc: 0.957971\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113282, acc: 0.957975\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113374, acc: 0.957876\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113362, acc: 0.957880\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113351, acc: 0.957884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113339, acc: 0.957889\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113327, acc: 0.957893\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113315, acc: 0.957898\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113304, acc: 0.957902\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113292, acc: 0.957906\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113280, acc: 0.957911\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113268, acc: 0.957915\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113257, acc: 0.957919\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113245, acc: 0.957924\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113233, acc: 0.957928\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113221, acc: 0.957932\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113210, acc: 0.957937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113198, acc: 0.957941\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113186, acc: 0.957946\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113174, acc: 0.957950\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113163, acc: 0.957954\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113151, acc: 0.957959\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113139, acc: 0.957963\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113127, acc: 0.957967\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113116, acc: 0.957972\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113208, acc: 0.957924\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113196, acc: 0.957929\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113184, acc: 0.957933\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113172, acc: 0.957937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113161, acc: 0.957942\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113149, acc: 0.957946\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113137, acc: 0.957950\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113125, acc: 0.957955\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113217, acc: 0.957907\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113206, acc: 0.957912\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113194, acc: 0.957916\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113182, acc: 0.957920\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113274, acc: 0.957890\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113262, acc: 0.957895\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113251, acc: 0.957899\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113239, acc: 0.957903\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113227, acc: 0.957908\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113319, acc: 0.957901\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113307, acc: 0.957905\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113295, acc: 0.957909\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113284, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113272, acc: 0.957918\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113260, acc: 0.957922\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113249, acc: 0.957927\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113237, acc: 0.957931\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113225, acc: 0.957935\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113213, acc: 0.957940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113202, acc: 0.957944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113190, acc: 0.957948\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113178, acc: 0.957953\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113167, acc: 0.957957\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113258, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113247, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113235, acc: 0.957918\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113223, acc: 0.957923\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113211, acc: 0.957927\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113200, acc: 0.957932\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113291, acc: 0.957926\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113280, acc: 0.957930\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113371, acc: 0.957921\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113359, acc: 0.957926\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113348, acc: 0.957930\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113336, acc: 0.957934\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113428, acc: 0.957928\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.957829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.957834\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.957838\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113587, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.957847\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.957851\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113552, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.957808\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.957813\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.957817\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113608, acc: 0.957821\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.957826\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.957830\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.957834\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.957839\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.957843\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113538, acc: 0.957848\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113526, acc: 0.957852\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.957856\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.957861\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.957865\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.957869\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113468, acc: 0.957874\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113456, acc: 0.957878\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113445, acc: 0.957882\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113433, acc: 0.957887\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113421, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113409, acc: 0.957895\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113398, acc: 0.957900\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113386, acc: 0.957904\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113374, acc: 0.957908\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113363, acc: 0.957913\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113351, acc: 0.957917\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113442, acc: 0.957911\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113431, acc: 0.957915\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113419, acc: 0.957920\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113407, acc: 0.957924\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113396, acc: 0.957928\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113384, acc: 0.957933\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113372, acc: 0.957937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113361, acc: 0.957941\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113349, acc: 0.957946\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113337, acc: 0.957950\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113326, acc: 0.957954\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113314, acc: 0.957958\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113303, acc: 0.957963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113291, acc: 0.957967\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113279, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113268, acc: 0.957976\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113256, acc: 0.957980\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113244, acc: 0.957984\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113233, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113221, acc: 0.957993\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113312, acc: 0.957946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113300, acc: 0.957950\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113289, acc: 0.957955\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113277, acc: 0.957959\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113266, acc: 0.957963\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113254, acc: 0.957968\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113242, acc: 0.957972\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113231, acc: 0.957976\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113219, acc: 0.957980\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113208, acc: 0.957985\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113196, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113184, acc: 0.957993\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113173, acc: 0.957998\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113161, acc: 0.958002\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113252, acc: 0.957904\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113343, acc: 0.957857\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113434, acc: 0.957810\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113422, acc: 0.957814\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113411, acc: 0.957819\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.957721\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.957725\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.957729\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.957723\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.957728\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.957721\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.957725\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113625, acc: 0.957729\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.957734\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.957738\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113578, acc: 0.957747\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.957700\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.957704\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.957708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.957713\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.957717\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.957721\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.957726\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.957730\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.957722\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.957726\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.957730\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.957734\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.957739\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.957743\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.957747\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.957756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.957760\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113539, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113527, acc: 0.957778\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113516, acc: 0.957782\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113504, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.957791\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.957789\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.957793\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113548, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.957751\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.957653\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.957657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.957662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.957666\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.957670\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.957679\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.957683\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.957688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113625, acc: 0.957692\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.957696\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.957701\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.957705\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.957607\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.957612\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.957620\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.957625\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.957629\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.957633\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.957638\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.957642\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.957646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113565, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.957655\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.957659\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113530, acc: 0.957663\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113518, acc: 0.957668\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113507, acc: 0.957672\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113495, acc: 0.957676\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.957630\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.957634\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.957638\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.957633\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.957641\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.957645\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.957650\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.957654\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.957658\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.957663\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.957667\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113537, acc: 0.957671\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113526, acc: 0.957676\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113514, acc: 0.957680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.957684\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.957655\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.957557\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.957562\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.957566\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.957570\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.957559\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.957563\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.957567\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.957561\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.957566\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.957545\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.957549\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.957553\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957558\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957461\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957465\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957469\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957473\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.957478\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.957482\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957486\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.957491\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957495\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957499\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957504\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957508\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957512\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.957517\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.957525\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.957529\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.957534\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.957538\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.957542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.957547\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.957551\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.957505\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.957509\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957502\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113942, acc: 0.957506\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.957511\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.957515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.957519\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.957523\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.957528\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.957532\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.957440\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957444\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.957448\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.957452\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.957457\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.957461\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957465\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.957470\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.957474\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.957478\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.957483\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.957487\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.957491\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957495\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.957500\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.957504\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.957497\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.957501\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.957506\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.957460\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.957464\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957468\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.957439\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.957443\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957447\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957401\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957406\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957410\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957414\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957318\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114110, acc: 0.957322\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957331\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957335\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957339\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957343\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957348\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957352\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.957361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.957365\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.957359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.957363\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957368\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957372\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957381\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957385\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957389\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957393\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957398\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.957392\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957396\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957375\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114136, acc: 0.957380\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957388\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.957393\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957386\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957390\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957394\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957398\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957403\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.957407\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957411\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957416\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957420\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957413\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.957417\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957422\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957426\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.957430\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.957434\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957439\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957443\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957447\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.957451\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.957456\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957460\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.957464\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957468\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.957477\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.957471\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.957475\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957480\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957473\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957477\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957481\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957486\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957490\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957494\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.957498\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957453\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957457\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957461\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957455\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957450\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957454\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957458\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957462\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957460\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957464\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957468\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957473\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957477\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957481\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957485\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957490\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957494\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957498\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957502\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957507\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957511\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957505\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957509\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957514\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957518\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957522\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957526\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957530\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957535\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957543\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957552\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957506\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957510\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957515\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957519\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957523\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957527\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957531\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957515\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957507\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957511\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957515\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957508\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957513\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957517\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957521\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957530\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957534\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957538\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957542\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957546\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957551\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957555\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957559\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957514\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957518\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957522\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957526\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957531\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957535\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957489\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957493\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957498\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957502\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957495\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957499\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957504\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957508\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957512\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957516\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957520\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957525\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957529\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957542\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957546\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957554\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957558\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957567\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957575\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957579\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957574\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957578\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957586\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957591\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957599\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957607\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957612\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957620\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957624\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957633\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957637\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957641\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957645\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957649\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957654\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114136, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957612\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957621\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957625\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957629\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957633\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114156, acc: 0.957638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.957642\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957646\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957650\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957654\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957465\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957370\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957366\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957371\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957375\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957379\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957373\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957378\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957382\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957386\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957390\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957399\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957403\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957407\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957411\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957415\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957420\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957424\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957428\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957432\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957424\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957428\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957433\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957437\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957441\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957445\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957449\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957454\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957458\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957462\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957466\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957460\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957465\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957469\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957473\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957477\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957481\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957486\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957490\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957494\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957498\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957502\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957506\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957511\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957515\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957523\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957527\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957532\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957536\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957540\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957544\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957548\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957540\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957544\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957561\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957569\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957578\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957582\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.957586\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114123, acc: 0.957590\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.957594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957599\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.957603\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957607\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957611\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.957615\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957610\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.957614\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957618\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957622\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957626\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957630\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957634\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957643\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.957651\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.957659\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.957631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.957635\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957590\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.957598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957603\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957607\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114115, acc: 0.957611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.957615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.957619\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.957623\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.957628\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.957636\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957640\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.957644\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.957648\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957652\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.957657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.957665\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957669\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957673\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957677\n",
      "target: tensor([0.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957388\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957392\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957396\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957401\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957405\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.957409\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957364\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957368\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957372\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957381\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957385\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957389\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957361\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957365\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957369\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957373\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957377\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957382\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957386\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957390\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957394\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957398\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957402\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957407\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957411\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.957415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957419\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.957423\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.957427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.957431\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957436\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.957440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.957444\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.957437\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.957441\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114110, acc: 0.957445\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957450\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.957454\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957458\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.957462\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957466\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957470\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957474\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.957479\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957473\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957477\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.957481\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.957485\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.957490\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957494\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957498\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957502\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957506\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.957510\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.957504\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957508\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.957512\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957520\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.957524\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957528\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957533\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957541\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957545\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.957549\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.957521\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957525\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.957529\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.957533\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.957537\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.957546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.957554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957558\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.957562\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.957570\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.957574\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957568\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.957572\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.957564\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957568\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.957580\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957584\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957588\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957593\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.957597\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.957601\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.957605\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957609\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113942, acc: 0.957613\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.957617\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.957621\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.957630\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.957634\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.957638\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.957631\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.957639\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.957643\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.957647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.957652\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.957656\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.957649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113971, acc: 0.957653\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.957657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.957661\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957665\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957670\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.957674\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.957678\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.957682\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957686\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957593\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.957602\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957606\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957610\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957614\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.957618\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957622\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.957626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.957630\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.957634\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.957638\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.957642\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.957647\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.957651\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.957655\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.957659\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.957663\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.957667\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.957671\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.957675\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.957670\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.957674\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.957678\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957682\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.957686\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.957690\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.957598\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.957606\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.957610\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.957614\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.957607\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.957611\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.957616\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.957620\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.957624\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.957628\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.957632\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.957636\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.957640\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.957644\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.957637\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.957642\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.957646\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.957650\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.957654\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957658\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.957662\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.957666\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.957674\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.957678\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.957682\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.957686\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.957690\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.957695\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.957699\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.957703\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.957707\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.957699\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.957703\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.957707\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.957701\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.957705\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.957709\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.957714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.957718\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.957722\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.957726\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.957730\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.957734\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.957738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.957742\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.957746\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.957750\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.957754\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.957747\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.957752\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.957756\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.957760\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.957768\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.957772\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.957776\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.957780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.957788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.957792\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.957796\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.957800\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.957808\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.957812\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.957816\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.957820\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.957824\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.957828\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.957832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.957837\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113547, acc: 0.957841\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.957835\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.957839\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.957843\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.957836\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.957845\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.957849\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.957853\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.957857\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113630, acc: 0.957861\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.957865\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.957859\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.957863\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.957857\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.957861\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.957865\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.957869\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.957873\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.957877\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.957881\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.957885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.957889\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.957882\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.957886\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957890\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.957894\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.957898\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.957893\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.957897\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957885\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.957845\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957849\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.957843\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957847\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957759\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.957763\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.957767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957771\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.957779\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.957783\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957776\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957780\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957784\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957788\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.957796\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.957800\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957808\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.957812\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957816\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.957809\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.957813\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.957817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957821\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957825\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957820\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957824\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.957828\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.957832\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114082, acc: 0.957836\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.957840\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957844\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957848\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.957852\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957856\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.957860\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.957864\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.957868\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.957861\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957865\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.957869\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957873\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.957877\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957868\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957872\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957876\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.957880\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.957872\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.957876\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957880\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.957884\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114128, acc: 0.957888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957892\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.957896\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957889\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957862\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957834\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957838\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957842\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957755\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957759\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957763\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957771\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957779\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957783\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957787\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957780\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957792\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957804\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957808\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957811\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957784\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957792\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957701\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957705\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957709\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957713\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957717\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957721\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957725\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957729\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957733\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957737\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957741\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957736\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957740\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957744\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957752\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957756\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957760\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957772\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957776\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957780\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957736\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957740\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957744\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957748\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957752\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957756\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957760\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957768\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957776\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957780\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957784\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957788\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957812\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957816\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957820\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957824\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.957828\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957832\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957836\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957840\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.957848\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.957852\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957809\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957813\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957821\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957825\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.957829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957833\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.957837\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.957841\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.957845\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.957849\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957758\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957715\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957719\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957723\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957731\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957735\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957739\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.957743\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.957747\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.957751\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957755\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957759\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957767\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957771\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.957779\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957783\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957787\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957791\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.957795\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957799\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.957803\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957807\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.957811\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957815\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957818\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957822\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113937, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957830\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.957834\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.957838\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.957842\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.957823\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957827\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.957831\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957835\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957839\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957843\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957846\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.957850\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.957854\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.957858\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.957862\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.957857\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.957861\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.957865\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113922, acc: 0.957869\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957873\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.957877\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957881\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.957884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.957888\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.957892\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.957896\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.957900\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.957904\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.957815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.957818\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.957822\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.957826\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.957830\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.957834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.957838\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.957842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.957846\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.957850\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.957854\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.957858\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.957776\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.957780\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.957784\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.957792\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.957800\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.957804\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.957808\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.957812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.957816\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.957820\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.957828\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.957835\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.957746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.957750\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.957754\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.957758\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.957756\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.957760\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.957764\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.957768\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.957725\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.957729\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.957737\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.957735\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.957739\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957696\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957700\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957699\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957703\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957706\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957710\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.957714\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.957718\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957722\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.957726\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957730\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957734\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957738\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957742\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.957746\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.957750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.957758\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.957762\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.957769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.957773\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957731\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.957746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.957750\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.957762\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.957766\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.957770\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.957774\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.957778\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.957782\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.957786\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.957790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.957794\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.957797\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.957801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.957809\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.957803\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.957811\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.957722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.957726\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.957730\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.957734\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.957737\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.957741\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.957699\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.957703\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.957707\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.957715\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.957719\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.957722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.957726\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.957730\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.957703\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.957707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.957711\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957715\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.957719\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.957723\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.957727\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.957731\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.957739\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.957750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.957766\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.957770\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.957774\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.957778\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.957782\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.957785\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.957780\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.957784\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.957788\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.957792\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.957800\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.957804\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.957811\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.957815\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.957819\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.957823\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.957827\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.957831\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.957835\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.957839\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113552, acc: 0.957846\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.957850\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113531, acc: 0.957854\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.957858\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113511, acc: 0.957862\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113500, acc: 0.957866\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.957870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113479, acc: 0.957874\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.957877\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.957881\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.957885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113437, acc: 0.957889\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113427, acc: 0.957893\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113416, acc: 0.957897\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113406, acc: 0.957901\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113395, acc: 0.957905\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113385, acc: 0.957909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113375, acc: 0.957912\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113364, acc: 0.957916\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113446, acc: 0.957910\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113435, acc: 0.957914\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113425, acc: 0.957918\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113414, acc: 0.957922\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113404, acc: 0.957925\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113394, acc: 0.957929\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113383, acc: 0.957933\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113465, acc: 0.957891\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113454, acc: 0.957895\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.957807\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.957811\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.957814\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.957818\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113494, acc: 0.957822\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113484, acc: 0.957826\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113473, acc: 0.957830\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113463, acc: 0.957834\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113452, acc: 0.957838\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.957796\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113523, acc: 0.957800\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113513, acc: 0.957803\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.957807\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113492, acc: 0.957811\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113482, acc: 0.957815\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.957819\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113461, acc: 0.957823\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.957818\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.957821\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.957825\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113511, acc: 0.957829\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.957833\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.957837\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.957841\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.957845\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113459, acc: 0.957849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.957852\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113438, acc: 0.957856\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113428, acc: 0.957860\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113417, acc: 0.957864\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113407, acc: 0.957868\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113396, acc: 0.957872\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113386, acc: 0.957876\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113376, acc: 0.957879\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113365, acc: 0.957883\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113355, acc: 0.957887\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113344, acc: 0.957891\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113334, acc: 0.957895\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113324, acc: 0.957899\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113405, acc: 0.957893\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113486, acc: 0.957887\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113476, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113465, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113455, acc: 0.957899\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113445, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113434, acc: 0.957906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113424, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113413, acc: 0.957914\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113403, acc: 0.957918\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113393, acc: 0.957922\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113474, acc: 0.957880\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.957842\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.957846\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.957808\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.957811\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.957815\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.957819\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.957823\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.957827\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113543, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.957835\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113522, acc: 0.957838\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113512, acc: 0.957842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.957846\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.957850\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.957854\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113470, acc: 0.957858\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113460, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113449, acc: 0.957865\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113439, acc: 0.957869\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113428, acc: 0.957873\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113510, acc: 0.957831\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113499, acc: 0.957835\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113489, acc: 0.957839\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.957843\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113468, acc: 0.957847\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.957851\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113447, acc: 0.957854\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113437, acc: 0.957858\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113427, acc: 0.957862\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113416, acc: 0.957866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113406, acc: 0.957870\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113395, acc: 0.957874\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113385, acc: 0.957878\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113375, acc: 0.957881\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113456, acc: 0.957874\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113445, acc: 0.957878\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113435, acc: 0.957882\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113425, acc: 0.957885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113414, acc: 0.957889\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113404, acc: 0.957893\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113394, acc: 0.957897\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113383, acc: 0.957901\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113373, acc: 0.957905\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113363, acc: 0.957908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113352, acc: 0.957912\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113342, acc: 0.957916\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113423, acc: 0.957907\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113504, acc: 0.957902\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.957895\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.957899\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.957903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.957907\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113543, acc: 0.957911\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113533, acc: 0.957915\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113522, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113512, acc: 0.957922\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113502, acc: 0.957926\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.957930\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.957924\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.957927\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113551, acc: 0.957931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.957935\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113531, acc: 0.957939\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.957943\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113510, acc: 0.957947\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113500, acc: 0.957950\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113489, acc: 0.957954\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113479, acc: 0.957958\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.957962\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.957966\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.957970\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113438, acc: 0.957973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113427, acc: 0.957977\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113417, acc: 0.957981\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113407, acc: 0.957985\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113396, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113386, acc: 0.957993\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113376, acc: 0.957996\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113365, acc: 0.958000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113355, acc: 0.958004\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113345, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113335, acc: 0.958012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113324, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113314, acc: 0.958019\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113304, acc: 0.958023\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113384, acc: 0.958018\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113374, acc: 0.958022\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113364, acc: 0.958025\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113353, acc: 0.958029\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113343, acc: 0.958033\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113333, acc: 0.958037\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113322, acc: 0.958041\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113312, acc: 0.958045\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113302, acc: 0.958048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113292, acc: 0.958052\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113281, acc: 0.958056\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113271, acc: 0.958060\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113261, acc: 0.958064\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113341, acc: 0.958045\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113331, acc: 0.958048\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113321, acc: 0.958052\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113310, acc: 0.958056\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113300, acc: 0.958060\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113381, acc: 0.958046\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113370, acc: 0.958049\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113360, acc: 0.958053\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113350, acc: 0.958057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113339, acc: 0.958061\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113329, acc: 0.958065\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113319, acc: 0.958068\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113399, acc: 0.958063\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113389, acc: 0.958067\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113379, acc: 0.958071\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113368, acc: 0.958075\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113358, acc: 0.958078\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113348, acc: 0.958082\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113338, acc: 0.958086\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113327, acc: 0.958090\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113317, acc: 0.958094\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113307, acc: 0.958097\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113296, acc: 0.958101\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113286, acc: 0.958105\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113367, acc: 0.958100\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113356, acc: 0.958103\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113346, acc: 0.958107\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113336, acc: 0.958111\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113325, acc: 0.958115\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113315, acc: 0.958119\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113305, acc: 0.958122\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113295, acc: 0.958126\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113284, acc: 0.958130\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113274, acc: 0.958134\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113264, acc: 0.958138\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113254, acc: 0.958141\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113243, acc: 0.958145\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113233, acc: 0.958149\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113223, acc: 0.958153\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113213, acc: 0.958157\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113202, acc: 0.958160\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113283, acc: 0.958154\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113272, acc: 0.958158\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113262, acc: 0.958162\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113252, acc: 0.958165\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113242, acc: 0.958169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113231, acc: 0.958173\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113221, acc: 0.958177\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113301, acc: 0.958170\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113291, acc: 0.958174\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113281, acc: 0.958178\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113271, acc: 0.958182\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113260, acc: 0.958186\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113341, acc: 0.958159\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113330, acc: 0.958163\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113320, acc: 0.958167\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113400, acc: 0.958162\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113390, acc: 0.958165\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113380, acc: 0.958169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113369, acc: 0.958173\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113359, acc: 0.958177\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113349, acc: 0.958180\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113339, acc: 0.958184\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113329, acc: 0.958188\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113409, acc: 0.958102\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113398, acc: 0.958105\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113388, acc: 0.958109\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113378, acc: 0.958113\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113368, acc: 0.958117\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113357, acc: 0.958120\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113347, acc: 0.958124\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113337, acc: 0.958128\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113327, acc: 0.958132\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113316, acc: 0.958136\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113306, acc: 0.958139\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113386, acc: 0.958098\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113376, acc: 0.958102\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113366, acc: 0.958106\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113446, acc: 0.958099\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113436, acc: 0.958103\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113425, acc: 0.958107\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113415, acc: 0.958111\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113405, acc: 0.958114\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.958028\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113475, acc: 0.958032\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113464, acc: 0.958036\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958030\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.958034\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958038\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113514, acc: 0.958042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.958045\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958044\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958048\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113552, acc: 0.958052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.958055\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958059\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113522, acc: 0.958063\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113512, acc: 0.958067\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958070\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.958074\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958078\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.958082\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113461, acc: 0.958086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113450, acc: 0.958089\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113440, acc: 0.958093\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113430, acc: 0.958097\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113420, acc: 0.958101\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113409, acc: 0.958104\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113399, acc: 0.958108\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113479, acc: 0.958082\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.958086\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113459, acc: 0.958090\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.958093\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958075\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113608, acc: 0.958033\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113587, acc: 0.958041\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958045\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958048\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113547, acc: 0.958056\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958060\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113526, acc: 0.958064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113516, acc: 0.958067\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113506, acc: 0.958071\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113496, acc: 0.958075\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.958079\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113475, acc: 0.958082\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958064\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958067\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.958071\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958075\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.958070\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958074\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958066\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958070\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958074\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958077\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958081\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958089\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958092\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958096\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958100\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958104\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958107\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113551, acc: 0.958111\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958115\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113530, acc: 0.958119\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.958122\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958117\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958121\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958125\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958129\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958132\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958136\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113539, acc: 0.958140\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958144\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113518, acc: 0.958147\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113508, acc: 0.958151\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958144\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958103\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958106\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958110\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958024\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958028\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958032\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958043\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958047\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958051\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958054\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958058\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958062\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958066\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958059\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958063\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958067\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958071\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958074\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958078\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958082\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958086\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958079\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958083\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958087\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958091\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958094\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958098\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958102\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113630, acc: 0.958106\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958109\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958113\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958117\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958121\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958124\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958128\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958132\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958136\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113539, acc: 0.958139\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958054\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113608, acc: 0.958058\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958061\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958065\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113578, acc: 0.958069\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958028\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.957943\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.957946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.957950\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.957954\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.957958\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.957932\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.957935\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.957939\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.957943\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.957947\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.957941\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957901\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.957904\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.957897\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957856\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957860\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957864\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957867\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957871\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.957838\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957842\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957849\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.957857\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.957861\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.957864\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.957868\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957872\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.957875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957879\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.957870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.957874\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957878\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957882\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957845\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957848\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957852\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957856\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957837\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957752\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957760\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.957764\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.957767\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957753\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957761\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957765\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957768\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957728\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957723\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957726\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957730\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957734\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957738\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957741\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957745\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957749\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957753\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957756\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957747\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957751\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957759\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957674\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957677\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957681\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957685\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957689\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957515\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957519\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957523\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957527\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957527\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957531\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957490\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957494\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957498\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957501\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957505\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957509\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957513\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957487\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957491\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957495\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957498\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957502\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957506\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957510\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957513\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957517\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957521\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957525\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957528\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957444\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957448\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957451\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957455\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957463\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957466\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957470\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957474\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957469\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957473\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957476\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957480\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957484\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957443\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957447\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957451\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957455\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957458\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957462\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957466\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957470\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957473\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957477\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957481\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957485\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957488\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957492\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957496\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957500\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957503\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957507\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957511\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957515\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957518\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957434\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957438\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957442\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957445\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957449\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957453\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957457\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957460\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957464\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957468\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957471\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957431\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957435\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957439\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957442\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957446\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957450\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957454\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957449\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957452\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957456\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957460\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957464\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957467\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957461\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957421\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957429\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957432\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957436\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957440\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957400\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957403\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957407\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957411\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957415\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957418\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957422\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957426\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957430\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957433\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957437\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957432\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957436\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957439\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957443\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957447\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957451\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957454\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957458\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957462\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957466\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957469\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957477\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957480\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957475\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957479\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957483\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957487\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957490\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957494\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957498\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957509\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957513\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957516\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957520\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957524\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957535\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957542\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957477\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957481\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957485\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957488\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957492\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957486\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957490\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957494\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957497\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957501\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957417\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957421\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957425\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957341\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957345\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957340\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957347\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957355\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957358\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957362\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957366\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957341\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957344\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957336\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957339\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957343\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957347\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957354\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957358\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957318\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957282\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957286\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957289\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957293\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957300\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957308\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957312\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957315\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957319\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957323\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957326\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957316\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957324\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957328\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957335\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957339\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957342\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957334\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957337\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957341\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957345\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957349\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957352\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957356\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957360\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957363\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957367\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957371\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957375\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957378\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957382\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957386\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957389\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957393\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957397\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957400\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957404\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957408\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957403\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957407\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957410\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957414\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957418\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957392\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957396\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957230\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.957234\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957245\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957248\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957256\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957260\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957255\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.957258\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957262\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957266\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957269\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957273\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957277\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957281\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957241\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957235\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957239\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957242\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957246\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957250\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.957245\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.957249\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114942, acc: 0.957252\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957256\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.957260\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957263\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114902, acc: 0.957267\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957271\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.957231\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957235\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957239\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957234\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957237\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957241\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957236\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957218\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.957213\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957217\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957134\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957138\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957142\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115272, acc: 0.957145\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957149\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.957153\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957156\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957160\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957164\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957167\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.957171\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957179\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957182\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957186\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957190\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957193\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957201\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957204\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957208\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957212\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957216\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957219\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957223\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957230\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957234\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957238\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957241\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957245\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957249\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957252\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957256\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114963, acc: 0.957260\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957264\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957267\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.957271\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957275\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957282\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.957286\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957289\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957284\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957288\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.957292\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957295\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957299\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.957303\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957306\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957310\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.957314\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957317\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957321\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957324\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.957331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957335\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.957338\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.957342\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957349\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957353\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957357\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957368\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.957371\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957375\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957379\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957382\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957380\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957384\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957387\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957391\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957395\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957398\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957402\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957409\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957417\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957420\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957424\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957428\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957431\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957426\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957430\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957434\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957437\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957441\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957445\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957448\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957452\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957459\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957463\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957466\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957470\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957462\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957465\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957469\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957472\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957476\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957480\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957398\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957359\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957362\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.957369\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957373\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957380\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957384\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957388\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957391\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957395\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957402\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957406\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957410\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957413\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957417\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957421\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957424\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957428\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957431\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957435\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957439\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957433\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957428\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957432\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957435\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957439\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957421\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957425\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957428\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957432\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957436\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957439\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957443\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957361\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957365\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957368\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957372\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957376\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957379\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957340\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957344\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957355\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957359\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957369\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957373\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957380\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957384\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957388\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957391\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957386\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957390\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957394\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957397\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957401\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957408\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957412\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957415\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957423\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957426\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957430\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957423\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957418\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957422\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957425\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957429\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957433\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957436\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957440\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957443\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957447\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957451\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957458\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957462\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957465\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957469\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957472\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957476\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957480\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957483\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957487\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957490\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957494\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957498\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957501\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957420\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957424\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957427\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957431\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957434\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957438\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957442\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957445\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957449\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957452\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957371\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957375\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957378\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957297\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957304\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957308\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957315\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957319\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957326\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957330\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957325\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957332\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957336\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957339\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957343\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957347\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957354\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957357\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957365\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957368\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957372\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957375\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957379\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957383\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957386\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957390\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957394\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957397\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957401\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957404\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957408\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957412\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957415\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957408\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957412\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957415\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957411\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957414\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957418\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957421\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957429\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957432\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957436\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957439\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957443\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957447\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957450\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957454\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957457\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957461\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957465\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957468\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957463\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957467\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957470\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957474\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957468\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957472\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957475\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957479\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957483\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957486\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957481\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957485\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957489\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957492\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957496\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957499\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957506\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957510\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957471\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957475\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957479\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957398\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957401\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957405\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957409\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957412\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957416\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957419\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957430\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957434\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957437\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957441\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957445\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957448\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957452\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957455\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957462\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957466\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957470\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957477\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957480\n",
      "target: tensor([7.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957463\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957466\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957470\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957474\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957477\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957481\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957484\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957488\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957492\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957495\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957499\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957506\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957509\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957504\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957507\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957514\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957518\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957521\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957529\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957532\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957536\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957539\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957543\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957546\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957550\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957557\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957564\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957568\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957575\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957589\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957596\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957600\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957603\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957607\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957611\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957614\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957576\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957579\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957583\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.957590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957604\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957608\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957611\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957615\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.957618\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957625\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957629\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957608\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957612\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957615\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957619\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957591\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957595\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957598\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957602\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957609\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957612\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.957620\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.957627\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957630\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957637\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957599\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957603\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957606\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957610\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957613\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114128, acc: 0.957620\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957624\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957627\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957631\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957625\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957629\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957632\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957636\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957639\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957643\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957646\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.957650\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957653\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957661\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.957664\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957668\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957671\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.957633\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.957636\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957632\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957627\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957589\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957592\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957596\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957603\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957606\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957610\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957613\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957617\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957611\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957618\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957538\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957542\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957546\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957549\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957556\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957560\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957563\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957567\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957570\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957577\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.957581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957584\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957588\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957591\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957512\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957515\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957519\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957511\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957514\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957518\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957521\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957528\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957532\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957535\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957542\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957549\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957553\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957556\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957563\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957567\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957571\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957574\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957581\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957588\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957592\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114136, acc: 0.957595\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957599\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.957602\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957606\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957609\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.957613\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.957616\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.957620\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.957627\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957630\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957634\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957644\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957648\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.957651\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.957655\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957658\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.957665\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957669\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.957662\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957666\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.957664\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.957640\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957647\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957654\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957650\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.957653\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114156, acc: 0.957657\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957660\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957664\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957667\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957674\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957678\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.957681\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.957685\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.957688\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.957692\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957695\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957690\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957694\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.957697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957701\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957704\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957708\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957711\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.957706\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.957710\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114123, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957717\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.957720\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.957724\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.957727\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.957734\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957741\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.957745\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957748\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957752\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957718\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.957721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957725\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114128, acc: 0.957649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957653\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957643\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.957650\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957653\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.957657\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957660\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957664\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957658\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957500\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957504\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957507\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957511\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957514\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957518\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957521\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957528\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957532\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957535\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957539\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957542\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957546\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957549\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957560\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957567\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114156, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957574\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114128, acc: 0.957581\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957584\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.957591\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.957598\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.957602\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.957605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957609\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957612\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.957616\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.957578\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.957582\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.957585\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.957592\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957596\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.957599\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957602\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957606\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957593\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957596\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957600\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.957603\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957607\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.957610\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.957614\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.957621\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.957624\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.957631\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.957635\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.957630\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.957637\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.957640\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114110, acc: 0.957644\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957647\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.957651\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957627\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957631\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957634\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957592\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957587\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957590\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957594\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957601\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957604\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957608\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957601\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957605\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957608\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957612\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957618\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957622\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957617\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957621\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957624\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957628\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957641\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957645\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957648\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957652\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957659\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957662\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957666\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957669\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957598\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957605\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957608\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957612\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957615\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957619\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.957622\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957626\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957629\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957632\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957636\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957639\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957643\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957612\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957606\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957613\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957619\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957611\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957615\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957618\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957622\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957629\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957635\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957639\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957642\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957646\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957649\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957653\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957656\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957663\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957667\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957670\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957673\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.957677\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.957680\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.957684\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.957687\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.957691\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957694\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957698\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.957701\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.957708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.957711\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.957715\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957718\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.957722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.957725\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957729\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.957732\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.957742\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.957746\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.957749\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957756\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.957760\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957763\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.957766\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.957770\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.957777\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.957780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.957784\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.957787\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.957794\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.957801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.957804\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957770\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.957774\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.957777\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.957772\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957775\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957779\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957792\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.957796\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.957765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.957691\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957613\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957617\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.957624\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.957627\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957631\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957634\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.957644\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.957648\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.957655\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.957661\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.957665\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.957672\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.957668\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.957672\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.957675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.957679\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957682\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.957686\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.957689\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.957692\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.957696\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.957703\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.957706\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.957710\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.957632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.957636\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113997, acc: 0.957629\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.957632\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957636\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957639\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957634\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957641\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.957645\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957648\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.957652\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.957655\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.957662\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957665\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.957669\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957672\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.957676\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.957679\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957674\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957678\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.957681\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.957685\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957648\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957654\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957658\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957661\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.957665\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.957668\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.957672\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.957678\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957682\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957685\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.957689\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.957692\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.957695\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.957699\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.957702\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.957706\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.957709\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.957712\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.957716\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957709\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.957713\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.957716\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.957719\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957642\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.957649\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.957644\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957647\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957650\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.957657\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.957664\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957647\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957642\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.957645\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.957649\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.957652\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.957655\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.957578\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957555\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957558\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957562\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957569\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.957575\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957579\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957542\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957549\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957552\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957559\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957562\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957569\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957565\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957575\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957578\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957582\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957585\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.957588\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.957592\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.957595\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.957599\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.957602\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957597\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.957588\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957592\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957595\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957598\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.957605\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957609\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957612\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957619\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957622\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957626\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957629\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957632\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957636\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957643\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957646\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957641\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957645\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957648\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957643\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957646\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957649\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957653\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957576\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957577\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957581\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957504\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957508\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957511\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957514\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957518\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957521\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957525\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957508\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957515\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957518\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957522\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957525\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957488\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957492\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957495\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957499\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957502\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957505\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957429\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957432\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957416\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957342\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957346\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957349\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957356\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957360\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957355\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957365\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957369\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957332\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957335\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957299\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957302\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957306\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957309\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957319\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957323\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957330\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957333\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957336\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957340\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957343\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957347\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957360\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957364\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957367\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957370\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957334\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957341\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957348\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957351\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957346\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957357\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957360\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957363\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957367\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957370\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957374\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957377\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957380\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957384\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957387\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957390\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957397\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957401\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957404\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957407\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957335\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957338\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957334\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957337\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957340\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957304\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957307\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957311\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957314\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957317\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957321\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957301\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957305\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957308\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957311\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957322\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957325\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957328\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957335\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957330\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957336\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957340\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957343\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957347\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957342\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957345\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957349\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957352\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957356\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957359\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957362\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957286\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957290\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957293\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957297\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957300\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957303\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957307\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957310\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957313\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957317\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957320\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957327\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957334\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957337\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957341\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957347\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957351\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957354\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957325\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957331\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957335\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957338\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957341\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957345\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957348\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957352\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957355\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957358\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957362\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957326\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957329\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957253\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957257\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957251\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957255\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957258\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957261\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957257\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957260\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957264\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957270\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957274\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957269\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957273\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957276\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957279\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957283\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957277\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957281\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957284\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957268\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957271\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957274\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957278\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957281\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957285\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957288\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957291\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957295\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957305\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957308\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957311\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957318\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957314\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957317\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957320\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957324\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957248\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957252\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957255\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957258\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957262\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957265\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957269\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957272\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957271\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957274\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957277\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957281\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957288\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957283\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.957286\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957290\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957293\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957218\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957221\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957215\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957218\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957221\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957225\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957218\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957216\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957220\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957226\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.957230\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114982, acc: 0.957233\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957197\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957201\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957204\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957207\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957211\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957175\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957178\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957181\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957185\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957178\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957182\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957185\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957188\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957192\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957195\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957198\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957202\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957197\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957201\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957204\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957207\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957211\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957214\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957218\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957221\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957228\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957231\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957234\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957238\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957241\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957244\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957248\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957251\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957254\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957258\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.957261\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957264\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957268\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957271\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957275\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957239\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.957242\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957245\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957240\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957243\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957240\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957244\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957237\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.957241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957244\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.957237\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957241\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957244\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957248\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957254\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957258\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957183\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957186\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957189\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957196\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957199\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957203\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957167\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115231, acc: 0.957170\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957174\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957177\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957180\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957175\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115333, acc: 0.957172\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115324, acc: 0.957175\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115315, acc: 0.957179\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957156\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957159\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957163\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115358, acc: 0.957166\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957169\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115340, acc: 0.957173\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.957176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957179\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957183\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957147\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.957150\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115364, acc: 0.957154\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957157\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115346, acc: 0.957160\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957164\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957167\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115397, acc: 0.957163\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115388, acc: 0.957166\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.957169\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115370, acc: 0.957173\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957176\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115352, acc: 0.957179\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957183\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957186\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115325, acc: 0.957189\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957193\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957196\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957188\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957192\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115358, acc: 0.957195\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115427, acc: 0.957172\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957176\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957179\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957174\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957177\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115538, acc: 0.957172\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957176\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957179\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957182\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957108\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957111\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115561, acc: 0.957115\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957118\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957121\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.957125\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957128\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115516, acc: 0.957131\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957135\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957138\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957141\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957145\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957148\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957151\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957155\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.957158\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115436, acc: 0.957161\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115427, acc: 0.957165\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957168\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957171\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115400, acc: 0.957175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957178\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115459, acc: 0.957174\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115528, acc: 0.957168\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957172\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115510, acc: 0.957175\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957178\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957182\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115483, acc: 0.957185\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115474, acc: 0.957188\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957192\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115456, acc: 0.957195\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115447, acc: 0.957198\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115516, acc: 0.957193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957196\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957200\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957203\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957206\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957209\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957213\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957216\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.957219\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957223\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.957226\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115417, acc: 0.957229\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957233\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115400, acc: 0.957236\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957239\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957243\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.957246\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115364, acc: 0.957249\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957253\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115346, acc: 0.957256\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957259\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957263\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115319, acc: 0.957266\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957269\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957273\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115292, acc: 0.957276\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.957279\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957283\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.957286\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957289\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115247, acc: 0.957292\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.957296\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.957299\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957302\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957306\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957309\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957305\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957308\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957311\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957314\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957318\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957313\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115295, acc: 0.957317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.957320\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.957323\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957330\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957333\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957337\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957340\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.957343\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.957346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957350\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957345\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957349\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957274\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115325, acc: 0.957278\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957281\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957284\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.957288\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957291\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957294\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115272, acc: 0.957298\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115263, acc: 0.957301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957304\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957311\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.957314\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957317\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.957313\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.957316\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115269, acc: 0.957320\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957323\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957329\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957333\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.957336\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.957339\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957343\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.957346\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957349\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957359\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957362\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957366\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957353\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115263, acc: 0.957352\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957355\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957358\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957362\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.957365\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957364\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.957367\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115269, acc: 0.957370\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957374\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957377\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115319, acc: 0.957369\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957376\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.957379\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.957382\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115275, acc: 0.957386\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957389\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115257, acc: 0.957392\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957396\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957399\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.957402\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957405\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957409\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957412\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957419\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957422\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957387\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957390\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957393\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957396\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.957400\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957403\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957406\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957410\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957413\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957416\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957419\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957423\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957426\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957429\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957433\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957436\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957442\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957407\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957410\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957414\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957417\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957420\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957427\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957430\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957408\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957411\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957414\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957418\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957424\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957427\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957431\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957434\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957437\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957444\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957447\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957450\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957454\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957457\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957460\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957463\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957467\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957470\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957473\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957476\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.957480\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957483\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.957486\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957489\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957493\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957496\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957499\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957503\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957506\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957509\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957512\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957516\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957514\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.957518\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957521\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957524\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957534\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.957512\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957515\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.957518\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957528\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957524\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957527\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.957530\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957533\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957537\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957540\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.957543\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957546\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957473\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957476\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957441\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957444\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957437\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957443\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957447\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957450\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957453\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957456\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957460\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957463\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957458\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957461\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957464\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957429\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957425\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957428\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957431\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957434\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957438\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957441\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957444\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957448\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957451\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957454\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957457\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957461\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957464\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957467\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957470\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957465\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.957468\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957472\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957475\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957478\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957481\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957446\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957450\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957453\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957456\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.957459\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957463\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957466\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957469\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957472\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957476\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957479\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957482\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957485\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957489\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957492\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957487\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957490\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957493\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957496\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957499\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957503\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957506\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957509\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957512\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957516\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957511\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957515\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957518\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957521\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957524\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957527\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957531\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957534\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957537\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957544\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957547\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957542\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957546\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957549\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957552\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957548\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957551\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957554\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957561\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957556\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957560\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957563\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957566\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957569\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957572\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957579\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957585\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957592\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957570\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957576\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957541\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957545\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957548\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957551\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957554\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957558\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957561\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957564\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957567\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957570\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957569\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957572\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957579\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957582\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957578\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.957581\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957587\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957591\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957594\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957600\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957603\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957607\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957610\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957613\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957620\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957629\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957625\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957628\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957631\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957634\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957638\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957641\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957636\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957632\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957635\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957639\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.957642\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957645\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957648\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115149, acc: 0.957651\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957658\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957661\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957664\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957667\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957674\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957677\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957680\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957683\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957687\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957690\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957693\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957665\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957668\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957674\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957678\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957681\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957684\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957687\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957690\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957694\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957700\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957703\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957706\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114963, acc: 0.957710\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.957716\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957719\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957722\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957716\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.957719\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957726\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957721\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957725\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957715\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957722\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957725\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957728\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957731\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957734\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957738\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115008, acc: 0.957741\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114999, acc: 0.957744\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114982, acc: 0.957750\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957749\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115022, acc: 0.957755\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957759\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957768\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.957771\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114970, acc: 0.957775\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957781\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.957790\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957794\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957800\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957803\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957810\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.957813\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957808\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957815\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957818\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.957812\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957815\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957818\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957813\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957816\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957819\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957822\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957826\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957829\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957832\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957760\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.957766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957769\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957773\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957776\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957779\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957782\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.957785\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957788\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.957792\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.957798\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957804\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957807\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957811\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957814\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957817\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.957820\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957823\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957751\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957755\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957758\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957761\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957764\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957767\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957770\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957774\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957766\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.957732\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957738\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957741\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957744\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957751\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957757\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957752\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957755\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957761\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957765\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957768\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957771\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957699\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114999, acc: 0.957705\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957709\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957712\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957677\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957672\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957654\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957657\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957663\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957666\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957670\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957665\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957631\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.957634\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957637\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957644\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957647\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.957650\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.957653\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957656\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957660\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.957663\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957666\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957669\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115149, acc: 0.957672\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957678\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957682\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957685\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957688\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957691\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957694\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957697\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957701\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957695\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957702\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957705\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957708\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957711\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957714\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957718\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957721\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957724\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957730\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957737\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957740\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957743\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957746\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.957755\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957759\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.957762\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957768\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957771\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957774\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957781\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957776\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114963, acc: 0.957779\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957781\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957790\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957793\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957718\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957721\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957724\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957730\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957733\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957736\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.957740\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957743\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957746\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957755\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957758\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957765\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957768\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957771\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957700\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957666\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957669\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957672\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957675\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957678\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957681\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957685\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957688\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957691\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957657\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957660\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957663\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957666\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957669\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957673\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957676\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957679\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957682\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957611\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957614\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957617\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957612\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957615\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957618\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957588\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957597\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957600\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.957529\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957532\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957535\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957539\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957548\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957551\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957554\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957557\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957561\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957567\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957570\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957573\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957568\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957571\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957577\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957581\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957584\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957587\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957593\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957596\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957599\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957603\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957606\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957609\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957612\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.957615\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957618\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957621\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957631\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957637\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957640\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957643\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957650\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957644\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957650\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957653\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957592\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957590\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957489\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957492\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957495\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957498\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957501\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.957505\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957508\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957511\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957514\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957517\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957520\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957523\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957527\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957530\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957533\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957536\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957539\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.957542\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957545\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957549\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957552\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957555\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957521\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957524\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957530\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957534\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957463\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957458\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957461\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957464\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.957467\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957470\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957474\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957477\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957480\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957483\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957486\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957489\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957492\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957496\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957499\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957502\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957505\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957508\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957511\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957514\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957510\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957513\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957516\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957514\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957518\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957521\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957524\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957530\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957525\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957531\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957534\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957537\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957541\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957544\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957547\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957550\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957553\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957519\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957523\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957526\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957521\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957525\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957528\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957531\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.957534\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957507\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957510\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957513\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957516\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957519\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957522\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957525\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957528\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957535\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957538\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957541\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957544\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957547\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957550\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957556\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957566\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957572\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957575\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957581\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957588\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957591\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.957594\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957599\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957598\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957601\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957604\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957607\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957610\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957613\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.957616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957620\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957618\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957622\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957617\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957620\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957624\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957627\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957630\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957633\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957636\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957639\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.957642\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957636\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957639\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957642\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957645\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957648\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957652\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957655\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957658\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957661\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957664\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957667\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957673\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957676\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957679\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957683\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957686\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957689\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957692\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.957695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957698\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957701\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957707\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957710\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957713\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957717\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957720\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.957723\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957726\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957729\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957732\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957735\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957738\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957741\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.957744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957747\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957677\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957681\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957684\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957687\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957690\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957693\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957696\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957699\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.957702\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957700\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957703\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957706\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957713\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957716\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114863, acc: 0.957719\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114927, acc: 0.957685\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957616\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957619\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957622\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957618\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957621\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957624\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957627\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957630\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957633\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957636\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957639\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957642\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957645\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.957644\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957638\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957641\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957644\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957647\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957654\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957623\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957602\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957581\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957584\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115211, acc: 0.957587\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957590\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.957593\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957596\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.957592\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957595\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957598\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115281, acc: 0.957571\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957574\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.957577\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957581\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957587\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115231, acc: 0.957590\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957596\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957599\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.957602\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957608\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957611\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957614\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115155, acc: 0.957618\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.957621\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957624\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957627\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957630\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957633\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957636\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957639\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957642\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957645\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957648\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957654\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957658\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957588\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957591\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957594\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957600\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957531\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957534\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957537\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957540\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957536\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957503\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957506\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957509\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.957512\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957518\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957521\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957524\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957527\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957520\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957523\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957526\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957522\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957525\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957528\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957524\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115329, acc: 0.957527\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.957530\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957533\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957536\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115295, acc: 0.957539\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957546\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.957549\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957555\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957558\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957561\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.957567\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957570\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957569\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957575\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.957578\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.957548\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957551\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957554\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957558\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115257, acc: 0.957564\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957567\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957570\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957573\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957576\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.957579\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957594\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957570\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957574\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957577\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957580\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957583\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957586\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957589\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957592\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957598\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957601\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957604\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957607\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957610\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957613\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957616\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957619\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957622\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957626\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957629\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957632\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957635\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957638\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957635\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957614\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957551\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115155, acc: 0.957563\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957569\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957575\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957579\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957582\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957585\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957588\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957591\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957594\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957597\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957600\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957603\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957606\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957609\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115022, acc: 0.957612\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957618\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957621\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957624\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957627\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957630\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957633\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957637\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.957640\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957643\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957646\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957649\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957652\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957619\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957586\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957589\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957592\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957595\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957598\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957604\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957607\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957577\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957580\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957584\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957579\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.957547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957550\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957553\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957556\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957559\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957562\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957571\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957574\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957572\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957575\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957578\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957584\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957587\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957593\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957561\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957564\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957567\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957570\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957573\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957582\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957585\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957588\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957583\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957586\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957589\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957592\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957571\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957577\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957580\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957583\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957590\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957593\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957524\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957527\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957530\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957533\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957542\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957545\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957548\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957554\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957563\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957567\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957573\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957576\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957582\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957588\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957594\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957597\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957593\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957596\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957599\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957602\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957569\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957575\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957578\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957574\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957569\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957572\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115231, acc: 0.957566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957575\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957578\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.957581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957585\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957588\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957591\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957597\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957600\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957603\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957606\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957609\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957540\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957543\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957549\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957556\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957559\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957557\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957561\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957564\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957567\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957570\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.957573\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957576\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957588\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957591\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957597\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957600\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957603\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957606\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957609\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957541\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957550\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957553\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957538\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957505\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957508\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957511\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957443\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115311, acc: 0.957438\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957441\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.957444\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.957448\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.957451\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.957454\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957457\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957460\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115245, acc: 0.957463\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957466\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957469\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.957472\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957475\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957478\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957474\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957477\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957480\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957483\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957486\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957489\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957492\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.957495\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957498\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957501\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957504\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957510\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957513\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957516\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957519\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957522\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957525\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957528\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957531\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957534\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957537\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957546\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957549\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957552\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957555\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957558\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957561\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115022, acc: 0.957564\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957567\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957570\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957573\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957505\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957508\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957514\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957517\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957449\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957445\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957440\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957443\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.957446\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.957449\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957416\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957419\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957422\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957428\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957431\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957434\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.957437\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957440\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957443\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957446\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957449\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957452\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957455\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957458\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957461\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957464\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957467\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957470\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957473\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957476\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957479\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957482\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957485\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957488\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957492\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957495\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957498\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957501\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957504\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957507\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957510\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957513\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957516\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957519\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957522\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957528\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957531\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957534\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.957537\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957540\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957543\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957549\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.957552\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.957555\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957558\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114863, acc: 0.957561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957564\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.957573\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957543\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957549\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957552\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.957555\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957558\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957561\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957564\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957567\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957573\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957576\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957579\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.957582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957585\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957588\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957591\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957594\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957597\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957600\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957596\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957599\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957602\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957605\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957608\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957611\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957614\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957617\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957620\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957623\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957626\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957629\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957632\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957638\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957644\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957647\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957650\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957653\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957656\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957659\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957662\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957665\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957668\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957671\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957674\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957677\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957673\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957676\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957679\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957682\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957684\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957683\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957686\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957689\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957692\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957695\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957698\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957710\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957713\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957716\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957719\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957722\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957725\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957728\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957731\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957699\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957702\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957705\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957708\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957711\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957707\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957710\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957713\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957716\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957719\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957722\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957724\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957727\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957730\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957733\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957736\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957739\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957745\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957748\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957751\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957745\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957741\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957743\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957749\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957752\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957755\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957761\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957767\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957770\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957706\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957709\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957712\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957715\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957724\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957718\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957721\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957724\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957727\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957730\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957726\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957729\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957732\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957735\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957706\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957709\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957712\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957717\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957720\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957723\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957726\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957729\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957732\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957738\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957741\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957744\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957747\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957750\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957753\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957756\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957759\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957692\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957695\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957698\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957701\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957704\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957713\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957716\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957718\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957721\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957724\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957730\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957733\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957736\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957739\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957742\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957745\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957748\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957751\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957731\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957734\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957708\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957710\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957716\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957711\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957714\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957717\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957719\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957722\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957725\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957728\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957731\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957734\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957737\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957740\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957743\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957739\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957745\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957748\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957751\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957754\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957757\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957760\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957755\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957761\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957764\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957767\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957770\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957772\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957775\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957778\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957781\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957787\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957755\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957761\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957764\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957765\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957768\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957771\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957704\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957707\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957710\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957713\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957716\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957711\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957714\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957683\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957686\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957688\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957691\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957694\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957697\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957700\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957703\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957706\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957709\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957712\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957715\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957718\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957721\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957724\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957730\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957733\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957735\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957738\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957741\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957744\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957747\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957756\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957759\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957762\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957768\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957736\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957739\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957745\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957744\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957740\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957743\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957746\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957725\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957728\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957731\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957734\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957737\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957740\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957743\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957739\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957725\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957727\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957730\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957733\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957736\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957742\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957748\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957751\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957766\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957768\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957771\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957777\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957780\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957783\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957789\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957795\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957801\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957734\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957668\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957654\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957657\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957660\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957663\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957666\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957668\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957671\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957674\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957670\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957666\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957668\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957671\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957667\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957670\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957672\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957675\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957678\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957681\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957684\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957690\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957676\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957679\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957682\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957684\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957687\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957690\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957693\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957696\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957702\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957705\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957708\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957711\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957714\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957717\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957719\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957722\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957725\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957731\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957734\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957737\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957731\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957734\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957740\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957743\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957746\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957749\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957752\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957755\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957757\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957760\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957756\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957759\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957762\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957731\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957699\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957702\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957705\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957708\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957711\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957714\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957717\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957720\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957722\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957725\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957731\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957700\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957703\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957706\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957708\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957711\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957714\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957717\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957720\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957723\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957726\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957729\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957732\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957738\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957740\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957743\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957752\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957761\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957775\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957758\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957761\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957767\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957770\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957773\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957776\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957756\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957761\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957767\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957770\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957766\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957703\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957706\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957709\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957712\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957715\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957718\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957724\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957738\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957672\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957675\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957678\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957681\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957684\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957687\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957690\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957693\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957696\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957699\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957701\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957704\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957710\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957713\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957716\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957719\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957722\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957725\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957727\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957724\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957726\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957729\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957732\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957738\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957741\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957744\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957678\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957673\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957675\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957672\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957674\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957677\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957680\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957649\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957652\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957647\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957650\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957653\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957656\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957659\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957664\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957667\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957670\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957605\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957576\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957585\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957591\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957594\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957597\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957600\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957603\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957583\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957577\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957580\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957592\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.957588\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.957590\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.957596\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.957605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957611\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957614\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957617\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957619\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957622\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957625\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957621\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.957624\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957627\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957630\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957633\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957636\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957639\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957641\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957644\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957647\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957650\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957653\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957649\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957652\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957655\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957658\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957663\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957666\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957669\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957672\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957678\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957681\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957684\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957687\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957689\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957692\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957695\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957701\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957704\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957712\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957715\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957714\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957717\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957720\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957723\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957726\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957729\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957732\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957734\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957740\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957743\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957746\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957749\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957752\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957757\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957760\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957763\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957766\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957769\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957741\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957743\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957746\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957742\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957745\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957747\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957750\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957753\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957759\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957762\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957765\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957767\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957770\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957759\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957762\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957765\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957771\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957773\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957776\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957779\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957782\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957788\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957791\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957793\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957799\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957808\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957816\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957819\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957822\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957825\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957794\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957797\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957800\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957803\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957808\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957814\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957817\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957820\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957789\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957787\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957793\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957795\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957798\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957804\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957773\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957776\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957779\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957782\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957784\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957781\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957789\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957791\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957794\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957797\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957732\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957738\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957740\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957746\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957749\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957752\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957755\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957758\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957763\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957766\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957775\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957780\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957783\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957786\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957789\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957803\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957806\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957802\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957808\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957813\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957816\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957819\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957817\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957820\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957792\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957799\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957802\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957785\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957791\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957794\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957799\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957802\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957805\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957800\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957803\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957806\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957812\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957814\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957817\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957823\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957759\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957761\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957750\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957753\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957722\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957725\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957731\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957734\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957736\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957739\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957745\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957747\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957750\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957752\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957755\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957725\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957727\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957730\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957733\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957736\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957739\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957744\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957750\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957759\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957761\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957764\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957776\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957781\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957784\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957756\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957759\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957765\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957768\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957770\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957773\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957776\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957779\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957782\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957785\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957787\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957776\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957782\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957785\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957781\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957784\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957787\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957789\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957792\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957798\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957793\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957792\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957798\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957801\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957804\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957809\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957812\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957815\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957810\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957813\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957816\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957819\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957824\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957827\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957830\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957833\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957838\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957841\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957844\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957840\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957843\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957846\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957849\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957852\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957854\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957857\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957860\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957863\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957866\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957835\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957841\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957849\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957852\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957855\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957858\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957860\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957863\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957866\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957869\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957872\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957874\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957877\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957880\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957886\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957889\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957891\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957887\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957890\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957832\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957835\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957837\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957839\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957845\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957848\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957851\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957798\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957801\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957804\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957806\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957809\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957812\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957815\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957823\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957821\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957824\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957827\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957830\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957833\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957836\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957841\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957844\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957847\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957833\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957836\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957839\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957844\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957847\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957850\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957853\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957855\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957858\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957861\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957864\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957867\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957869\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957872\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957875\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957878\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957881\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957883\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957886\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957889\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957892\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957897\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957903\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957906\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957909\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957911\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957848\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957851\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957856\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957831\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957834\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957837\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957840\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957843\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957845\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957848\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957851\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957854\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957859\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957865\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957868\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957871\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957876\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957879\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957851\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957854\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957860\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957862\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957865\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957868\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957871\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957874\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957876\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957879\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957882\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957863\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957859\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957853\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957856\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957862\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957865\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957867\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957870\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957873\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957876\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957879\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957881\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957884\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957887\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957890\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957892\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957865\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957868\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957871\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957873\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957876\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957879\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957882\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957887\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957890\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957893\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957896\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957898\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957901\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957903\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957899\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957902\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957905\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957891\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957894\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957830\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957833\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957836\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957839\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957842\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957838\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957840\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957843\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957852\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957854\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957857\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957860\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957863\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957866\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957868\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957871\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957874\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957877\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957879\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957882\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957885\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957888\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957893\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957896\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957891\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957893\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957896\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957899\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957895\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957898\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957904\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957906\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957909\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957912\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957917\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957920\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957916\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957922\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957925\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957927\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957930\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957933\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957928\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957931\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957934\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957937\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957942\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957945\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957948\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957951\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957953\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957956\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957959\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957962\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957964\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957967\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957904\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957907\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957844\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957847\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957845\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957850\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957853\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957859\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957829\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957766\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957768\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957771\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957774\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957777\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957769\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957772\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957775\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957777\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957780\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957783\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957786\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957789\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957791\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957794\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957797\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957800\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957801\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957804\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957807\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957810\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957812\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957815\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957818\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957821\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957824\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957826\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957829\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957832\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957772\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957777\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957780\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957783\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957779\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957782\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957785\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957787\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957793\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957763\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957768\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957764\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957767\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957775\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957778\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957781\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957783\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957792\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957794\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957791\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957796\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957802\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957804\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957807\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957810\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957813\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957821\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957824\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957826\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957832\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957824\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957832\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957835\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957816\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957819\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957789\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957726\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957729\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957734\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957737\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957740\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957743\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957745\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957748\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957718\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957721\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957724\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957726\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957729\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957732\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957738\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957740\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957746\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957749\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957751\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957754\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957762\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957768\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957771\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957773\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957776\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957779\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957782\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957787\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957790\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957795\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957798\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957801\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957804\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957806\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957812\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957782\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957785\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957788\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957793\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957801\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957804\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957807\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957803\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114545, acc: 0.957806\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957743\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957746\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957749\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957752\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957754\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957763\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957765\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957768\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957771\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957774\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957770\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957773\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957775\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957778\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957781\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957784\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957786\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957789\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957795\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957797\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957800\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957803\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957806\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957808\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957811\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957814\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957817\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957818\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957823\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957804\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957810\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957806\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957809\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957811\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957814\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957820\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957815\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957818\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957821\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957823\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957826\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957832\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957837\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957840\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957845\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957851\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957856\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957859\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957867\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957884\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957886\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957889\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957892\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957897\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957900\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957903\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957905\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957908\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957914\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957916\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957919\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957914\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957916\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957919\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957922\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957925\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957921\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957924\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957920\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957923\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957925\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957928\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957933\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957936\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957939\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957942\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957944\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957950\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957955\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957958\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957961\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957966\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957969\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957972\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957955\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957958\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957961\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957964\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957966\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957969\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957972\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957977\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957980\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957983\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957985\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957988\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957984\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957986\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957989\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957992\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957994\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957997\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.958000\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.958003\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.958005\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.958011\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.958013\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.958016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958022\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958024\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.958027\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.958030\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.958032\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.958035\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.958038\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.958041\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.958043\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.958046\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.958049\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.958051\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.958054\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958057\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114123, acc: 0.958060\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.958062\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958065\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.958068\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958070\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.958073\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958076\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958078\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958081\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958084\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958087\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958089\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958092\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958095\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958097\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958089\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958092\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958095\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958097\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.958100\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958103\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958106\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.958108\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958111\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958114\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958110\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958113\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.958109\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.958112\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.958114\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114096, acc: 0.958117\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958120\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958122\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.958125\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958128\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.958124\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.958127\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958129\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958132\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958135\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.958138\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958140\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958143\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958146\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958148\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958151\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958154\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958149\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958152\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958155\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958157\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958160\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958163\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.958158\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958161\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.958164\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.958145\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958140\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.958143\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.958146\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.958148\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.958143\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.958146\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.958149\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.958151\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958147\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.958150\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.958153\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958156\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.958158\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.958161\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.958164\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958166\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.958169\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958172\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958174\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.958177\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958180\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.958182\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.958185\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.958188\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.958190\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.958193\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958196\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.958189\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958192\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.958195\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.958189\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.958192\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.958195\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958200\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958203\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.958173\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.958176\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.958147\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.958149\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.958152\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.958155\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.958157\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958160\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.958163\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.958165\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958168\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.958171\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.958174\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.958176\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958179\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.958182\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958184\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958187\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.958125\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.958128\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.958131\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.958134\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.958136\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.958139\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958142\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.958144\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.958147\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958150\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.958152\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958155\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.958158\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.958160\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.958163\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958150\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958152\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958155\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.958094\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.958032\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.958035\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.958038\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.958034\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.958037\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.958039\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.958042\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.958045\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.958047\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.958053\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.958056\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.958058\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.958055\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.958057\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.958060\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.958063\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.958065\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.958068\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.958071\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.958073\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.958079\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.958065\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.958068\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.958071\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.958073\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.958079\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.958081\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.958076\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.958079\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.958081\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.958084\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.958087\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.958089\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.958092\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.958095\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.958034\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.958036\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.958039\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957978\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957981\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957983\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957986\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957991\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957930\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957933\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957935\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957920\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957922\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957925\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957864\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957867\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957869\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957872\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957875\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957877\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957880\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957883\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957885\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957862\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957864\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957867\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957870\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957872\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957875\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957878\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957880\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957819\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957825\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957827\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957798\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957801\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957809\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957812\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957814\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957788\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957791\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957801\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957807\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957815\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957823\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957825\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957828\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957831\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957833\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957836\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957839\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957842\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957844\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957841\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957843\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957851\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957854\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957857\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957862\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957858\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957861\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957800\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957803\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957805\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957808\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957811\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957814\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957795\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957798\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957787\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957726\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957729\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957732\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957734\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957737\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957740\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957742\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957682\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957684\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957687\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957690\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957692\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957695\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957700\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957703\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957706\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957709\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957711\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957693\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957674\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957677\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957680\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957682\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957685\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957688\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957690\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957693\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957696\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957698\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957693\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957696\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957699\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957701\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957704\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957707\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957709\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957706\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957708\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957711\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957714\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957716\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957722\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957724\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957727\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957730\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957735\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957738\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957740\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957746\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957751\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957754\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957757\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957759\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957762\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957765\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957767\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957773\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957775\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957778\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114504, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957754\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957757\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957753\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957755\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957761\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957763\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957766\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957771\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957774\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957777\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957779\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957721\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957724\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957727\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957729\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957732\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957740\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957743\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957745\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957748\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957751\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957753\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957756\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957759\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957761\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957764\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957767\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957769\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957772\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957775\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957777\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957749\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957745\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957748\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957750\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957729\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957732\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957740\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957743\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957739\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957742\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957744\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957747\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957750\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957752\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957755\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957763\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957766\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957737\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957740\n",
      "target: tensor([9.]), logits: tensor([11.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957730\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957732\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957735\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957709\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957717\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957719\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957722\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957725\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957727\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957730\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957733\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957735\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957738\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957741\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957743\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957739\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957744\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957747\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957750\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957721\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957724\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957726\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957723\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957725\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957731\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957702\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957701\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957703\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957706\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957702\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957708\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957713\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957716\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957718\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957721\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957724\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957726\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957729\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957732\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957727\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957730\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.957726\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114815, acc: 0.957728\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957731\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957734\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957736\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957739\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957744\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957747\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957750\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957697\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957700\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957703\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957708\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957711\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957713\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957716\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957719\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957721\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957724\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957720\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957722\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957721\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957724\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957727\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.957708\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.957711\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957714\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.957716\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957722\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957724\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957730\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957732\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957735\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957738\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957740\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957743\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957745\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957748\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957751\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957753\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957727\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957730\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957733\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957735\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957741\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957743\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957746\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957749\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957751\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957754\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957756\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957759\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957762\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957733\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957736\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957738\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957741\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957744\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957746\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957749\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957689\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957692\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957694\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957690\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957686\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114846, acc: 0.957688\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957691\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957694\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.957696\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957699\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957702\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957704\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.957710\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957681\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114837, acc: 0.957684\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957677\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957674\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957676\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957679\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957682\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957684\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957687\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957690\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957692\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957695\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.957698\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957700\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957697\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957699\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.957696\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957698\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957693\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.957696\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957698\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957694\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957699\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957702\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957705\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957710\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957713\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957715\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115003, acc: 0.957718\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957720\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957723\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957695\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957700\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957702\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957705\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115008, acc: 0.957708\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957713\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.957716\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957718\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957721\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957661\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957664\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957667\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957669\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957641\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957643\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957646\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957649\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957654\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957657\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957659\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.957662\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957664\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957667\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957670\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957672\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114968, acc: 0.957675\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957674\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957677\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957617\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957620\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957622\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957625\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957628\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957630\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957633\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957636\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957638\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957641\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957643\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957592\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957594\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957600\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957602\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957605\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.957610\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957613\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.957615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957618\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957621\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957623\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957626\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957629\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957631\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957634\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957637\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.957639\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957635\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957638\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957640\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957643\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957645\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957648\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957651\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957622\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957625\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957628\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957630\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957633\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957635\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.957638\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957641\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957643\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.957646\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957649\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957651\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.957654\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.957656\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957659\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957662\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957664\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957667\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.957639\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957635\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.957638\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957643\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957645\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957648\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957651\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957653\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957656\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.957652\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957654\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957660\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957662\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957665\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957667\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957670\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957613\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114942, acc: 0.957616\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957619\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.957621\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957624\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957627\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957634\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.957606\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957609\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957611\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957614\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957617\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.957619\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957622\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957624\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957619\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957622\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957625\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957627\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957630\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957632\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957573\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957576\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.957578\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957581\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957584\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114928, acc: 0.957589\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957592\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957594\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.957599\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957602\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.957597\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957600\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957602\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957543\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957546\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.957548\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957551\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957559\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957561\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957567\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957572\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957575\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114889, acc: 0.957580\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114867, acc: 0.957588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.957590\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957593\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114846, acc: 0.957595\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957598\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957601\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.957603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957606\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957609\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957611\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957583\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957586\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957588\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114837, acc: 0.957591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957593\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957599\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957604\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.957606\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957609\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.957612\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957614\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957617\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957619\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957613\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957619\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957621\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957624\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957626\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957629\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957634\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957637\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957639\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957642\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957645\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957647\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957650\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957652\n",
      "target: tensor([1.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957563\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.957565\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957564\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957567\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957574\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957577\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957582\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957580\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957583\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114957, acc: 0.957585\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957588\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957591\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114935, acc: 0.957593\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957590\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957592\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957589\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957591\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957566\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957576\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957581\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957587\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957589\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957592\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957594\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957600\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.957602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957605\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957601\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957598\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957600\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957603\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957606\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957608\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957611\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957613\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957619\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957621\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957624\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957620\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957625\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957566\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957569\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957574\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957576\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957582\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957584\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957587\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957592\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957595\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115019, acc: 0.957597\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957600\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957602\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957544\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957546\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957549\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957552\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957554\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957557\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957552\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957534\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957537\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957539\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957542\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957544\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957550\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957552\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957546\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957549\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957551\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957554\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957556\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957564\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957567\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957569\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957580\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957582\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957590\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957593\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957592\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957594\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957597\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957600\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957602\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957605\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957607\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957600\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957602\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957605\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957607\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957610\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957613\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957615\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115020, acc: 0.957618\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957614\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957617\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957620\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957622\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957625\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957627\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115032, acc: 0.957630\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115025, acc: 0.957632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957638\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115004, acc: 0.957640\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114997, acc: 0.957643\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114990, acc: 0.957645\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957648\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.957651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957653\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957649\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115016, acc: 0.957652\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115009, acc: 0.957654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.957657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957659\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957662\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114981, acc: 0.957664\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114974, acc: 0.957667\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957670\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957672\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115014, acc: 0.957666\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957669\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957671\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114993, acc: 0.957674\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114986, acc: 0.957676\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114979, acc: 0.957679\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957682\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957684\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957687\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114951, acc: 0.957689\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957692\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957694\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957697\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957692\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957695\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957691\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957694\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957666\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957671\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957667\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957670\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957673\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957678\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957680\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957683\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957685\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957688\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957684\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957690\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115155, acc: 0.957631\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957634\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957636\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957639\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957642\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957644\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957647\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957649\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957652\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957655\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957627\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957604\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957607\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957609\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957612\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957614\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957617\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957619\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957622\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.957609\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957612\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957617\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957620\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.957618\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.957621\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957617\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.957591\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957594\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115350, acc: 0.957590\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957592\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115329, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957600\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115315, acc: 0.957603\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957605\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957608\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115354, acc: 0.957604\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115408, acc: 0.957576\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957569\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957542\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.957544\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957547\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115609, acc: 0.957542\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115602, acc: 0.957544\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115595, acc: 0.957547\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957550\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957552\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957549\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115627, acc: 0.957551\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115620, acc: 0.957554\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.957556\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957529\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115660, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957534\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957536\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115639, acc: 0.957539\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.957542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115625, acc: 0.957544\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957547\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957529\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115664, acc: 0.957532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957534\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115711, acc: 0.957506\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115704, acc: 0.957509\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957512\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.957514\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.957517\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115676, acc: 0.957519\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115669, acc: 0.957522\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957524\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957527\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115648, acc: 0.957530\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.957532\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115694, acc: 0.957504\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115687, acc: 0.957507\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115680, acc: 0.957510\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957492\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957495\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115720, acc: 0.957497\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115713, acc: 0.957500\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115706, acc: 0.957502\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957505\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957510\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957513\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957515\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115664, acc: 0.957518\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957520\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115650, acc: 0.957523\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.957525\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957528\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.957531\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115622, acc: 0.957533\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115615, acc: 0.957536\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957538\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.957543\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957546\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115580, acc: 0.957549\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115566, acc: 0.957554\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115559, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957559\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115606, acc: 0.957501\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957504\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115592, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115585, acc: 0.957509\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115631, acc: 0.957508\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957510\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957505\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115731, acc: 0.957500\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115784, acc: 0.957497\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115777, acc: 0.957499\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.957502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115763, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115756, acc: 0.957507\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115749, acc: 0.957510\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115742, acc: 0.957512\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115735, acc: 0.957515\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957517\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957520\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115714, acc: 0.957523\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115707, acc: 0.957525\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115700, acc: 0.957528\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115693, acc: 0.957530\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115687, acc: 0.957533\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115680, acc: 0.957535\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957538\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115666, acc: 0.957540\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115652, acc: 0.957546\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115705, acc: 0.957488\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115698, acc: 0.957490\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957493\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957495\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957498\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115670, acc: 0.957501\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115663, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115656, acc: 0.957506\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957508\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115702, acc: 0.957496\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.957498\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115689, acc: 0.957501\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115742, acc: 0.957495\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115735, acc: 0.957497\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957500\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115714, acc: 0.957505\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115707, acc: 0.957508\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115700, acc: 0.957510\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115753, acc: 0.957506\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957509\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115739, acc: 0.957511\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115732, acc: 0.957514\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115725, acc: 0.957516\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115718, acc: 0.957519\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115711, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115704, acc: 0.957524\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957527\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957529\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957532\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957534\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957530\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115723, acc: 0.957533\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115716, acc: 0.957535\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957538\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115702, acc: 0.957540\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115695, acc: 0.957543\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115688, acc: 0.957545\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115741, acc: 0.957542\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957545\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957547\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115720, acc: 0.957550\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115713, acc: 0.957552\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115706, acc: 0.957555\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957557\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115753, acc: 0.957500\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115739, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115792, acc: 0.957477\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115785, acc: 0.957480\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115778, acc: 0.957482\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115771, acc: 0.957485\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115764, acc: 0.957488\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115757, acc: 0.957490\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115750, acc: 0.957493\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115743, acc: 0.957495\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115736, acc: 0.957498\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115729, acc: 0.957500\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115722, acc: 0.957503\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957505\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115708, acc: 0.957508\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115701, acc: 0.957511\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115755, acc: 0.957507\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115748, acc: 0.957510\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115741, acc: 0.957512\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957515\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957517\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115720, acc: 0.957520\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115713, acc: 0.957522\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115706, acc: 0.957525\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957528\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957530\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957533\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957535\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957538\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115664, acc: 0.957540\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957543\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115650, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.957548\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957550\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115630, acc: 0.957553\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.957556\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115616, acc: 0.957558\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115609, acc: 0.957561\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115602, acc: 0.957563\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115595, acc: 0.957566\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957568\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957571\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115574, acc: 0.957573\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115560, acc: 0.957579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115553, acc: 0.957581\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957584\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957586\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115532, acc: 0.957589\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957591\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115512, acc: 0.957596\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115558, acc: 0.957595\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.957591\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.957594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957596\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115576, acc: 0.957604\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957607\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.957609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957612\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957584\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957587\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957592\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957594\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115574, acc: 0.957597\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115627, acc: 0.957540\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115620, acc: 0.957542\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.957545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115606, acc: 0.957547\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957550\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115652, acc: 0.957532\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115645, acc: 0.957535\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957537\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115631, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957542\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115617, acc: 0.957545\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115610, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115603, acc: 0.957550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957555\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957558\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115576, acc: 0.957560\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957563\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.957565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957568\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115548, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.957575\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957578\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957581\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115514, acc: 0.957583\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115560, acc: 0.957558\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115553, acc: 0.957561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957566\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115532, acc: 0.957568\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.957579\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957581\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115490, acc: 0.957584\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115484, acc: 0.957586\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115477, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115470, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115463, acc: 0.957594\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115456, acc: 0.957596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115449, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957604\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115488, acc: 0.957600\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115481, acc: 0.957603\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.957576\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957578\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957581\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957583\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957586\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957566\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115532, acc: 0.957569\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957571\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957574\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957581\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115490, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115483, acc: 0.957586\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115476, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115470, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115522, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957587\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957592\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115554, acc: 0.957534\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115548, acc: 0.957537\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957539\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.957542\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957545\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957547\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957550\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957552\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.957555\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115486, acc: 0.957560\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115538, acc: 0.957552\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957555\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957557\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957560\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957562\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.957565\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957559\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115550, acc: 0.957562\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957564\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115536, acc: 0.957567\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957569\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115522, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957574\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115508, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115547, acc: 0.957555\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115540, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115534, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957562\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957565\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957567\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957570\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957575\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115479, acc: 0.957580\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115472, acc: 0.957582\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957585\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115458, acc: 0.957587\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957582\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.957585\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957587\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115490, acc: 0.957590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115483, acc: 0.957593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115476, acc: 0.957595\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957598\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957600\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115456, acc: 0.957603\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115449, acc: 0.957605\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957608\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957610\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115428, acc: 0.957613\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115421, acc: 0.957615\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115414, acc: 0.957618\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115467, acc: 0.957561\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115460, acc: 0.957563\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957566\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115446, acc: 0.957568\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115440, acc: 0.957571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115433, acc: 0.957573\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.957576\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957549\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115472, acc: 0.957551\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957554\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115458, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115451, acc: 0.957559\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115437, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115430, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115423, acc: 0.957569\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115417, acc: 0.957571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115410, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115403, acc: 0.957576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957582\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957584\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957527\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115428, acc: 0.957530\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115421, acc: 0.957532\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115414, acc: 0.957535\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115407, acc: 0.957537\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115401, acc: 0.957540\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115394, acc: 0.957542\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957545\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957547\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.957550\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115366, acc: 0.957552\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115359, acc: 0.957555\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115353, acc: 0.957557\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115346, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115398, acc: 0.957503\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115451, acc: 0.957491\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.957493\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115437, acc: 0.957496\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115430, acc: 0.957498\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115423, acc: 0.957501\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115417, acc: 0.957503\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115410, acc: 0.957506\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115403, acc: 0.957508\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957511\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957513\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115375, acc: 0.957519\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957521\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115362, acc: 0.957524\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957526\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115348, acc: 0.957529\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.957531\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957534\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957536\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.957539\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957541\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957544\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957546\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.957549\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957551\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957554\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957556\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957559\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957566\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957569\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957571\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957574\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115211, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957581\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957584\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.957559\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.957564\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957567\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.957572\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.957574\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957577\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.957584\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957587\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957589\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115155, acc: 0.957592\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957595\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957600\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957596\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957599\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957542\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957544\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957547\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957550\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957552\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957555\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.957557\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957560\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957562\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.957558\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957561\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957563\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.957566\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957576\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957578\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957575\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957580\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957555\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115305, acc: 0.957549\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115357, acc: 0.957546\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115351, acc: 0.957548\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115344, acc: 0.957551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957553\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957556\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115323, acc: 0.957558\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957561\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957568\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957571\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957573\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115335, acc: 0.957568\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.957574\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957576\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957579\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.957584\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957586\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957589\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957591\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957594\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957596\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957599\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957601\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957604\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957606\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957609\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957611\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957608\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.957610\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115258, acc: 0.957613\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957618\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957620\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115230, acc: 0.957623\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.957625\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115276, acc: 0.957613\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115269, acc: 0.957615\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957618\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957614\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957617\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957619\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.957622\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957624\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957627\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957632\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957634\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957637\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957639\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957642\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957644\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957647\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957649\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957652\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957654\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.957657\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957659\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957662\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957664\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957667\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957669\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957672\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957674\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957677\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957679\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957682\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957684\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957680\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957683\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957685\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957688\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115208, acc: 0.957684\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957686\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957689\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957691\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957688\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115226, acc: 0.957690\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.957693\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957695\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115258, acc: 0.957641\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957644\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957646\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.957649\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115231, acc: 0.957651\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.957654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115217, acc: 0.957656\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115211, acc: 0.957659\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115263, acc: 0.957655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957658\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.957660\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115242, acc: 0.957663\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957665\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957668\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957670\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.957673\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115208, acc: 0.957675\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115202, acc: 0.957678\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957674\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115247, acc: 0.957677\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957679\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957682\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.957626\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957628\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115272, acc: 0.957631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.957633\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115258, acc: 0.957636\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115251, acc: 0.957638\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.957637\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957639\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.957641\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115276, acc: 0.957644\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115263, acc: 0.957649\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957651\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.957654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.957656\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115236, acc: 0.957659\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957661\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115222, acc: 0.957664\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957660\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957663\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957665\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115254, acc: 0.957668\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115247, acc: 0.957670\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957673\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.957678\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115220, acc: 0.957680\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957683\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957685\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.957688\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115193, acc: 0.957690\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957693\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957695\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957698\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957700\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957703\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957705\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957708\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957710\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957713\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957715\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957717\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957720\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957722\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957725\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957727\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957730\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957718\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957720\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957723\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957725\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957728\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957730\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957735\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957740\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957742\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957716\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957718\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957721\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957723\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957726\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957723\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957726\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957670\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957672\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957675\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957677\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957680\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957682\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957687\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957689\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957692\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957694\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957697\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957702\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957703\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957706\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957708\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957711\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957713\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957716\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957718\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957721\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957723\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957726\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957731\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957723\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957726\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957728\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957672\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957646\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957648\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957645\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115241, acc: 0.957647\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957650\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957652\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957655\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957654\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957656\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.957659\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957661\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957664\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957666\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957669\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957671\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957673\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957676\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.957678\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957681\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.957654\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115288, acc: 0.957653\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957656\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115275, acc: 0.957658\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957661\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957663\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.957666\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957668\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115241, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115234, acc: 0.957673\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957675\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957674\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957676\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115317, acc: 0.957672\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957666\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115362, acc: 0.957668\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957671\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957673\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115342, acc: 0.957676\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115394, acc: 0.957672\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957675\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115438, acc: 0.957663\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115490, acc: 0.957607\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957604\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957606\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115528, acc: 0.957608\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115521, acc: 0.957611\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115514, acc: 0.957613\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115508, acc: 0.957616\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957618\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115494, acc: 0.957621\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957617\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957620\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115584, acc: 0.957567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957569\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957572\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115564, acc: 0.957574\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957577\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115550, acc: 0.957579\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957581\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.957584\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957580\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957524\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115633, acc: 0.957527\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115626, acc: 0.957529\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.957532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.957534\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115606, acc: 0.957537\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957539\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115592, acc: 0.957542\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115586, acc: 0.957544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957546\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115572, acc: 0.957549\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115566, acc: 0.957551\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115559, acc: 0.957554\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115610, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957501\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957503\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115648, acc: 0.957506\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957508\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115635, acc: 0.957511\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115628, acc: 0.957513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115621, acc: 0.957516\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115615, acc: 0.957518\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957520\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957496\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957499\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115639, acc: 0.957501\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.957504\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115626, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.957509\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957511\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115606, acc: 0.957514\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957516\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115592, acc: 0.957519\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115585, acc: 0.957521\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957523\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115572, acc: 0.957526\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115565, acc: 0.957528\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115559, acc: 0.957531\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957533\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957536\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957538\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957533\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957536\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115576, acc: 0.957538\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957541\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115621, acc: 0.957485\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115614, acc: 0.957488\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957490\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957493\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.957495\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957498\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957500\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115574, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957505\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.957501\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957504\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957506\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957508\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115592, acc: 0.957511\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115585, acc: 0.957513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115578, acc: 0.957516\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115572, acc: 0.957518\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115565, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115558, acc: 0.957523\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957526\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957528\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115538, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115532, acc: 0.957533\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957536\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957538\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115512, acc: 0.957540\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957543\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957545\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957555\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957558\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115458, acc: 0.957560\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115451, acc: 0.957563\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115445, acc: 0.957565\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115438, acc: 0.957567\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115431, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115425, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957575\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115411, acc: 0.957577\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957522\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115456, acc: 0.957524\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115449, acc: 0.957527\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957529\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115436, acc: 0.957532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115429, acc: 0.957534\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115422, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115416, acc: 0.957539\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957542\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957544\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957546\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115447, acc: 0.957530\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957526\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957528\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957531\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957533\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957536\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957538\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115516, acc: 0.957483\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957485\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957488\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115496, acc: 0.957490\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957492\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115482, acc: 0.957495\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957491\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957493\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957496\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957498\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957501\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957503\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957502\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115538, acc: 0.957504\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957507\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957509\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957512\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957514\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957516\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115556, acc: 0.957500\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115549, acc: 0.957502\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115542, acc: 0.957505\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.957501\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957504\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115580, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957509\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957508\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.957510\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.957513\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115598, acc: 0.957515\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115591, acc: 0.957518\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115584, acc: 0.957520\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115578, acc: 0.957522\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115571, acc: 0.957525\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115564, acc: 0.957527\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115615, acc: 0.957472\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115609, acc: 0.957475\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115660, acc: 0.957471\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957474\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957476\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957450\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957452\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957455\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957457\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957460\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115664, acc: 0.957462\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957464\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.957467\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115644, acc: 0.957469\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115637, acc: 0.957472\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115631, acc: 0.957474\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957477\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115617, acc: 0.957479\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.957482\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.957484\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957487\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115591, acc: 0.957489\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115584, acc: 0.957491\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957494\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115571, acc: 0.957496\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115622, acc: 0.957493\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115615, acc: 0.957495\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957498\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115602, acc: 0.957500\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115595, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957505\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115582, acc: 0.957508\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.957510\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115568, acc: 0.957513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.957515\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957517\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115548, acc: 0.957520\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115542, acc: 0.957522\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957525\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115528, acc: 0.957527\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115522, acc: 0.957530\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957532\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957535\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957537\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957539\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957528\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115526, acc: 0.957530\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115520, acc: 0.957533\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957535\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957537\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957540\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957542\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115486, acc: 0.957545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957547\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.957550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115466, acc: 0.957552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115460, acc: 0.957554\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957557\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115447, acc: 0.957559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115440, acc: 0.957562\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115433, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115484, acc: 0.957538\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115477, acc: 0.957540\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957543\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115464, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115458, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115451, acc: 0.957550\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115444, acc: 0.957553\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115438, acc: 0.957555\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115431, acc: 0.957557\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115424, acc: 0.957560\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957562\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957559\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115462, acc: 0.957561\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115455, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115449, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957569\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957571\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115429, acc: 0.957574\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115422, acc: 0.957576\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115416, acc: 0.957579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957583\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957586\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115446, acc: 0.957531\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115440, acc: 0.957533\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957517\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115484, acc: 0.957519\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115477, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957524\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115464, acc: 0.957526\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115457, acc: 0.957529\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115508, acc: 0.957512\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957515\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957517\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115488, acc: 0.957519\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115482, acc: 0.957522\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115532, acc: 0.957517\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115526, acc: 0.957520\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957522\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115512, acc: 0.957524\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957527\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.957529\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115486, acc: 0.957534\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115479, acc: 0.957537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.957539\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115466, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115460, acc: 0.957544\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957546\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115446, acc: 0.957549\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115440, acc: 0.957551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115433, acc: 0.957554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115420, acc: 0.957559\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115413, acc: 0.957561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115407, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115400, acc: 0.957566\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115393, acc: 0.957568\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957573\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.957576\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957578\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115360, acc: 0.957580\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115354, acc: 0.957583\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.957585\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.957588\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957590\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115327, acc: 0.957593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.957595\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957597\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.957593\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115358, acc: 0.957595\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115352, acc: 0.957597\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115345, acc: 0.957600\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957545\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957548\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957550\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957552\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957555\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957557\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115356, acc: 0.957560\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957562\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957565\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957567\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957569\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957566\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.957569\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115424, acc: 0.957514\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957516\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115411, acc: 0.957519\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.957521\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115398, acc: 0.957524\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957526\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957528\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115378, acc: 0.957531\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115429, acc: 0.957525\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115422, acc: 0.957527\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115415, acc: 0.957530\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957532\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957535\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957537\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957540\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957542\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957544\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957547\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957549\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115356, acc: 0.957552\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957554\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957559\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957555\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957558\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957562\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957558\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115411, acc: 0.957560\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.957562\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115398, acc: 0.957565\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957567\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957570\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115378, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115371, acc: 0.957575\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115415, acc: 0.957573\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957578\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957580\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957583\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957585\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957588\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957590\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957592\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115356, acc: 0.957595\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115350, acc: 0.957597\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957600\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957602\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957605\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115323, acc: 0.957607\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115317, acc: 0.957609\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957555\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957557\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115354, acc: 0.957560\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.957564\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115378, acc: 0.957565\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115371, acc: 0.957568\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.957570\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115415, acc: 0.957566\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115409, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957571\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957576\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115382, acc: 0.957578\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115376, acc: 0.957581\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957583\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957586\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115356, acc: 0.957588\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115350, acc: 0.957590\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957593\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957595\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957541\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957543\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.957546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957548\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115417, acc: 0.957522\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115468, acc: 0.957468\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.957470\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115455, acc: 0.957472\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115448, acc: 0.957475\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957477\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957471\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115542, acc: 0.957468\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115536, acc: 0.957471\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115586, acc: 0.957445\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957447\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957449\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115566, acc: 0.957452\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115560, acc: 0.957454\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115553, acc: 0.957457\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957459\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115540, acc: 0.957462\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957458\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115584, acc: 0.957461\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957463\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957466\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115564, acc: 0.957468\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957470\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.957473\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957469\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.957443\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115645, acc: 0.957445\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957448\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.957450\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115625, acc: 0.957453\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957455\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957457\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957460\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957462\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957408\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957410\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.957415\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.957418\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115616, acc: 0.957420\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115610, acc: 0.957422\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115603, acc: 0.957425\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115596, acc: 0.957427\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957430\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957432\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957434\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957437\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115564, acc: 0.957439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957442\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.957444\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115544, acc: 0.957447\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.957449\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957451\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115524, acc: 0.957454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115518, acc: 0.957456\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957459\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957461\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957463\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957466\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957468\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957471\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115472, acc: 0.957473\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957475\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115459, acc: 0.957478\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115452, acc: 0.957480\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115446, acc: 0.957483\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115439, acc: 0.957485\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115433, acc: 0.957488\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.957490\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115420, acc: 0.957492\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115413, acc: 0.957495\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115406, acc: 0.957497\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115400, acc: 0.957500\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115393, acc: 0.957502\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957504\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115380, acc: 0.957507\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115374, acc: 0.957509\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115424, acc: 0.957506\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115417, acc: 0.957508\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115411, acc: 0.957511\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.957513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115398, acc: 0.957516\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115391, acc: 0.957518\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957520\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115378, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115428, acc: 0.957497\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115422, acc: 0.957499\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115415, acc: 0.957502\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957498\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115459, acc: 0.957501\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115452, acc: 0.957503\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115446, acc: 0.957506\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115439, acc: 0.957508\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957454\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115483, acc: 0.957456\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115476, acc: 0.957459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115470, acc: 0.957461\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115463, acc: 0.957463\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957438\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957440\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957442\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957445\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115487, acc: 0.957447\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957450\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115474, acc: 0.957452\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115467, acc: 0.957454\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.957457\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115454, acc: 0.957459\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115448, acc: 0.957462\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115441, acc: 0.957464\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957466\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115428, acc: 0.957469\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957465\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115472, acc: 0.957467\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957470\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115459, acc: 0.957472\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115452, acc: 0.957475\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115445, acc: 0.957477\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115439, acc: 0.957479\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115432, acc: 0.957482\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115482, acc: 0.957428\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115476, acc: 0.957430\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957433\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957427\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957429\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957434\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957436\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115487, acc: 0.957439\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957441\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115474, acc: 0.957444\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115467, acc: 0.957446\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.957449\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115454, acc: 0.957451\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115504, acc: 0.957447\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115498, acc: 0.957449\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957452\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957454\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957457\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957459\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957461\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115458, acc: 0.957464\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115452, acc: 0.957466\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957443\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957445\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115482, acc: 0.957448\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115476, acc: 0.957450\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957452\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957398\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957401\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115506, acc: 0.957403\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115500, acc: 0.957406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957408\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115487, acc: 0.957410\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115480, acc: 0.957413\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115530, acc: 0.957409\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115580, acc: 0.957397\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957400\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957402\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115617, acc: 0.957348\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115610, acc: 0.957350\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.957353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957355\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.957330\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.957332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957334\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957330\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957333\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115664, acc: 0.957338\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115658, acc: 0.957340\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.957342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115645, acc: 0.957345\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.957350\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115625, acc: 0.957352\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.957355\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957357\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957353\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957358\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957354\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115686, acc: 0.957359\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957364\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115666, acc: 0.957366\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115660, acc: 0.957368\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957371\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.957373\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957376\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957378\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115627, acc: 0.957380\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115621, acc: 0.957383\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115614, acc: 0.957385\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957388\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957384\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.957386\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115644, acc: 0.957389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957391\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115631, acc: 0.957393\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115625, acc: 0.957396\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957398\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957401\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957403\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957405\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115592, acc: 0.957408\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957391\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957394\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.957396\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.957399\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115616, acc: 0.957401\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115666, acc: 0.957398\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957400\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957403\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957405\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957407\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115689, acc: 0.957353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.957356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115676, acc: 0.957358\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115670, acc: 0.957361\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115720, acc: 0.957307\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115713, acc: 0.957309\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115707, acc: 0.957312\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115700, acc: 0.957314\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115694, acc: 0.957316\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115743, acc: 0.957313\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115737, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115724, acc: 0.957320\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115717, acc: 0.957322\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115711, acc: 0.957325\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115704, acc: 0.957327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115698, acc: 0.957329\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957334\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115672, acc: 0.957339\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.957341\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957344\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115652, acc: 0.957346\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957349\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115639, acc: 0.957351\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115633, acc: 0.957353\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115626, acc: 0.957356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115620, acc: 0.957358\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.957361\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115607, acc: 0.957363\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115600, acc: 0.957365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.957368\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115644, acc: 0.957314\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115637, acc: 0.957316\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115687, acc: 0.957263\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115680, acc: 0.957265\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957239\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115779, acc: 0.957186\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115773, acc: 0.957188\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115766, acc: 0.957191\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115760, acc: 0.957193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115753, acc: 0.957195\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115747, acc: 0.957198\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.957200\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957203\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957205\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957207\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957210\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115764, acc: 0.957156\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115814, acc: 0.957140\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115863, acc: 0.957137\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115857, acc: 0.957139\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115850, acc: 0.957141\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115844, acc: 0.957144\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115837, acc: 0.957146\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115831, acc: 0.957149\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115824, acc: 0.957151\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115818, acc: 0.957153\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115811, acc: 0.957156\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115805, acc: 0.957158\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115798, acc: 0.957161\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115792, acc: 0.957163\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115785, acc: 0.957165\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115779, acc: 0.957168\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115772, acc: 0.957170\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115766, acc: 0.957173\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115759, acc: 0.957175\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115753, acc: 0.957177\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957180\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.957182\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115733, acc: 0.957185\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957187\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957189\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115714, acc: 0.957192\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115708, acc: 0.957194\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115757, acc: 0.957169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115751, acc: 0.957171\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115744, acc: 0.957173\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115738, acc: 0.957176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115731, acc: 0.957178\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115725, acc: 0.957181\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115718, acc: 0.957183\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115712, acc: 0.957185\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115705, acc: 0.957188\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957190\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115686, acc: 0.957195\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115735, acc: 0.957191\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115729, acc: 0.957194\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115722, acc: 0.957196\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115772, acc: 0.957193\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115765, acc: 0.957195\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115759, acc: 0.957197\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115752, acc: 0.957200\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957202\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115739, acc: 0.957205\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115733, acc: 0.957207\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115782, acc: 0.957182\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115776, acc: 0.957184\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115769, acc: 0.957186\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115763, acc: 0.957189\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115812, acc: 0.957185\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115806, acc: 0.957188\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115799, acc: 0.957190\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115793, acc: 0.957193\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115787, acc: 0.957195\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115780, acc: 0.957197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115774, acc: 0.957200\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115767, acc: 0.957202\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115761, acc: 0.957205\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115754, acc: 0.957207\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115748, acc: 0.957209\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115741, acc: 0.957212\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115791, acc: 0.957209\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115784, acc: 0.957211\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115778, acc: 0.957213\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115827, acc: 0.957160\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115821, acc: 0.957162\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115814, acc: 0.957165\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115864, acc: 0.957111\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115857, acc: 0.957114\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115851, acc: 0.957116\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115900, acc: 0.957109\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115893, acc: 0.957112\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115887, acc: 0.957114\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115881, acc: 0.957116\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115874, acc: 0.957119\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115868, acc: 0.957121\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115861, acc: 0.957124\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115855, acc: 0.957126\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115848, acc: 0.957128\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115842, acc: 0.957131\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115835, acc: 0.957133\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115829, acc: 0.957135\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115822, acc: 0.957138\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115816, acc: 0.957140\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115865, acc: 0.957136\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115859, acc: 0.957139\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115852, acc: 0.957141\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115846, acc: 0.957144\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115839, acc: 0.957146\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115889, acc: 0.957121\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115882, acc: 0.957123\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115876, acc: 0.957125\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115869, acc: 0.957128\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115919, acc: 0.957074\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115912, acc: 0.957077\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.957079\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115899, acc: 0.957081\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115893, acc: 0.957084\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115886, acc: 0.957086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115880, acc: 0.957089\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115929, acc: 0.957035\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115923, acc: 0.957038\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115916, acc: 0.957040\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115910, acc: 0.957043\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115903, acc: 0.957045\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115897, acc: 0.957047\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115890, acc: 0.957050\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115884, acc: 0.957052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115877, acc: 0.957054\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115871, acc: 0.957057\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115865, acc: 0.957059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115858, acc: 0.957062\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115852, acc: 0.957064\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115845, acc: 0.957066\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115839, acc: 0.957069\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115832, acc: 0.957071\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115882, acc: 0.957055\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115931, acc: 0.957052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115924, acc: 0.957054\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115918, acc: 0.957057\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.957003\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.957006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115954, acc: 0.957008\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.957011\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115941, acc: 0.957013\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115990, acc: 0.956988\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.956990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115978, acc: 0.956992\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115971, acc: 0.956995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115965, acc: 0.956997\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.956999\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.957002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.957004\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.957007\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115932, acc: 0.957009\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115926, acc: 0.957011\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115919, acc: 0.957014\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115913, acc: 0.957016\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115962, acc: 0.956991\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.956993\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115949, acc: 0.956996\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115943, acc: 0.956998\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115936, acc: 0.957000\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115930, acc: 0.957003\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115923, acc: 0.957005\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115917, acc: 0.957008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115911, acc: 0.957010\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115904, acc: 0.957012\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115953, acc: 0.957009\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115947, acc: 0.957011\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.957013\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115934, acc: 0.957016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115928, acc: 0.957018\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115921, acc: 0.957020\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115915, acc: 0.957023\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115908, acc: 0.957025\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115902, acc: 0.957028\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115895, acc: 0.957030\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115889, acc: 0.957032\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115882, acc: 0.957035\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115876, acc: 0.957037\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115925, acc: 0.957034\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115919, acc: 0.957036\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115912, acc: 0.957039\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.957041\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115955, acc: 0.957025\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116004, acc: 0.957020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.957023\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115991, acc: 0.957025\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115985, acc: 0.957028\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116034, acc: 0.956975\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116027, acc: 0.956977\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116021, acc: 0.956979\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116014, acc: 0.956982\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116008, acc: 0.956984\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116002, acc: 0.956986\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116051, acc: 0.956983\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116044, acc: 0.956985\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116093, acc: 0.956932\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116087, acc: 0.956934\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116080, acc: 0.956937\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116074, acc: 0.956939\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116067, acc: 0.956942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116061, acc: 0.956944\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116055, acc: 0.956946\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116104, acc: 0.956921\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116097, acc: 0.956923\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116091, acc: 0.956926\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116084, acc: 0.956928\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116078, acc: 0.956931\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116071, acc: 0.956933\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116065, acc: 0.956935\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116114, acc: 0.956910\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116108, acc: 0.956912\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116101, acc: 0.956915\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116095, acc: 0.956917\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116088, acc: 0.956919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116082, acc: 0.956922\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116075, acc: 0.956924\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116069, acc: 0.956927\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116063, acc: 0.956929\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116056, acc: 0.956931\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956934\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116099, acc: 0.956908\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116092, acc: 0.956911\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116086, acc: 0.956913\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116079, acc: 0.956916\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116073, acc: 0.956918\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116066, acc: 0.956920\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116060, acc: 0.956923\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116054, acc: 0.956925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116047, acc: 0.956928\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116041, acc: 0.956930\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116034, acc: 0.956932\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116028, acc: 0.956935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116021, acc: 0.956937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116015, acc: 0.956940\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116009, acc: 0.956942\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116002, acc: 0.956944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956947\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116045, acc: 0.956921\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116038, acc: 0.956924\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116032, acc: 0.956926\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116025, acc: 0.956929\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116074, acc: 0.956925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116068, acc: 0.956927\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116117, acc: 0.956923\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116110, acc: 0.956926\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116104, acc: 0.956928\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116098, acc: 0.956931\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116091, acc: 0.956933\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116085, acc: 0.956935\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116134, acc: 0.956932\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116127, acc: 0.956935\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116121, acc: 0.956937\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116114, acc: 0.956939\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116108, acc: 0.956942\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116102, acc: 0.956944\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116095, acc: 0.956946\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116089, acc: 0.956949\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116082, acc: 0.956951\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116076, acc: 0.956954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116069, acc: 0.956956\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116063, acc: 0.956958\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116057, acc: 0.956961\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116044, acc: 0.956965\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116093, acc: 0.956913\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116086, acc: 0.956915\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116080, acc: 0.956917\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116073, acc: 0.956920\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116067, acc: 0.956922\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116061, acc: 0.956924\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116054, acc: 0.956927\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116048, acc: 0.956929\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116041, acc: 0.956932\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116035, acc: 0.956934\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116029, acc: 0.956936\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116022, acc: 0.956939\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116016, acc: 0.956941\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116009, acc: 0.956944\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116003, acc: 0.956946\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956948\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115990, acc: 0.956951\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.956953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115977, acc: 0.956955\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115971, acc: 0.956958\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.956960\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.956963\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.956965\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.956967\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.956970\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115988, acc: 0.956963\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.956965\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.956968\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.956970\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115962, acc: 0.956972\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.956975\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115949, acc: 0.956977\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115943, acc: 0.956979\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115936, acc: 0.956982\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115930, acc: 0.956984\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115924, acc: 0.956987\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115917, acc: 0.956989\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115911, acc: 0.956991\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115904, acc: 0.956994\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115898, acc: 0.956996\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115892, acc: 0.956998\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115885, acc: 0.957001\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115879, acc: 0.957003\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115872, acc: 0.957006\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115921, acc: 0.957002\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115915, acc: 0.957005\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115908, acc: 0.957007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115902, acc: 0.957010\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115896, acc: 0.957012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115889, acc: 0.957014\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115883, acc: 0.957017\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115877, acc: 0.957019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115870, acc: 0.957021\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115864, acc: 0.957024\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115857, acc: 0.957026\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115851, acc: 0.957028\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115845, acc: 0.957031\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115838, acc: 0.957033\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115832, acc: 0.957036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115825, acc: 0.957038\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115819, acc: 0.957040\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115813, acc: 0.957043\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115861, acc: 0.957039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115855, acc: 0.957041\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115904, acc: 0.957030\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115897, acc: 0.957032\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115891, acc: 0.957035\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.957031\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115988, acc: 0.957027\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115982, acc: 0.957029\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115976, acc: 0.957032\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115969, acc: 0.957034\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115963, acc: 0.957036\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.957039\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116005, acc: 0.956986\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115999, acc: 0.956988\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115992, acc: 0.956991\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115986, acc: 0.956993\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115980, acc: 0.956996\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115973, acc: 0.956998\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.957000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115960, acc: 0.957003\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115954, acc: 0.957005\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.957007\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115990, acc: 0.956985\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115983, acc: 0.956987\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115977, acc: 0.956989\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115971, acc: 0.956992\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.956994\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.956996\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.956999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.957001\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.957004\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115932, acc: 0.957006\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.957002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.957005\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116023, acc: 0.956952\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116017, acc: 0.956954\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116011, acc: 0.956957\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116004, acc: 0.956959\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.956961\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115991, acc: 0.956964\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115985, acc: 0.956966\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115979, acc: 0.956968\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115972, acc: 0.956971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115966, acc: 0.956973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115960, acc: 0.956976\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116008, acc: 0.956972\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116002, acc: 0.956975\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115995, acc: 0.956977\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115989, acc: 0.956980\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115983, acc: 0.956982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115976, acc: 0.956984\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115970, acc: 0.956987\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116018, acc: 0.956984\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.956986\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116006, acc: 0.956988\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115999, acc: 0.956991\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115993, acc: 0.956993\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115987, acc: 0.956995\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115980, acc: 0.956998\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115974, acc: 0.957000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.957002\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.957005\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115955, acc: 0.957007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.957009\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115997, acc: 0.956957\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115991, acc: 0.956959\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.956962\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115978, acc: 0.956964\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115971, acc: 0.956966\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115965, acc: 0.956969\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115959, acc: 0.956971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.956973\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115946, acc: 0.956976\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115995, acc: 0.956973\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115988, acc: 0.956975\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115982, acc: 0.956977\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.956980\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115969, acc: 0.956982\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115963, acc: 0.956985\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.956987\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116005, acc: 0.956934\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.956937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115992, acc: 0.956939\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115986, acc: 0.956941\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115979, acc: 0.956944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115973, acc: 0.956946\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.956949\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115960, acc: 0.956951\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115954, acc: 0.956953\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.956956\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115941, acc: 0.956958\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115935, acc: 0.956960\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115929, acc: 0.956963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115922, acc: 0.956965\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115916, acc: 0.956967\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.956952\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.956954\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115952, acc: 0.956956\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.956959\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.956961\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115933, acc: 0.956963\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.956960\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.956963\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.956965\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116017, acc: 0.956940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116010, acc: 0.956942\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116004, acc: 0.956945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.956947\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116046, acc: 0.956943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116040, acc: 0.956946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116033, acc: 0.956948\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116027, acc: 0.956950\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116021, acc: 0.956953\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116014, acc: 0.956955\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116008, acc: 0.956957\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116056, acc: 0.956946\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956948\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116044, acc: 0.956951\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116037, acc: 0.956953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116031, acc: 0.956955\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116025, acc: 0.956958\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116018, acc: 0.956960\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.956963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116005, acc: 0.956965\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115999, acc: 0.956967\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115993, acc: 0.956970\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115986, acc: 0.956972\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115980, acc: 0.956974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115974, acc: 0.956977\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.956979\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.956981\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115955, acc: 0.956984\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.956986\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115942, acc: 0.956988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115936, acc: 0.956991\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115929, acc: 0.956993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115923, acc: 0.956995\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115917, acc: 0.956998\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115910, acc: 0.957000\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115904, acc: 0.957003\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115898, acc: 0.957005\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115891, acc: 0.957007\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.957003\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115988, acc: 0.956950\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115982, acc: 0.956953\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.956955\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115969, acc: 0.956957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115963, acc: 0.956960\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.956962\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115950, acc: 0.956965\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.956961\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115992, acc: 0.956964\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115986, acc: 0.956966\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115979, acc: 0.956968\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116028, acc: 0.956944\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116076, acc: 0.956928\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116069, acc: 0.956930\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116063, acc: 0.956932\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116057, acc: 0.956935\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116044, acc: 0.956939\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116038, acc: 0.956942\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116031, acc: 0.956944\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116025, acc: 0.956946\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116019, acc: 0.956949\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.956951\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116006, acc: 0.956954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116000, acc: 0.956956\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115993, acc: 0.956958\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115987, acc: 0.956961\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.956963\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115974, acc: 0.956965\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.956968\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115962, acc: 0.956970\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116010, acc: 0.956918\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116004, acc: 0.956920\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115997, acc: 0.956922\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116046, acc: 0.956898\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116039, acc: 0.956900\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116033, acc: 0.956902\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116027, acc: 0.956905\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116020, acc: 0.956907\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116069, acc: 0.956904\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116062, acc: 0.956906\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116056, acc: 0.956909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956911\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116098, acc: 0.956907\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116091, acc: 0.956910\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116085, acc: 0.956912\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116079, acc: 0.956914\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116072, acc: 0.956917\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116066, acc: 0.956919\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116060, acc: 0.956921\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116053, acc: 0.956924\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116047, acc: 0.956926\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116041, acc: 0.956928\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116034, acc: 0.956931\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116028, acc: 0.956933\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116022, acc: 0.956935\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116015, acc: 0.956938\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116009, acc: 0.956940\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116003, acc: 0.956942\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115997, acc: 0.956945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115990, acc: 0.956947\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.956949\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116032, acc: 0.956944\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116026, acc: 0.956946\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116019, acc: 0.956949\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116013, acc: 0.956951\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116007, acc: 0.956953\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116055, acc: 0.956950\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116049, acc: 0.956953\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116042, acc: 0.956955\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116036, acc: 0.956957\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116030, acc: 0.956960\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116023, acc: 0.956962\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116017, acc: 0.956964\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116011, acc: 0.956967\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116004, acc: 0.956969\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115998, acc: 0.956971\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115992, acc: 0.956974\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115985, acc: 0.956976\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115979, acc: 0.956978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115973, acc: 0.956981\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115966, acc: 0.956983\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115960, acc: 0.956985\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115954, acc: 0.956988\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115948, acc: 0.956990\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956965\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115989, acc: 0.956968\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115983, acc: 0.956970\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115977, acc: 0.956972\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116025, acc: 0.956920\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116073, acc: 0.956917\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116067, acc: 0.956919\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116115, acc: 0.956916\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116163, acc: 0.956913\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116156, acc: 0.956916\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116204, acc: 0.956864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116198, acc: 0.956866\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116192, acc: 0.956868\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116186, acc: 0.956871\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116179, acc: 0.956873\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116173, acc: 0.956875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116167, acc: 0.956878\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116160, acc: 0.956880\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116208, acc: 0.956873\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116202, acc: 0.956876\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116196, acc: 0.956878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116189, acc: 0.956880\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116183, acc: 0.956883\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116177, acc: 0.956885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116170, acc: 0.956887\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116164, acc: 0.956890\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116158, acc: 0.956892\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116151, acc: 0.956894\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116145, acc: 0.956897\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116139, acc: 0.956899\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116133, acc: 0.956901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116126, acc: 0.956904\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116174, acc: 0.956901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116168, acc: 0.956903\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116162, acc: 0.956905\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116155, acc: 0.956908\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116203, acc: 0.956899\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116197, acc: 0.956901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116191, acc: 0.956904\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116184, acc: 0.956906\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116178, acc: 0.956908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116172, acc: 0.956911\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116220, acc: 0.956859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116213, acc: 0.956861\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116207, acc: 0.956864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116201, acc: 0.956866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116195, acc: 0.956868\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116188, acc: 0.956871\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116182, acc: 0.956873\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116230, acc: 0.956870\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116224, acc: 0.956872\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116217, acc: 0.956875\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116211, acc: 0.956877\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116205, acc: 0.956879\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116198, acc: 0.956882\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116192, acc: 0.956884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116186, acc: 0.956886\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116234, acc: 0.956883\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116227, acc: 0.956885\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116221, acc: 0.956888\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116215, acc: 0.956890\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116208, acc: 0.956892\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116202, acc: 0.956895\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116250, acc: 0.956890\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116244, acc: 0.956893\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116237, acc: 0.956895\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116231, acc: 0.956897\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116225, acc: 0.956900\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116219, acc: 0.956902\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116212, acc: 0.956904\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116206, acc: 0.956907\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116200, acc: 0.956909\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116193, acc: 0.956911\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116187, acc: 0.956914\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116181, acc: 0.956916\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116174, acc: 0.956918\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116168, acc: 0.956921\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116216, acc: 0.956918\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116264, acc: 0.956915\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116258, acc: 0.956917\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116251, acc: 0.956919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116245, acc: 0.956922\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116239, acc: 0.956924\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116287, acc: 0.956872\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116334, acc: 0.956847\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116382, acc: 0.956796\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116376, acc: 0.956798\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116370, acc: 0.956800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116363, acc: 0.956803\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116357, acc: 0.956805\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116351, acc: 0.956807\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116345, acc: 0.956810\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116338, acc: 0.956812\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116332, acc: 0.956814\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116326, acc: 0.956817\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116319, acc: 0.956819\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116313, acc: 0.956821\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116307, acc: 0.956824\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116300, acc: 0.956826\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116294, acc: 0.956828\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116288, acc: 0.956831\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116282, acc: 0.956833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116275, acc: 0.956835\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116269, acc: 0.956838\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116263, acc: 0.956840\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116256, acc: 0.956842\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116250, acc: 0.956845\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116244, acc: 0.956847\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116238, acc: 0.956849\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116231, acc: 0.956852\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116225, acc: 0.956854\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.116273, acc: 0.956851\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116266, acc: 0.956853\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116260, acc: 0.956856\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116254, acc: 0.956858\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116248, acc: 0.956860\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116241, acc: 0.956863\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116289, acc: 0.956857\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116283, acc: 0.956859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116277, acc: 0.956862\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116270, acc: 0.956864\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116264, acc: 0.956866\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116258, acc: 0.956869\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116251, acc: 0.956871\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116245, acc: 0.956873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116239, acc: 0.956876\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116233, acc: 0.956878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116226, acc: 0.956880\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116220, acc: 0.956883\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116214, acc: 0.956885\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116207, acc: 0.956887\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116201, acc: 0.956890\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116195, acc: 0.956892\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116189, acc: 0.956894\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116182, acc: 0.956897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116176, acc: 0.956899\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116170, acc: 0.956901\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116164, acc: 0.956904\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116211, acc: 0.956888\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116205, acc: 0.956890\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116199, acc: 0.956893\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116192, acc: 0.956895\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116186, acc: 0.956897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116180, acc: 0.956900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116174, acc: 0.956902\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116167, acc: 0.956904\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116161, acc: 0.956907\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116155, acc: 0.956909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116149, acc: 0.956911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116142, acc: 0.956914\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116136, acc: 0.956916\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116130, acc: 0.956918\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116123, acc: 0.956921\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116117, acc: 0.956923\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116111, acc: 0.956925\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116105, acc: 0.956928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116098, acc: 0.956930\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116092, acc: 0.956932\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116086, acc: 0.956935\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116134, acc: 0.956910\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116127, acc: 0.956912\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116121, acc: 0.956915\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116115, acc: 0.956917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116109, acc: 0.956919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116102, acc: 0.956922\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116096, acc: 0.956924\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116090, acc: 0.956926\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116083, acc: 0.956929\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116077, acc: 0.956931\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116071, acc: 0.956933\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116065, acc: 0.956935\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116058, acc: 0.956938\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116052, acc: 0.956940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116046, acc: 0.956942\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116040, acc: 0.956945\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116033, acc: 0.956947\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116027, acc: 0.956949\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116021, acc: 0.956952\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116015, acc: 0.956954\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116008, acc: 0.956956\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116056, acc: 0.956953\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116050, acc: 0.956955\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116044, acc: 0.956957\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116037, acc: 0.956960\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116031, acc: 0.956962\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116025, acc: 0.956964\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116019, acc: 0.956967\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.956969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116006, acc: 0.956971\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116054, acc: 0.956947\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116047, acc: 0.956949\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116041, acc: 0.956951\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116035, acc: 0.956954\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116029, acc: 0.956956\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116022, acc: 0.956958\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116016, acc: 0.956961\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116010, acc: 0.956963\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116057, acc: 0.956960\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116051, acc: 0.956962\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116045, acc: 0.956964\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116039, acc: 0.956967\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116033, acc: 0.956969\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116026, acc: 0.956971\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116020, acc: 0.956974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116014, acc: 0.956976\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116008, acc: 0.956978\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116001, acc: 0.956981\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116049, acc: 0.956965\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116043, acc: 0.956967\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116036, acc: 0.956970\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116030, acc: 0.956972\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116024, acc: 0.956974\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116071, acc: 0.956970\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116065, acc: 0.956972\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116059, acc: 0.956974\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116106, acc: 0.956971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116100, acc: 0.956974\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116094, acc: 0.956976\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116088, acc: 0.956978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116082, acc: 0.956981\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116075, acc: 0.956983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116069, acc: 0.956985\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116117, acc: 0.956980\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116110, acc: 0.956982\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.116158, acc: 0.956967\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116152, acc: 0.956969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116145, acc: 0.956971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116139, acc: 0.956974\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116133, acc: 0.956976\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116180, acc: 0.956973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116174, acc: 0.956975\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116168, acc: 0.956977\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116162, acc: 0.956980\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116155, acc: 0.956982\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116149, acc: 0.956984\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116143, acc: 0.956987\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116137, acc: 0.956989\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116130, acc: 0.956991\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116124, acc: 0.956994\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116118, acc: 0.956996\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116112, acc: 0.956998\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116105, acc: 0.957001\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116099, acc: 0.957003\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116093, acc: 0.957005\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116087, acc: 0.957007\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.116081, acc: 0.957010\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.116074, acc: 0.957012\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116068, acc: 0.957014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.116062, acc: 0.957017\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.116056, acc: 0.957019\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116049, acc: 0.957021\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116043, acc: 0.957024\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116037, acc: 0.957026\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116031, acc: 0.957028\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116024, acc: 0.957031\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.116018, acc: 0.957033\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.957035\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116006, acc: 0.957037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.116000, acc: 0.957040\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115993, acc: 0.957042\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115987, acc: 0.957044\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.957047\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.957049\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.957051\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115962, acc: 0.957054\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.957056\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115950, acc: 0.957058\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115944, acc: 0.957061\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115937, acc: 0.957063\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115931, acc: 0.957065\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115925, acc: 0.957067\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115919, acc: 0.957070\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115913, acc: 0.957072\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.957074\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115900, acc: 0.957077\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115947, acc: 0.957052\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115941, acc: 0.957054\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115935, acc: 0.957057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115929, acc: 0.957059\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115923, acc: 0.957061\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115916, acc: 0.957064\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.957039\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.957041\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115951, acc: 0.957044\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.957046\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.957048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115933, acc: 0.957051\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115927, acc: 0.957053\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115974, acc: 0.957048\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.957050\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.957052\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115955, acc: 0.957055\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115949, acc: 0.957057\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115943, acc: 0.957059\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115937, acc: 0.957061\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.957010\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115978, acc: 0.957012\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115972, acc: 0.957015\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115965, acc: 0.957017\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115959, acc: 0.957019\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115953, acc: 0.957022\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115947, acc: 0.957024\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.957026\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115934, acc: 0.957029\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115982, acc: 0.957026\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.957028\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115969, acc: 0.957030\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115963, acc: 0.957032\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115957, acc: 0.957035\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115951, acc: 0.957037\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115944, acc: 0.957039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115938, acc: 0.957042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115932, acc: 0.957044\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115926, acc: 0.957046\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115973, acc: 0.956995\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115967, acc: 0.956997\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.957000\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115954, acc: 0.957002\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116002, acc: 0.956951\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956953\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115989, acc: 0.956955\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115983, acc: 0.956958\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.116030, acc: 0.956906\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.116024, acc: 0.956909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.116018, acc: 0.956911\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.116012, acc: 0.956913\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.116006, acc: 0.956916\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115999, acc: 0.956918\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115993, acc: 0.956920\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115987, acc: 0.956923\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115981, acc: 0.956925\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115975, acc: 0.956927\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.956929\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115962, acc: 0.956932\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115956, acc: 0.956934\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115950, acc: 0.956936\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115944, acc: 0.956939\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115937, acc: 0.956941\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115931, acc: 0.956943\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115925, acc: 0.956946\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115919, acc: 0.956948\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115913, acc: 0.956950\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.956952\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115900, acc: 0.956955\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115894, acc: 0.956957\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115941, acc: 0.956953\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115988, acc: 0.956950\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115982, acc: 0.956953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115976, acc: 0.956955\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115970, acc: 0.956957\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115964, acc: 0.956960\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115958, acc: 0.956962\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115951, acc: 0.956964\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115945, acc: 0.956967\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115939, acc: 0.956969\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115986, acc: 0.956965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115980, acc: 0.956967\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115974, acc: 0.956970\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115968, acc: 0.956972\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115961, acc: 0.956974\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115955, acc: 0.956977\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115949, acc: 0.956979\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115996, acc: 0.956976\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115990, acc: 0.956978\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115984, acc: 0.956980\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115978, acc: 0.956983\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115971, acc: 0.956985\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115965, acc: 0.956987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115959, acc: 0.956990\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115953, acc: 0.956992\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115947, acc: 0.956994\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115940, acc: 0.956997\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115934, acc: 0.956999\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115928, acc: 0.957001\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115922, acc: 0.957003\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115916, acc: 0.957006\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115910, acc: 0.957008\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115903, acc: 0.957010\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115897, acc: 0.957013\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115891, acc: 0.957015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115885, acc: 0.957017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115879, acc: 0.957019\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115873, acc: 0.957022\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115866, acc: 0.957024\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115860, acc: 0.957026\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115854, acc: 0.957029\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115848, acc: 0.957031\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115842, acc: 0.957033\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115835, acc: 0.957036\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115829, acc: 0.957038\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115823, acc: 0.957040\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115817, acc: 0.957042\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115811, acc: 0.957045\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115805, acc: 0.957047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115798, acc: 0.957049\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115792, acc: 0.957052\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115786, acc: 0.957054\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115780, acc: 0.957056\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115774, acc: 0.957058\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115768, acc: 0.957061\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115761, acc: 0.957063\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115755, acc: 0.957065\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115749, acc: 0.957068\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115743, acc: 0.957070\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115790, acc: 0.957066\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115784, acc: 0.957068\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115778, acc: 0.957071\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115772, acc: 0.957073\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115819, acc: 0.957068\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115812, acc: 0.957070\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115806, acc: 0.957072\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115853, acc: 0.957057\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115847, acc: 0.957059\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115841, acc: 0.957061\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115835, acc: 0.957064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115829, acc: 0.957066\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115823, acc: 0.957068\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115816, acc: 0.957071\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115810, acc: 0.957073\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115804, acc: 0.957075\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115798, acc: 0.957077\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115792, acc: 0.957080\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115786, acc: 0.957082\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115779, acc: 0.957084\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115773, acc: 0.957087\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115767, acc: 0.957089\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115814, acc: 0.957038\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115808, acc: 0.957040\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115802, acc: 0.957042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115796, acc: 0.957045\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115789, acc: 0.957047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115783, acc: 0.957049\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115777, acc: 0.957052\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115771, acc: 0.957054\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115818, acc: 0.957038\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115865, acc: 0.957014\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115859, acc: 0.957016\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115906, acc: 0.957012\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115900, acc: 0.957014\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115894, acc: 0.957017\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115887, acc: 0.957019\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115881, acc: 0.957021\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115875, acc: 0.957024\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115869, acc: 0.957026\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115863, acc: 0.957028\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115857, acc: 0.957030\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115904, acc: 0.957027\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115897, acc: 0.957030\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115891, acc: 0.957032\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115885, acc: 0.957034\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115879, acc: 0.957036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115873, acc: 0.957039\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115867, acc: 0.957041\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115860, acc: 0.957043\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115854, acc: 0.957046\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115848, acc: 0.957048\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115842, acc: 0.957050\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115836, acc: 0.957052\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115883, acc: 0.957037\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115877, acc: 0.957039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115870, acc: 0.957042\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115864, acc: 0.957044\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115858, acc: 0.957046\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115852, acc: 0.957048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115846, acc: 0.957051\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115840, acc: 0.957053\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115834, acc: 0.957055\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115827, acc: 0.957058\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115821, acc: 0.957060\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115815, acc: 0.957062\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115809, acc: 0.957064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115803, acc: 0.957067\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115797, acc: 0.957069\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115791, acc: 0.957071\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115784, acc: 0.957073\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115778, acc: 0.957076\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115772, acc: 0.957078\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115766, acc: 0.957080\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115760, acc: 0.957083\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115807, acc: 0.957077\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115801, acc: 0.957080\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115794, acc: 0.957082\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115788, acc: 0.957084\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115782, acc: 0.957086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115776, acc: 0.957089\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.957091\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115764, acc: 0.957093\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115758, acc: 0.957095\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115752, acc: 0.957098\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115745, acc: 0.957100\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115739, acc: 0.957102\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115733, acc: 0.957105\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957107\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957109\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957111\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957114\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115702, acc: 0.957116\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.957118\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.957120\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957123\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957125\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115672, acc: 0.957127\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115666, acc: 0.957130\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115660, acc: 0.957132\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957134\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115700, acc: 0.957110\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115694, acc: 0.957112\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115741, acc: 0.957104\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115735, acc: 0.957106\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115729, acc: 0.957108\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115723, acc: 0.957111\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115716, acc: 0.957113\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115710, acc: 0.957115\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115704, acc: 0.957117\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115698, acc: 0.957120\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957122\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115686, acc: 0.957124\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115680, acc: 0.957127\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115674, acc: 0.957129\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957131\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957133\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957136\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957138\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.957133\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.957135\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957137\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957139\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957142\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.957144\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957146\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957148\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.957151\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.957153\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115635, acc: 0.957155\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115682, acc: 0.957152\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115675, acc: 0.957155\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115669, acc: 0.957157\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115663, acc: 0.957159\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115657, acc: 0.957161\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115651, acc: 0.957164\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115645, acc: 0.957166\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115639, acc: 0.957168\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115633, acc: 0.957170\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957166\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115726, acc: 0.957142\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115720, acc: 0.957144\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115714, acc: 0.957146\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115708, acc: 0.957149\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115702, acc: 0.957151\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115748, acc: 0.957136\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115742, acc: 0.957138\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115736, acc: 0.957140\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957142\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115724, acc: 0.957145\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115718, acc: 0.957147\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115712, acc: 0.957149\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115706, acc: 0.957151\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957154\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957147\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.957149\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957152\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957154\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115722, acc: 0.957156\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115716, acc: 0.957158\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957161\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115703, acc: 0.957163\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957165\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957168\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957170\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957172\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957174\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957177\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957179\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957181\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115648, acc: 0.957183\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957186\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957188\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.957164\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957166\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957168\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.957171\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115658, acc: 0.957173\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115652, acc: 0.957175\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957177\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957180\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957182\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115628, acc: 0.957184\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115622, acc: 0.957186\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115616, acc: 0.957189\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115610, acc: 0.957191\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115604, acc: 0.957193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957195\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115644, acc: 0.957171\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957121\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957123\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957125\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115672, acc: 0.957128\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115666, acc: 0.957130\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115660, acc: 0.957132\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115654, acc: 0.957134\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115648, acc: 0.957137\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957139\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957141\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115630, acc: 0.957143\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957146\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957148\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957150\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957152\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957155\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957131\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957133\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957135\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115628, acc: 0.957137\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115622, acc: 0.957140\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115668, acc: 0.957137\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957139\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115656, acc: 0.957141\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115650, acc: 0.957143\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115644, acc: 0.957146\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115638, acc: 0.957148\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115632, acc: 0.957150\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115625, acc: 0.957152\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115619, acc: 0.957155\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115613, acc: 0.957157\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115607, acc: 0.957159\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115601, acc: 0.957161\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115595, acc: 0.957164\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115589, acc: 0.957166\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957168\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957170\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115571, acc: 0.957173\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115565, acc: 0.957175\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115559, acc: 0.957177\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957174\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957176\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.957179\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957181\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115633, acc: 0.957157\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115680, acc: 0.957153\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115674, acc: 0.957155\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115668, acc: 0.957157\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957159\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115656, acc: 0.957162\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957164\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.957166\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115637, acc: 0.957168\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115684, acc: 0.957165\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115678, acc: 0.957167\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115672, acc: 0.957169\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115718, acc: 0.957145\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115712, acc: 0.957147\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115706, acc: 0.957150\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115700, acc: 0.957152\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957148\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.957151\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957153\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957155\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115722, acc: 0.957157\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115716, acc: 0.957160\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115710, acc: 0.957162\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115704, acc: 0.957164\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115698, acc: 0.957166\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115692, acc: 0.957169\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115686, acc: 0.957171\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957173\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957178\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957180\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957182\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957184\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.957181\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115689, acc: 0.957183\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115736, acc: 0.957133\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957135\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115776, acc: 0.957131\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.957134\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115764, acc: 0.957136\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115758, acc: 0.957138\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115752, acc: 0.957140\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115746, acc: 0.957143\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115740, acc: 0.957145\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115734, acc: 0.957147\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115728, acc: 0.957149\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115722, acc: 0.957152\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957154\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957156\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115703, acc: 0.957158\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957161\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957163\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957165\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957167\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957170\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957172\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957174\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957176\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957179\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.957181\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115637, acc: 0.957183\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.957159\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115729, acc: 0.957156\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115723, acc: 0.957158\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115717, acc: 0.957160\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115711, acc: 0.957162\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115705, acc: 0.957165\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115699, acc: 0.957167\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115693, acc: 0.957169\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115739, acc: 0.957154\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115733, acc: 0.957156\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115727, acc: 0.957158\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957161\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957163\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957165\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115703, acc: 0.957167\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957170\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957172\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957174\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957176\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957179\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957181\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957183\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115654, acc: 0.957185\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115648, acc: 0.957188\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115642, acc: 0.957190\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115636, acc: 0.957192\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115630, acc: 0.957194\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115624, acc: 0.957196\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115618, acc: 0.957199\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115612, acc: 0.957201\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115606, acc: 0.957203\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115600, acc: 0.957205\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115594, acc: 0.957208\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115588, acc: 0.957210\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115582, acc: 0.957212\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115576, acc: 0.957214\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115570, acc: 0.957217\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115564, acc: 0.957219\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115558, acc: 0.957221\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115552, acc: 0.957223\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115546, acc: 0.957226\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115540, acc: 0.957228\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957230\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957232\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115521, acc: 0.957235\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957237\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115562, acc: 0.957232\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115608, acc: 0.957229\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115602, acc: 0.957231\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115596, acc: 0.957233\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115590, acc: 0.957236\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115584, acc: 0.957238\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115578, acc: 0.957240\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115572, acc: 0.957242\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115566, acc: 0.957245\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115560, acc: 0.957247\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115553, acc: 0.957249\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115547, acc: 0.957251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957253\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957256\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957258\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115523, acc: 0.957260\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115517, acc: 0.957262\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957265\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957267\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.957269\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957271\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115487, acc: 0.957274\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115481, acc: 0.957276\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957273\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115521, acc: 0.957275\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957277\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957280\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115503, acc: 0.957282\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957284\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957286\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957288\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115479, acc: 0.957291\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.957293\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115467, acc: 0.957295\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115461, acc: 0.957297\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957282\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957284\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957287\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957284\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957286\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957288\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115523, acc: 0.957290\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115517, acc: 0.957293\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115563, acc: 0.957290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957292\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.957294\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957296\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957303\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957300\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957302\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115561, acc: 0.957304\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957307\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115549, acc: 0.957309\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957311\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.957313\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957318\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957322\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957324\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957329\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957331\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957316\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115523, acc: 0.957320\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115517, acc: 0.957323\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115511, acc: 0.957325\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115505, acc: 0.957327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115499, acc: 0.957329\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115493, acc: 0.957332\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115487, acc: 0.957334\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115481, acc: 0.957336\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115475, acc: 0.957338\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115469, acc: 0.957340\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115463, acc: 0.957343\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115457, acc: 0.957345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115451, acc: 0.957347\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115445, acc: 0.957349\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115439, acc: 0.957352\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115433, acc: 0.957354\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115479, acc: 0.957330\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115473, acc: 0.957332\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957329\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115565, acc: 0.957326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115559, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957325\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.957329\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957332\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957334\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.957336\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957338\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115563, acc: 0.957340\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957343\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.957345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957347\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957354\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115521, acc: 0.957356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957358\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957360\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115503, acc: 0.957363\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957365\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115491, acc: 0.957367\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115485, acc: 0.957369\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957365\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957367\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957372\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957374\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957376\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957378\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957380\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115483, acc: 0.957383\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115477, acc: 0.957385\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115471, acc: 0.957387\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115465, acc: 0.957389\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115459, acc: 0.957392\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115453, acc: 0.957394\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115447, acc: 0.957396\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115441, acc: 0.957398\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115435, acc: 0.957400\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115481, acc: 0.957397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115475, acc: 0.957400\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115572, acc: 0.957298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115566, acc: 0.957300\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115560, acc: 0.957302\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115554, acc: 0.957305\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115600, acc: 0.957290\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115646, acc: 0.957287\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115640, acc: 0.957289\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115634, acc: 0.957291\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115628, acc: 0.957293\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115674, acc: 0.957290\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115668, acc: 0.957292\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115662, acc: 0.957294\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115656, acc: 0.957296\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115702, acc: 0.957281\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115696, acc: 0.957283\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115690, acc: 0.957286\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115736, acc: 0.957236\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115730, acc: 0.957238\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115776, acc: 0.957233\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115770, acc: 0.957235\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115764, acc: 0.957237\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115758, acc: 0.957240\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115752, acc: 0.957242\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115798, acc: 0.957238\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115792, acc: 0.957241\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115786, acc: 0.957243\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115832, acc: 0.957238\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115826, acc: 0.957241\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115820, acc: 0.957243\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115813, acc: 0.957245\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115807, acc: 0.957247\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115801, acc: 0.957250\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115795, acc: 0.957252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115789, acc: 0.957254\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115835, acc: 0.957204\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115829, acc: 0.957207\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115823, acc: 0.957209\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115817, acc: 0.957211\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115811, acc: 0.957213\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115805, acc: 0.957215\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115799, acc: 0.957218\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115793, acc: 0.957220\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115787, acc: 0.957222\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115781, acc: 0.957224\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115775, acc: 0.957227\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115769, acc: 0.957229\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115763, acc: 0.957231\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115757, acc: 0.957233\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115751, acc: 0.957235\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115745, acc: 0.957238\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115791, acc: 0.957223\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115785, acc: 0.957225\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115779, acc: 0.957227\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115773, acc: 0.957229\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115767, acc: 0.957231\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115761, acc: 0.957234\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115755, acc: 0.957236\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115749, acc: 0.957238\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115743, acc: 0.957240\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115737, acc: 0.957242\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115731, acc: 0.957245\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115725, acc: 0.957247\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115719, acc: 0.957249\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115713, acc: 0.957251\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115707, acc: 0.957254\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115701, acc: 0.957256\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115695, acc: 0.957258\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115689, acc: 0.957260\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115683, acc: 0.957262\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957265\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.957269\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957271\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957273\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.957276\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.957278\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115635, acc: 0.957280\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.957282\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115675, acc: 0.957259\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115721, acc: 0.957235\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115715, acc: 0.957237\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115709, acc: 0.957239\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115703, acc: 0.957242\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115697, acc: 0.957244\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115691, acc: 0.957246\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115685, acc: 0.957248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115679, acc: 0.957250\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115673, acc: 0.957253\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115667, acc: 0.957255\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115661, acc: 0.957257\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115655, acc: 0.957259\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115649, acc: 0.957262\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115643, acc: 0.957264\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115637, acc: 0.957266\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115631, acc: 0.957268\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115677, acc: 0.957265\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115671, acc: 0.957267\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115665, acc: 0.957270\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115659, acc: 0.957272\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115653, acc: 0.957274\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115647, acc: 0.957276\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115641, acc: 0.957278\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115635, acc: 0.957281\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115629, acc: 0.957283\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115623, acc: 0.957285\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115617, acc: 0.957287\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.957289\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957292\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957294\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.957296\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.957303\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115621, acc: 0.957300\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115615, acc: 0.957302\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115609, acc: 0.957304\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115603, acc: 0.957306\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957309\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115591, acc: 0.957311\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115585, acc: 0.957313\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957315\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957317\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957320\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115561, acc: 0.957322\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957324\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115549, acc: 0.957326\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957328\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115589, acc: 0.957313\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115583, acc: 0.957316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115577, acc: 0.957318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115571, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115565, acc: 0.957322\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115611, acc: 0.957319\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115605, acc: 0.957321\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115599, acc: 0.957324\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115593, acc: 0.957326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115587, acc: 0.957328\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115581, acc: 0.957330\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115575, acc: 0.957332\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957335\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115563, acc: 0.957337\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957339\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115603, acc: 0.957336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115597, acc: 0.957338\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115591, acc: 0.957340\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115585, acc: 0.957342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115579, acc: 0.957344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115573, acc: 0.957347\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115567, acc: 0.957349\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115561, acc: 0.957351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115555, acc: 0.957353\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115549, acc: 0.957355\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115543, acc: 0.957358\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.957360\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957362\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957364\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957366\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957369\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957371\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115502, acc: 0.957373\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115496, acc: 0.957375\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115541, acc: 0.957352\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115535, acc: 0.957354\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115529, acc: 0.957356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115523, acc: 0.957358\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115569, acc: 0.957309\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115563, acc: 0.957311\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115557, acc: 0.957313\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115551, acc: 0.957315\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115545, acc: 0.957318\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115539, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115533, acc: 0.957322\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115527, acc: 0.957324\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115521, acc: 0.957326\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115515, acc: 0.957329\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115509, acc: 0.957331\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115503, acc: 0.957333\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115497, acc: 0.957335\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115492, acc: 0.957337\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115537, acc: 0.957314\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115531, acc: 0.957316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115525, acc: 0.957318\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115519, acc: 0.957320\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115513, acc: 0.957323\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115507, acc: 0.957325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115501, acc: 0.957327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115495, acc: 0.957329\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115489, acc: 0.957331\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115484, acc: 0.957334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115478, acc: 0.957336\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115472, acc: 0.957338\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115466, acc: 0.957340\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115460, acc: 0.957342\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115454, acc: 0.957345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115448, acc: 0.957347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115442, acc: 0.957349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115436, acc: 0.957351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115430, acc: 0.957353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115424, acc: 0.957356\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115418, acc: 0.957358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115412, acc: 0.957360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115406, acc: 0.957362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115400, acc: 0.957364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115395, acc: 0.957367\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115389, acc: 0.957369\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115383, acc: 0.957371\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115377, acc: 0.957373\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115371, acc: 0.957375\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.957377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115359, acc: 0.957380\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115353, acc: 0.957382\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115347, acc: 0.957384\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.957386\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115335, acc: 0.957388\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115329, acc: 0.957391\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115323, acc: 0.957393\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115317, acc: 0.957395\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957371\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115357, acc: 0.957374\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115351, acc: 0.957376\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115345, acc: 0.957378\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115339, acc: 0.957380\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115333, acc: 0.957382\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115327, acc: 0.957385\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115321, acc: 0.957387\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957391\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957393\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.957396\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115292, acc: 0.957398\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.957400\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957402\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957404\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957406\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957409\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115256, acc: 0.957411\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.957413\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957417\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957420\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.957422\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957424\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115215, acc: 0.957426\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115209, acc: 0.957428\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957431\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115248, acc: 0.957407\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115243, acc: 0.957409\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115237, acc: 0.957411\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115231, acc: 0.957414\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957416\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957418\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115213, acc: 0.957420\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957422\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957425\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957429\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957431\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957428\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115274, acc: 0.957379\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957381\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957383\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957334\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957339\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957341\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.957343\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.957345\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115272, acc: 0.957347\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957352\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.957349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.957353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115288, acc: 0.957355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957358\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115327, acc: 0.957334\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957336\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.957345\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957296\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957298\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115331, acc: 0.957301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957303\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957305\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957307\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957309\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957311\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957314\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115341, acc: 0.957311\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115335, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957315\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115375, acc: 0.957312\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115369, acc: 0.957314\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115363, acc: 0.957316\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115357, acc: 0.957319\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115351, acc: 0.957321\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115345, acc: 0.957323\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115339, acc: 0.957325\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115333, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957330\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957332\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957283\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957285\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957287\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957289\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957292\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115337, acc: 0.957294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957296\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957298\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957300\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957302\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957305\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957307\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957309\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.957313\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957310\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115324, acc: 0.957313\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115318, acc: 0.957315\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957317\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.957319\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957321\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115294, acc: 0.957324\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115288, acc: 0.957326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957328\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.957330\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957332\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.957334\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115259, acc: 0.957337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115253, acc: 0.957339\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115247, acc: 0.957341\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115241, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.957345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957347\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115224, acc: 0.957350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957352\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957354\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115257, acc: 0.957343\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957295\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957297\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115290, acc: 0.957299\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.957301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957303\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957305\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115318, acc: 0.957282\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957284\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.957287\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957289\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115345, acc: 0.957286\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115339, acc: 0.957288\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957290\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957292\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957294\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957291\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957293\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957295\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957298\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957300\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115338, acc: 0.957302\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957306\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957308\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957311\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957315\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957317\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957319\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.957321\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957326\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957328\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957330\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.957332\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115249, acc: 0.957335\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.957339\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957341\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.957337\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957336\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957338\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957335\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957338\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115343, acc: 0.957340\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115338, acc: 0.957342\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957346\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957348\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957355\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.957357\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.957361\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957366\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115267, acc: 0.957368\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115261, acc: 0.957370\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.957364\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115295, acc: 0.957368\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957370\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.957372\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.957375\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957377\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115265, acc: 0.957379\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115310, acc: 0.957330\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957332\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115299, acc: 0.957335\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.957337\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957339\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957316\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957322\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115308, acc: 0.957324\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115302, acc: 0.957327\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.957329\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957331\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115336, acc: 0.957327\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115330, acc: 0.957330\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115324, acc: 0.957332\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115318, acc: 0.957334\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115306, acc: 0.957338\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115301, acc: 0.957340\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115295, acc: 0.957343\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957345\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115283, acc: 0.957347\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957324\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957322\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957325\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957327\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115344, acc: 0.957331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115338, acc: 0.957333\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115377, acc: 0.957332\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115371, acc: 0.957335\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115365, acc: 0.957337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115359, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115404, acc: 0.957336\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115398, acc: 0.957338\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115392, acc: 0.957340\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115387, acc: 0.957343\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115381, acc: 0.957345\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115426, acc: 0.957296\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115420, acc: 0.957298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115414, acc: 0.957300\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115408, acc: 0.957303\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115402, acc: 0.957305\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115396, acc: 0.957307\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115390, acc: 0.957309\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115385, acc: 0.957311\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115379, acc: 0.957313\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115373, acc: 0.957316\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115367, acc: 0.957318\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115361, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115355, acc: 0.957322\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115349, acc: 0.957324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115344, acc: 0.957326\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115338, acc: 0.957329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115332, acc: 0.957331\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115326, acc: 0.957333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115320, acc: 0.957335\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115314, acc: 0.957337\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115309, acc: 0.957339\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115303, acc: 0.957342\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115297, acc: 0.957344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957346\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115285, acc: 0.957348\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115279, acc: 0.957350\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957352\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957355\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115312, acc: 0.957331\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115307, acc: 0.957334\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115351, acc: 0.957285\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115346, acc: 0.957287\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115340, acc: 0.957289\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115334, acc: 0.957292\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115328, acc: 0.957294\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115322, acc: 0.957296\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115316, acc: 0.957298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115311, acc: 0.957300\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115305, acc: 0.957302\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115299, acc: 0.957304\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.957307\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957309\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115281, acc: 0.957311\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115276, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115270, acc: 0.957315\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.957317\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115258, acc: 0.957320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115252, acc: 0.957322\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957324\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115241, acc: 0.957326\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115229, acc: 0.957330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115217, acc: 0.957335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115211, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957339\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.957341\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957346\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957348\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957350\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957352\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957354\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957356\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.957361\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957363\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957365\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957367\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957369\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957371\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957374\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957376\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957378\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957355\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.957306\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957309\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.957315\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957317\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115206, acc: 0.957314\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.957316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.957319\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115188, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957323\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957325\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115165, acc: 0.957329\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957332\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957334\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957336\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957338\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957340\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957344\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957347\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957349\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957351\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957353\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957355\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957360\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957364\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957366\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957368\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957370\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115049, acc: 0.957372\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957375\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957370\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957372\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957374\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957378\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957380\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957332\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957334\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957331\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957333\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957336\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957338\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957340\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957344\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957346\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957323\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957325\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957321\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957323\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.957326\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957328\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957330\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.957332\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115195, acc: 0.957329\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957331\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957333\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115177, acc: 0.957335\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115160, acc: 0.957341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957344\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957346\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957348\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957350\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957352\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957354\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957361\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957363\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957367\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957372\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957374\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957376\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957378\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957380\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957382\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957384\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957338\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957340\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957345\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957297\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957299\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957303\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957305\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957307\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957309\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957312\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957314\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957318\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957322\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957325\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957327\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957329\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957331\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957340\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114988, acc: 0.957342\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114982, acc: 0.957344\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114977, acc: 0.957346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114971, acc: 0.957348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114965, acc: 0.957350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.957352\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114954, acc: 0.957355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114942, acc: 0.957359\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957363\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957367\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114963, acc: 0.957364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114958, acc: 0.957366\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114952, acc: 0.957368\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114946, acc: 0.957370\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114940, acc: 0.957373\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.957375\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957377\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957379\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957381\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.957383\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957385\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.957388\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.957390\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957392\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.957394\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957396\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957398\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957400\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957402\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114854, acc: 0.957405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957407\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957409\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957411\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.957413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.957415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957417\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957420\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957422\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957426\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114840, acc: 0.957378\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.957357\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.957359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.957361\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957358\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.957362\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114933, acc: 0.957344\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957296\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114972, acc: 0.957298\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957300\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114960, acc: 0.957302\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957304\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957306\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957308\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957313\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114914, acc: 0.957319\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957323\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957326\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957328\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957330\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957325\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114924, acc: 0.957327\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957329\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957331\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957336\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957338\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957340\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.957344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.957346\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114867, acc: 0.957348\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957350\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957353\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114849, acc: 0.957355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957357\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957334\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957336\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957338\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114871, acc: 0.957340\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957343\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957345\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957353\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957355\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.957357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957360\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.957364\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957366\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957368\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957370\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957372\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957325\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957329\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957331\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957337\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.957339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957342\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957344\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957346\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957352\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957354\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957361\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957363\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957365\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957367\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957369\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957371\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957374\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957378\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957380\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957382\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957384\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957386\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957388\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957366\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957366\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957369\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957371\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957323\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957325\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957327\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957329\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957331\n",
      "target: tensor([7.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114838, acc: 0.957321\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957323\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957325\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957327\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957280\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957282\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957286\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957288\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957290\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957292\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957278\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957280\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114863, acc: 0.957282\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114857, acc: 0.957284\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957282\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957284\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957286\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114884, acc: 0.957288\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.957290\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114873, acc: 0.957292\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957244\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.957238\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957240\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114949, acc: 0.957243\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114944, acc: 0.957245\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957247\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957249\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957251\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957253\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957255\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957260\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.957262\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957264\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957266\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.957268\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957270\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957274\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957277\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.957279\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114846, acc: 0.957281\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957283\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.957285\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957287\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957289\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957291\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.957294\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957296\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957298\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.957293\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957295\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957299\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.957301\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957304\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957306\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957310\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957314\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957316\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957318\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957323\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957325\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957329\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957333\n",
      "target: tensor([9.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957325\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957328\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957332\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.957334\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957336\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957338\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957340\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957342\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957345\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957347\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957299\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957301\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957305\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957308\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957310\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957312\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957314\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957318\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957320\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957322\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957325\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957327\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957329\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957333\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957285\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957290\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957292\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957294\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957296\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957298\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957284\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957236\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957238\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.957240\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957243\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957245\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957197\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957199\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957201\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.957204\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.957206\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957208\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957210\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957212\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957214\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957211\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114837, acc: 0.957213\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114831, acc: 0.957215\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957217\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957219\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957216\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957218\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114852, acc: 0.957220\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957222\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114841, acc: 0.957224\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.957226\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114879, acc: 0.957224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.957228\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114862, acc: 0.957230\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.957232\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114851, acc: 0.957234\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.957236\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957239\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114834, acc: 0.957241\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114878, acc: 0.957193\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.957195\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.957197\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957200\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957202\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957199\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957201\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.957203\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957205\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114876, acc: 0.957207\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114870, acc: 0.957209\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114865, acc: 0.957212\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957214\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957216\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114847, acc: 0.957218\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957220\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114836, acc: 0.957222\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114830, acc: 0.957224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114825, acc: 0.957226\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957229\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957231\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957233\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957235\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957237\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957241\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957243\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957245\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957248\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957250\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957252\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957254\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957256\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957258\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957260\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957262\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957264\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957267\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957269\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957271\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957273\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957275\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957277\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957279\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957281\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957283\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957286\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957288\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957290\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957292\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957294\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957296\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957291\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957288\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957287\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957289\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957291\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957293\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957295\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957297\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957299\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957301\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957303\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957306\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957310\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957312\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957314\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957318\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957322\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957327\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957329\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957335\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957339\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957333\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957335\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957337\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957342\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957344\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957346\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957348\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957350\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957352\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957354\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957356\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957361\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957363\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957365\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957367\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957369\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957371\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957373\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957375\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957377\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957379\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957382\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957386\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957388\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957384\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957387\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957389\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957386\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957388\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957390\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957392\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957394\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957396\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957398\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957401\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957403\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957407\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957409\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957411\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957413\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957415\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957417\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957414\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957391\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957394\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957346\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957353\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957355\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957359\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957355\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957352\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957354\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957353\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957356\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957360\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957362\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957366\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957368\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957321\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957322\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957325\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957327\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957329\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957306\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957308\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957310\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957315\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957317\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957319\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957315\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957318\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957320\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957322\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957324\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957328\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957330\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957322\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957325\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957327\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.957329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957333\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957335\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957339\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957341\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957345\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957352\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957354\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957356\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957358\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957355\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957352\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957355\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957357\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957354\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957358\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957360\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957362\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957364\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957366\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957368\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957371\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957373\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957375\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957377\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957379\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957381\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957383\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957385\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957387\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957389\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957391\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957394\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957396\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957398\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957400\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957402\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957404\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957406\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957408\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957410\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957412\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957414\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957416\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957412\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957417\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957419\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957372\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957374\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957378\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957380\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957382\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957386\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957391\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957393\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957395\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957397\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957399\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957401\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957403\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957405\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957402\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957404\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957406\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957408\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957410\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957412\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957414\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957416\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957419\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957421\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957425\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957427\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957429\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957431\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957433\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957435\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957437\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957439\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957441\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957444\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957446\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957448\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957450\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957452\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957454\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957456\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957453\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957455\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957457\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957459\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957462\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957464\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957466\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957468\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957470\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957472\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957474\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957476\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957478\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957475\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957477\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957479\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957482\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957478\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957480\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957482\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957484\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957486\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957488\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957490\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957492\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957494\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957496\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957498\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957500\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957502\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957505\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957507\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957509\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957513\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957515\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957517\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957519\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957521\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957518\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957515\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957517\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957519\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957521\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957523\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957520\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957522\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957524\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957526\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957528\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957530\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957532\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957534\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957541\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957545\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957498\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957500\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957502\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957504\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957506\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957508\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957511\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957513\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957515\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957517\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957519\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957521\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957523\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957527\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957529\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957531\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957533\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957535\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957537\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957539\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957542\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957544\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957546\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957548\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957550\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957546\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957546\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957548\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957526\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957528\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957523\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957525\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957527\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114540, acc: 0.957529\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957482\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957484\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957486\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957489\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957491\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957493\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957495\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957497\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957499\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957501\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957503\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957505\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957507\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957509\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957511\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957513\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957515\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957517\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957495\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957497\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957499\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957501\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957503\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957501\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957503\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957505\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957507\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114530, acc: 0.957509\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957511\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957513\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957515\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957517\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957519\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957521\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957523\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957525\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957528\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957530\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957532\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957534\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957536\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957538\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957540\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957542\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957539\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957543\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957521\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957523\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957525\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957527\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957529\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957531\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957533\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957536\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957533\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957535\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957539\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957543\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957545\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957547\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957549\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957551\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957553\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957555\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957557\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957560\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957562\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957564\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957566\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957565\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957567\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957569\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957571\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957575\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957577\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957579\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957575\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957577\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957579\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957584\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957586\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957588\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957590\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957592\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957596\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957598\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957600\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957604\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957606\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957608\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957610\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957612\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957614\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957612\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957614\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957616\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957618\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957620\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957622\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957624\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957621\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957623\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957625\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957627\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957629\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957633\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957635\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957638\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957634\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957636\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957638\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957640\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957642\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957644\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957647\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957649\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957651\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957648\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957650\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957652\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957654\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957656\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957658\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957660\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957662\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957664\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957666\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957668\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957663\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957665\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957668\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957672\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957674\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957676\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957672\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957674\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957652\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957654\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957608\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957610\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957612\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957614\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114467, acc: 0.957616\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957618\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957620\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957622\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957624\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957627\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957629\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957631\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957633\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957635\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957637\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957641\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957643\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957645\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957647\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957649\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957651\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957653\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957655\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957654\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957656\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957652\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957654\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957656\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957658\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957660\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957662\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957640\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957642\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957644\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957646\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957643\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957645\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957647\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957649\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957653\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957655\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957657\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957659\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957661\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957664\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957641\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957643\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957599\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957579\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957581\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957583\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957569\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957574\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957576\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957580\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957584\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114521, acc: 0.957586\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957590\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957592\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957570\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957572\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957574\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957576\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957578\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957580\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957582\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957584\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957586\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957588\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957590\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957592\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114481, acc: 0.957594\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957596\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957598\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957595\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957597\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957599\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957603\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957601\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957603\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957605\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957559\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957561\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957559\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957557\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957561\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957557\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957559\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957566\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([11.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957560\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957562\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957561\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957563\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957565\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957567\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957569\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957574\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957576\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957578\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957580\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957560\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957562\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957564\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957566\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957568\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957572\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957574\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957576\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957578\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957580\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957582\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957560\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957562\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957566\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957568\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957570\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957524\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957526\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957530\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957533\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957535\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957537\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957533\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957535\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957537\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957539\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957541\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957538\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957540\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957542\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957546\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957548\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957552\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957554\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957556\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957558\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957560\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957562\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957564\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957543\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957545\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957547\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957543\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957540\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.957535\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957537\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957539\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.957541\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114844, acc: 0.957543\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114839, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957547\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957549\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.957551\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957553\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957555\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957557\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957559\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.957562\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957566\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957568\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957572\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957574\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957571\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114799, acc: 0.957573\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957575\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957577\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957581\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957583\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957585\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957582\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957584\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957586\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957589\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957591\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114775, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114770, acc: 0.957597\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114812, acc: 0.957593\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114807, acc: 0.957595\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957552\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957556\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957558\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957560\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957557\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957559\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957561\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114895, acc: 0.957563\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114938, acc: 0.957560\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957558\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115022, acc: 0.957555\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957557\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115011, acc: 0.957559\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115006, acc: 0.957561\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115000, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114995, acc: 0.957565\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957567\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114984, acc: 0.957569\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957571\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957573\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957572\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957566\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957568\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957565\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957567\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957575\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957577\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957579\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957565\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957567\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115166, acc: 0.957564\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115208, acc: 0.957561\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957563\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115197, acc: 0.957565\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957567\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115186, acc: 0.957569\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957571\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115175, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115170, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957577\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115159, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957581\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957583\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957585\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957588\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957590\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957592\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957594\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957596\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957598\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957600\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957602\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957604\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957606\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957602\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957604\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957606\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957609\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957611\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957613\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957615\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957617\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957619\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957621\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957625\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957627\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957629\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957626\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957628\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957625\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957627\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957629\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957631\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957633\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957635\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957637\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957639\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957641\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957643\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957645\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957642\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957644\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957646\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957648\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957650\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957652\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957654\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957656\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957658\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957660\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957662\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957664\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957666\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957668\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957672\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957674\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957653\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957655\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957657\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957659\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957661\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957663\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957665\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957667\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957645\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957647\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957649\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957651\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957653\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957655\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957657\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957612\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957614\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957616\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957618\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957620\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957622\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957624\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957626\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957604\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957606\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957608\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957610\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957612\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957614\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957616\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115057, acc: 0.957618\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115052, acc: 0.957620\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957622\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115041, acc: 0.957624\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957626\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115030, acc: 0.957628\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957625\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957627\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115061, acc: 0.957629\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957631\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957633\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957635\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115039, acc: 0.957637\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115034, acc: 0.957639\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115028, acc: 0.957641\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115023, acc: 0.957643\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957640\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957642\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957644\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957646\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957603\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957605\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957601\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957603\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957558\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957560\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115147, acc: 0.957562\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957559\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957563\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957565\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.957567\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957571\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957573\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957575\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957577\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957579\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957587\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957590\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957592\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957594\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115091, acc: 0.957596\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957598\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957600\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957602\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957604\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957606\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957608\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957610\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115047, acc: 0.957612\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957614\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115036, acc: 0.957616\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957618\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957614\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957616\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957618\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957620\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957622\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115046, acc: 0.957624\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957626\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957628\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957630\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115024, acc: 0.957632\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115018, acc: 0.957634\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115013, acc: 0.957636\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957638\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115002, acc: 0.957641\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957643\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115038, acc: 0.957597\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115033, acc: 0.957599\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115027, acc: 0.957601\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115022, acc: 0.957603\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957581\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957583\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957538\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957535\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957537\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957539\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115173, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957538\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957540\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957542\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957544\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957546\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957548\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957550\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957552\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957554\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957556\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957558\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957560\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957564\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957560\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957564\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957566\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957568\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957570\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957572\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957574\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.957528\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957530\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957532\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957528\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957530\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957532\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957534\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957536\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957538\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957540\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957542\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957545\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957547\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957549\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957551\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957553\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957555\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957557\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957559\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957561\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957563\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957559\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957561\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957563\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957565\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115101, acc: 0.957567\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957569\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957571\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957573\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957575\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957577\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957579\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957581\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957583\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957562\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957564\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957566\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957521\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957523\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957525\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115161, acc: 0.957503\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957505\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115150, acc: 0.957507\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957504\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957506\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115181, acc: 0.957508\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957510\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115218, acc: 0.957507\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957509\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957513\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957515\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115238, acc: 0.957505\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115232, acc: 0.957507\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115227, acc: 0.957509\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115221, acc: 0.957511\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115216, acc: 0.957513\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115210, acc: 0.957515\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957517\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115200, acc: 0.957519\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.957521\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957523\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957525\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115178, acc: 0.957527\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115172, acc: 0.957529\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115167, acc: 0.957531\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115162, acc: 0.957533\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115156, acc: 0.957535\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957537\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957539\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957541\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957543\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115129, acc: 0.957545\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957547\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957549\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957551\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115107, acc: 0.957553\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115102, acc: 0.957555\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115144, acc: 0.957534\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115138, acc: 0.957536\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115133, acc: 0.957538\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115127, acc: 0.957540\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115122, acc: 0.957542\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115116, acc: 0.957544\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957546\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957550\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957552\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115089, acc: 0.957554\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115084, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957558\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115073, acc: 0.957560\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957546\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957548\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115104, acc: 0.957550\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115098, acc: 0.957552\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115093, acc: 0.957554\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957556\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115082, acc: 0.957558\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115077, acc: 0.957560\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957562\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115066, acc: 0.957564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957566\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957568\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115050, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115044, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957571\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957573\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115117, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115111, acc: 0.957562\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115106, acc: 0.957564\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115100, acc: 0.957566\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115095, acc: 0.957568\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957570\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957564\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115120, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957570\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957567\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115151, acc: 0.957569\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957571\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115140, acc: 0.957573\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957575\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957577\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957579\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957581\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115113, acc: 0.957583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957585\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957587\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957589\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957591\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115086, acc: 0.957593\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957595\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115075, acc: 0.957597\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957599\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957601\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115059, acc: 0.957603\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115054, acc: 0.957605\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957607\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115043, acc: 0.957609\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957602\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115079, acc: 0.957604\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957606\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115068, acc: 0.957608\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115063, acc: 0.957610\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957611\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957608\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957610\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115135, acc: 0.957607\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957609\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115124, acc: 0.957611\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957613\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957615\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115108, acc: 0.957617\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957619\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115097, acc: 0.957621\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115092, acc: 0.957623\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115087, acc: 0.957625\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115081, acc: 0.957627\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115076, acc: 0.957629\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115070, acc: 0.957631\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957586\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957583\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957585\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115143, acc: 0.957587\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957589\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115132, acc: 0.957591\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957593\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957595\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957585\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115157, acc: 0.957587\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115152, acc: 0.957589\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115146, acc: 0.957591\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115141, acc: 0.957593\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115136, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115130, acc: 0.957597\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115125, acc: 0.957599\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115119, acc: 0.957601\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115114, acc: 0.957603\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115109, acc: 0.957605\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115103, acc: 0.957607\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.115145, acc: 0.957562\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115139, acc: 0.957564\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115134, acc: 0.957566\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115128, acc: 0.957568\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115123, acc: 0.957570\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115118, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115112, acc: 0.957574\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115154, acc: 0.957570\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.957570\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115184, acc: 0.957572\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115179, acc: 0.957574\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957576\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115168, acc: 0.957578\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957580\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115204, acc: 0.957558\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115246, acc: 0.957537\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115240, acc: 0.957539\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115235, acc: 0.957541\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115276, acc: 0.957538\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957540\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957542\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957544\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.957546\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115296, acc: 0.957543\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115291, acc: 0.957545\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115286, acc: 0.957547\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115280, acc: 0.957549\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115275, acc: 0.957551\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115269, acc: 0.957553\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115264, acc: 0.957555\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115305, acc: 0.957552\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115300, acc: 0.957554\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115295, acc: 0.957556\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115289, acc: 0.957558\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115284, acc: 0.957560\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115278, acc: 0.957562\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115273, acc: 0.957564\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115268, acc: 0.957566\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115262, acc: 0.957568\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115304, acc: 0.957565\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115298, acc: 0.957567\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115293, acc: 0.957569\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115287, acc: 0.957571\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115282, acc: 0.957573\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115277, acc: 0.957575\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115271, acc: 0.957577\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115266, acc: 0.957579\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115260, acc: 0.957581\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115255, acc: 0.957583\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115250, acc: 0.957585\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115244, acc: 0.957587\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115239, acc: 0.957589\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115233, acc: 0.957591\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115228, acc: 0.957593\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115223, acc: 0.957595\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115217, acc: 0.957597\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957599\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957601\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957603\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957605\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115190, acc: 0.957607\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957609\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957611\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957613\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957615\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115163, acc: 0.957617\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115205, acc: 0.957614\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115199, acc: 0.957616\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115194, acc: 0.957618\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115189, acc: 0.957620\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115183, acc: 0.957622\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115225, acc: 0.957619\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115219, acc: 0.957621\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115214, acc: 0.957623\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115208, acc: 0.957625\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115203, acc: 0.957627\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115198, acc: 0.957629\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115192, acc: 0.957631\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115187, acc: 0.957633\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115182, acc: 0.957635\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115176, acc: 0.957637\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115171, acc: 0.957639\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115212, acc: 0.957626\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115207, acc: 0.957628\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115201, acc: 0.957630\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115196, acc: 0.957632\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115191, acc: 0.957634\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115185, acc: 0.957636\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115180, acc: 0.957638\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115174, acc: 0.957639\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115169, acc: 0.957641\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115164, acc: 0.957643\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115158, acc: 0.957645\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115153, acc: 0.957647\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115148, acc: 0.957649\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115142, acc: 0.957651\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115137, acc: 0.957653\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115131, acc: 0.957655\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115126, acc: 0.957657\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115121, acc: 0.957659\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115115, acc: 0.957661\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115110, acc: 0.957663\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115105, acc: 0.957665\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115099, acc: 0.957667\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115094, acc: 0.957669\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115088, acc: 0.957671\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115083, acc: 0.957673\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115078, acc: 0.957675\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115072, acc: 0.957677\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115067, acc: 0.957679\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115062, acc: 0.957681\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115056, acc: 0.957683\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115051, acc: 0.957685\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115045, acc: 0.957687\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115040, acc: 0.957689\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115035, acc: 0.957691\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115029, acc: 0.957693\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115071, acc: 0.957671\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115065, acc: 0.957673\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115060, acc: 0.957675\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115055, acc: 0.957677\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.115096, acc: 0.957656\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115090, acc: 0.957658\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115085, acc: 0.957660\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.115080, acc: 0.957662\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.115074, acc: 0.957664\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115069, acc: 0.957666\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.115064, acc: 0.957668\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115058, acc: 0.957670\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115053, acc: 0.957672\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.115048, acc: 0.957674\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115042, acc: 0.957676\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.115037, acc: 0.957678\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.115031, acc: 0.957680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115026, acc: 0.957682\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115021, acc: 0.957684\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115015, acc: 0.957686\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.115010, acc: 0.957688\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.115005, acc: 0.957690\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114999, acc: 0.957692\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114994, acc: 0.957694\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114989, acc: 0.957696\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114983, acc: 0.957697\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114978, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114973, acc: 0.957701\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114967, acc: 0.957703\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114962, acc: 0.957705\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114956, acc: 0.957707\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114998, acc: 0.957686\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114992, acc: 0.957688\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114987, acc: 0.957690\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114982, acc: 0.957692\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114976, acc: 0.957694\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.115017, acc: 0.957691\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115012, acc: 0.957693\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.115007, acc: 0.957695\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.115001, acc: 0.957697\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114996, acc: 0.957699\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114991, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114985, acc: 0.957703\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114980, acc: 0.957705\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114975, acc: 0.957707\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114969, acc: 0.957709\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114964, acc: 0.957711\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114959, acc: 0.957713\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114953, acc: 0.957715\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114948, acc: 0.957717\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114943, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114937, acc: 0.957721\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114932, acc: 0.957723\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114926, acc: 0.957725\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114921, acc: 0.957727\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114910, acc: 0.957731\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114905, acc: 0.957733\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114900, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114894, acc: 0.957736\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957715\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114930, acc: 0.957717\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957719\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114966, acc: 0.957675\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114961, acc: 0.957677\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114955, acc: 0.957679\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114950, acc: 0.957680\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114945, acc: 0.957682\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114939, acc: 0.957684\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114934, acc: 0.957686\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114929, acc: 0.957688\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114923, acc: 0.957690\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114918, acc: 0.957692\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957694\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114907, acc: 0.957696\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114902, acc: 0.957698\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957700\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114891, acc: 0.957702\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114886, acc: 0.957704\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.957706\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114875, acc: 0.957708\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114916, acc: 0.957704\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114911, acc: 0.957706\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957708\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114947, acc: 0.957705\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114941, acc: 0.957707\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114936, acc: 0.957709\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114931, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114925, acc: 0.957713\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114920, acc: 0.957715\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114915, acc: 0.957717\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957719\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957721\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114899, acc: 0.957723\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957725\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957727\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114883, acc: 0.957728\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957730\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114919, acc: 0.957686\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114913, acc: 0.957688\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114908, acc: 0.957690\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114903, acc: 0.957692\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114897, acc: 0.957694\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114892, acc: 0.957696\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114887, acc: 0.957698\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114881, acc: 0.957700\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114922, acc: 0.957655\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114917, acc: 0.957657\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114912, acc: 0.957659\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114906, acc: 0.957661\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114901, acc: 0.957663\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957665\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957667\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957669\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957671\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957673\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957675\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957677\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114858, acc: 0.957679\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957681\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957683\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114842, acc: 0.957685\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114837, acc: 0.957687\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957689\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114826, acc: 0.957691\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957693\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957695\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114810, acc: 0.957697\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114805, acc: 0.957698\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114800, acc: 0.957700\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114794, acc: 0.957702\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114789, acc: 0.957704\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114784, acc: 0.957706\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957708\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957710\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957712\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957714\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957716\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957718\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957697\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957699\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957701\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114777, acc: 0.957703\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957705\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957707\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957709\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957689\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957691\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957693\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.957690\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114868, acc: 0.957687\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114909, acc: 0.957684\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114904, acc: 0.957686\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114898, acc: 0.957688\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114893, acc: 0.957690\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114888, acc: 0.957692\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114882, acc: 0.957694\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114877, acc: 0.957696\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114872, acc: 0.957698\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114866, acc: 0.957700\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114861, acc: 0.957702\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114856, acc: 0.957704\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114850, acc: 0.957706\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114845, acc: 0.957708\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114840, acc: 0.957710\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114835, acc: 0.957712\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114829, acc: 0.957714\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957716\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957718\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957720\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957723\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957725\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957727\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957729\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957731\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114776, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957735\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957737\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957739\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957741\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957743\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957745\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957747\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957749\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957751\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957753\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957755\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957753\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957755\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957757\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957759\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957761\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957763\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957765\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957767\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114752, acc: 0.957766\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957768\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957774\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957776\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957778\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957780\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957782\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957788\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957790\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957794\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957796\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957798\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957800\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957802\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957803\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957807\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957809\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957811\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957789\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957791\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957795\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957799\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957803\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957807\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957783\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957785\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957787\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957788\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114760, acc: 0.957769\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114755, acc: 0.957771\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957773\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957775\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957777\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957779\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957781\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957783\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957785\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957787\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957789\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957793\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957795\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957797\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957799\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957800\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957802\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957794\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957796\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957798\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114771, acc: 0.957754\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114766, acc: 0.957756\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114761, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957762\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957764\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957765\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957767\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957769\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957771\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957775\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957777\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957779\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957781\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957785\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957787\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957789\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957791\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957793\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957795\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957785\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957787\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957785\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957787\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957791\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114756, acc: 0.957786\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114751, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114746, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957794\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957800\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957802\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957804\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957806\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957808\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957810\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957814\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957817\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957815\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957817\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957819\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957821\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957822\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957828\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957830\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957834\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957836\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957815\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957814\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957816\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957818\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957820\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957822\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957803\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957805\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957807\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957811\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114765, acc: 0.957808\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114754, acc: 0.957811\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114749, acc: 0.957813\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957817\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114779, acc: 0.957814\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114820, acc: 0.957811\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114860, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114855, acc: 0.957810\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114896, acc: 0.957789\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114890, acc: 0.957791\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114885, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114880, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114874, acc: 0.957797\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114869, acc: 0.957799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114864, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114859, acc: 0.957803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114853, acc: 0.957805\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957807\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957809\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114838, acc: 0.957810\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114832, acc: 0.957812\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114827, acc: 0.957814\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114822, acc: 0.957816\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114817, acc: 0.957818\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957822\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114795, acc: 0.957826\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114790, acc: 0.957828\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114785, acc: 0.957830\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114780, acc: 0.957832\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114774, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114769, acc: 0.957836\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114764, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114759, acc: 0.957839\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957843\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957845\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957847\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957849\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957851\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957853\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957855\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957859\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957861\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957863\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957865\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957867\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957868\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957870\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957872\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957874\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957876\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957878\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957880\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957882\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957884\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957886\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957888\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957890\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957892\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957895\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957899\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957901\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957905\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957907\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957909\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957911\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957913\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957917\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957919\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957921\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957900\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957902\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957903\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957905\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957907\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957886\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957890\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957892\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957894\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957896\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957898\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957900\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957902\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957904\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957906\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957900\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957902\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957904\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957906\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957908\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957911\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957913\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957917\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957915\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957916\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957920\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957922\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957924\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957926\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957928\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957930\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957932\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957934\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957936\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957938\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957940\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957941\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957943\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957945\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957949\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957951\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957953\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957955\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957957\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957959\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957961\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957940\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957944\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957946\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957947\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957949\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114498, acc: 0.957951\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957952\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957948\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957950\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957952\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957910\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957912\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957914\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957916\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957918\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957920\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957921\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114516, acc: 0.957925\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114511, acc: 0.957927\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957929\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957931\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957930\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957932\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957934\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957936\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957938\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957942\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957944\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957946\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957948\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957949\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957951\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957955\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957959\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957961\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957963\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957965\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957967\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957969\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957971\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957972\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957974\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957976\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957978\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957980\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957982\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957961\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957963\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957965\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957967\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957969\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957971\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957973\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957975\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957977\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957956\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957958\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957959\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957961\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957963\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957965\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957967\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957971\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957973\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957975\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957977\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957979\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957981\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957984\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957986\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957988\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957990\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957992\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957994\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957995\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957996\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957998\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.958000\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957977\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957979\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957981\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957937\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957939\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957941\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957943\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957945\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957947\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957949\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957951\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957952\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957954\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957956\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957958\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957960\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957955\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957957\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957959\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957957\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957936\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957938\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957940\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957942\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957943\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114476, acc: 0.957945\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957947\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957949\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957951\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957953\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957955\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957957\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957936\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957938\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957940\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957942\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957944\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957946\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957948\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957949\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957951\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957953\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957955\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957912\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957914\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957916\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957917\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957919\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114448, acc: 0.957921\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114443, acc: 0.957923\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957925\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957927\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957929\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957931\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114462, acc: 0.957921\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114457, acc: 0.957923\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957925\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957927\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957929\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957931\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957933\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957939\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957942\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957944\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957946\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957948\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957950\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957952\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957956\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957958\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957959\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957961\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957963\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957965\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957967\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957969\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.957971\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957968\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957970\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957972\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957951\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957953\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957940\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957941\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957943\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957943\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957946\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957948\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957950\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957952\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957954\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957956\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957958\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957960\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957962\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957964\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957920\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957922\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957924\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957926\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957928\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957930\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957932\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957934\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957935\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957937\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957894\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957896\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957898\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957895\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114452, acc: 0.957899\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957901\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957903\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957905\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957907\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957909\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957910\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957912\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957914\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957916\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957918\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957920\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957922\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957924\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957926\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957928\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957929\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957931\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957930\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957931\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957933\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957930\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957932\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957911\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957913\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957910\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957912\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957914\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957916\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957918\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957920\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957899\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957903\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957905\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957907\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957909\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957912\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114471, acc: 0.957914\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114466, acc: 0.957916\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957918\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957920\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957877\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114491, acc: 0.957879\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114486, acc: 0.957881\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957871\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957873\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957875\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957877\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957879\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957883\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957884\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957871\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957873\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957877\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957858\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957860\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957862\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957864\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957866\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957856\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957858\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957860\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957862\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957864\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957866\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957868\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957870\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957872\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957869\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957848\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957845\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957847\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957849\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957846\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957848\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957850\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957852\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957854\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957851\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957853\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957855\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957857\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957858\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957860\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957866\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957868\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957870\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957872\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957874\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957871\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957873\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957860\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957862\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957866\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957867\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957869\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957871\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957873\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957875\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957877\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957879\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957881\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957883\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957885\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957886\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957890\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957892\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957896\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957898\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957900\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957879\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957836\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957833\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957835\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957837\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957839\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957841\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957840\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957844\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957846\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957848\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957850\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957852\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957849\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957850\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957852\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957854\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957856\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957858\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957860\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957864\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957867\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957869\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957871\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957873\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957875\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957877\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957879\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957881\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957860\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957862\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957819\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957821\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957823\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957825\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957827\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957830\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957832\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957838\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957840\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957836\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957838\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957840\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957844\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957803\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957805\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957806\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957810\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957812\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957814\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957816\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957818\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957820\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957823\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957825\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957827\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957831\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957828\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957825\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957831\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957833\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957835\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957834\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957835\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957837\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957839\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957841\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957843\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957845\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957847\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957849\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957851\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957852\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957854\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957836\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957838\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957840\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957841\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957839\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957843\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957844\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957846\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957848\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957850\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957852\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957854\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957856\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957858\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957860\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957861\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957863\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957865\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957867\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957869\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957871\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957875\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957877\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957880\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957882\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957880\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957881\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957883\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957885\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957887\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957889\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957893\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957895\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957898\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957900\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957902\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957904\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957906\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957908\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957910\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957912\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957913\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957915\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957917\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957915\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957917\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957918\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957920\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957922\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957924\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957926\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957930\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957932\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957929\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957933\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957935\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957937\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957938\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957940\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114550, acc: 0.957942\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957939\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957941\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957943\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957947\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957948\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957950\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957952\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957954\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957941\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957943\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957945\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957947\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957949\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957950\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957952\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957954\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957945\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957947\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957949\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957951\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957952\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957954\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957956\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957958\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957960\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957962\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957964\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114535, acc: 0.957966\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957923\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957925\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957927\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957928\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114555, acc: 0.957930\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957932\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957934\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957936\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957938\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957940\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957942\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114519, acc: 0.957943\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114514, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114509, acc: 0.957947\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957905\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957906\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957908\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957910\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114528, acc: 0.957912\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957894\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957895\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957899\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957879\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957881\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957882\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957886\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957888\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957845\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957803\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957805\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957801\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957803\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957805\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957807\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957808\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957810\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957814\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957816\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957818\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957820\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957822\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957824\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957825\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957833\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957835\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957832\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957834\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957835\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957837\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957839\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957841\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957837\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957839\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957841\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957843\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957845\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957847\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957849\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957851\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957852\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957854\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957856\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957858\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957855\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957857\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957859\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957861\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957863\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957865\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957867\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957869\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957870\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957872\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957874\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957876\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114557, acc: 0.957878\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114552, acc: 0.957880\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114547, acc: 0.957882\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114542, acc: 0.957884\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957885\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957887\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957889\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114522, acc: 0.957891\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114517, acc: 0.957893\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957874\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114546, acc: 0.957876\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114541, acc: 0.957878\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114536, acc: 0.957880\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114531, acc: 0.957882\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114526, acc: 0.957884\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957886\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957888\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957889\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957893\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957895\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957897\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957894\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114524, acc: 0.957896\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957893\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957894\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957896\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957898\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957893\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957894\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957896\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957898\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114606, acc: 0.957900\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114601, acc: 0.957902\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114596, acc: 0.957904\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114591, acc: 0.957906\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114586, acc: 0.957908\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957909\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114576, acc: 0.957911\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114571, acc: 0.957913\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114566, acc: 0.957915\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114561, acc: 0.957917\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114556, acc: 0.957919\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114551, acc: 0.957921\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957920\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957922\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957924\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957881\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957883\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957863\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957865\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957866\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957868\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957870\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957783\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957785\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957787\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957789\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957793\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957750\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957752\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957754\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957756\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957758\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957762\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957721\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957723\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957725\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957727\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957728\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957730\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957732\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957734\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957736\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957738\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957740\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957697\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957699\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957701\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957703\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957705\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957707\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957709\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957710\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957712\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957670\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957672\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957674\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957676\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957677\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957679\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957659\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957661\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957663\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957665\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957667\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957668\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957670\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957672\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957674\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957676\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957678\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957680\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957681\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957683\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957685\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957687\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957689\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957691\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957693\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957695\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957696\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957698\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957700\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957702\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957704\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957706\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957708\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957710\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957713\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957715\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114572, acc: 0.957717\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957714\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957712\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957714\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957716\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957718\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957697\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957699\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957701\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957703\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957705\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957707\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957709\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957710\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957707\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957709\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957711\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957713\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957715\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957717\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957719\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957720\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957722\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957724\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957726\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957728\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114617, acc: 0.957730\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957727\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957729\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957730\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957732\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957734\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957736\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114670, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957737\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957739\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957740\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957742\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957722\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957724\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957726\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957728\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957730\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957731\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957735\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957737\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957739\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957741\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957743\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957744\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957746\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957750\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957752\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957754\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957759\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957761\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957763\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114612, acc: 0.957743\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957745\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957747\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957750\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957752\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114581, acc: 0.957754\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957753\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957750\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957754\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957757\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957759\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957761\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957763\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957765\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957767\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957768\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957770\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957772\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957774\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957776\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957763\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957765\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957767\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957769\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957770\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957752\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957754\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957756\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957758\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957760\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114650, acc: 0.957755\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114645, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114640, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114635, acc: 0.957760\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114630, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957766\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957768\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957766\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957768\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957769\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957751\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957753\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957757\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957759\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957762\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957760\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957762\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957763\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957765\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957767\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957769\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957768\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957770\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957772\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957768\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957771\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957775\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957773\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957774\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957776\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957778\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957780\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957786\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114741, acc: 0.957783\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114736, acc: 0.957785\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957787\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957789\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957794\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957778\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957780\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957781\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957783\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957785\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957787\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114709, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114704, acc: 0.957791\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114699, acc: 0.957793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114694, acc: 0.957794\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957796\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957754\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957756\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957758\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957760\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114747, acc: 0.957759\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114742, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114737, acc: 0.957762\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114732, acc: 0.957764\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114727, acc: 0.957766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114722, acc: 0.957768\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957770\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957772\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957775\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957777\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957779\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957781\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957783\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957785\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957786\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957768\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957766\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957767\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957758\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957762\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957766\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957768\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957771\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957773\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957775\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957777\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957779\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114717, acc: 0.957781\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114712, acc: 0.957782\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114707, acc: 0.957784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114702, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957788\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957790\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957794\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957791\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957794\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957796\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957793\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957795\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957797\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114802, acc: 0.957796\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114797, acc: 0.957798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114792, acc: 0.957800\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114787, acc: 0.957801\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114782, acc: 0.957803\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114821, acc: 0.957801\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114816, acc: 0.957803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114811, acc: 0.957804\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114806, acc: 0.957806\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114801, acc: 0.957808\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957810\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957812\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957814\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114781, acc: 0.957815\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957813\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957815\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957817\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114804, acc: 0.957818\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957815\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114838, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957819\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957820\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957822\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957826\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957828\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957830\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957833\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957835\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957837\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957839\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957841\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957843\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957844\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957850\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957852\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957854\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957855\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957857\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957859\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957861\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957863\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957865\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957866\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957868\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957870\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114731, acc: 0.957828\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114726, acc: 0.957830\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957832\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957834\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957836\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114750, acc: 0.957816\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114745, acc: 0.957818\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114740, acc: 0.957819\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114735, acc: 0.957821\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114730, acc: 0.957823\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114725, acc: 0.957825\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114720, acc: 0.957827\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957829\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957830\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957828\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957830\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957832\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957833\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114772, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114767, acc: 0.957833\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114762, acc: 0.957835\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114757, acc: 0.957836\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114796, acc: 0.957795\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114791, acc: 0.957796\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114786, acc: 0.957798\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114824, acc: 0.957789\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114819, acc: 0.957791\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114814, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114809, acc: 0.957795\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114848, acc: 0.957792\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114843, acc: 0.957794\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114838, acc: 0.957796\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114833, acc: 0.957798\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114828, acc: 0.957800\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114823, acc: 0.957801\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114818, acc: 0.957803\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114813, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114808, acc: 0.957807\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114803, acc: 0.957809\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114798, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114793, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114788, acc: 0.957814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114783, acc: 0.957816\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114778, acc: 0.957818\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114773, acc: 0.957820\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114768, acc: 0.957822\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114763, acc: 0.957824\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114758, acc: 0.957825\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957827\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957829\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957833\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957835\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957838\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957840\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957844\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957846\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957847\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957849\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957851\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957853\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114721, acc: 0.957811\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114716, acc: 0.957813\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114711, acc: 0.957815\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957817\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114701, acc: 0.957819\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114696, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114691, acc: 0.957822\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957824\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957826\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957828\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114715, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114710, acc: 0.957810\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114705, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114700, acc: 0.957813\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114695, acc: 0.957815\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114690, acc: 0.957817\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114685, acc: 0.957819\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114680, acc: 0.957821\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114675, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957819\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957821\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957823\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957825\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957830\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957834\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957836\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957838\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957840\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114697, acc: 0.957827\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114692, acc: 0.957829\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114687, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114682, acc: 0.957832\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957836\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957838\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114662, acc: 0.957840\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114657, acc: 0.957842\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114652, acc: 0.957843\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114647, acc: 0.957845\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114686, acc: 0.957825\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114681, acc: 0.957827\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114676, acc: 0.957829\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114671, acc: 0.957831\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114666, acc: 0.957833\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114661, acc: 0.957835\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114656, acc: 0.957836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957838\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114646, acc: 0.957840\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114641, acc: 0.957842\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114636, acc: 0.957844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114631, acc: 0.957846\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114626, acc: 0.957847\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114621, acc: 0.957849\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114616, acc: 0.957851\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114611, acc: 0.957853\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957846\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957848\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957849\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957851\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114625, acc: 0.957853\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114620, acc: 0.957855\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957857\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957859\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957860\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957819\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957821\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957822\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957824\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957826\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957828\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957830\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957832\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957833\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957835\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957837\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957839\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957841\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957843\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957844\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114607, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114602, acc: 0.957806\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114597, acc: 0.957808\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114592, acc: 0.957810\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114587, acc: 0.957812\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114582, acc: 0.957814\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114577, acc: 0.957816\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114615, acc: 0.957803\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114610, acc: 0.957805\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957785\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957787\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957789\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114677, acc: 0.957747\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114672, acc: 0.957749\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114667, acc: 0.957751\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114706, acc: 0.957748\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114744, acc: 0.957744\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114739, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114734, acc: 0.957748\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114729, acc: 0.957749\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114724, acc: 0.957751\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114719, acc: 0.957753\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114714, acc: 0.957755\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114753, acc: 0.957735\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114748, acc: 0.957737\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114743, acc: 0.957739\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114738, acc: 0.957741\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114733, acc: 0.957742\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114728, acc: 0.957744\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114723, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114718, acc: 0.957748\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114713, acc: 0.957750\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114708, acc: 0.957752\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114703, acc: 0.957753\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114698, acc: 0.957755\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957759\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957761\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957763\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957764\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957766\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957768\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957770\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114653, acc: 0.957772\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114648, acc: 0.957773\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114643, acc: 0.957775\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114638, acc: 0.957777\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114633, acc: 0.957779\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114628, acc: 0.957781\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114623, acc: 0.957783\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957784\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957786\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957788\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957770\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957772\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957774\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957776\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114665, acc: 0.957773\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957774\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957776\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114651, acc: 0.957778\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114689, acc: 0.957776\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114684, acc: 0.957777\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114679, acc: 0.957779\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114674, acc: 0.957781\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114669, acc: 0.957783\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114664, acc: 0.957785\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114659, acc: 0.957787\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957788\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957790\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957792\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957794\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957796\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957801\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957803\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114605, acc: 0.957807\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957808\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957810\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957812\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957816\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957818\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957819\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114565, acc: 0.957821\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114560, acc: 0.957823\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957803\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957805\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957807\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957809\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957767\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114660, acc: 0.957748\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114655, acc: 0.957749\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114693, acc: 0.957745\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114688, acc: 0.957747\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114683, acc: 0.957749\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114678, acc: 0.957751\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114673, acc: 0.957752\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114668, acc: 0.957754\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114663, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114658, acc: 0.957758\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114654, acc: 0.957760\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114649, acc: 0.957761\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114644, acc: 0.957763\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114639, acc: 0.957765\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114634, acc: 0.957767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114629, acc: 0.957769\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114624, acc: 0.957771\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114619, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114614, acc: 0.957774\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114609, acc: 0.957776\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957778\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114642, acc: 0.957765\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114637, acc: 0.957767\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114632, acc: 0.957769\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114627, acc: 0.957771\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114622, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114618, acc: 0.957774\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114613, acc: 0.957776\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957778\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114603, acc: 0.957780\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114598, acc: 0.957782\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114593, acc: 0.957784\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114588, acc: 0.957785\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114583, acc: 0.957787\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114578, acc: 0.957789\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114573, acc: 0.957791\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114568, acc: 0.957793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114563, acc: 0.957794\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114558, acc: 0.957796\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114553, acc: 0.957798\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114548, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114543, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114538, acc: 0.957804\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114533, acc: 0.957805\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957807\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114567, acc: 0.957805\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114562, acc: 0.957807\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114600, acc: 0.957787\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114595, acc: 0.957789\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114590, acc: 0.957790\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114585, acc: 0.957792\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114580, acc: 0.957794\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114575, acc: 0.957796\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114570, acc: 0.957798\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114608, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114604, acc: 0.957758\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114599, acc: 0.957760\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114594, acc: 0.957762\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114589, acc: 0.957764\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114584, acc: 0.957766\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114579, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114574, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114569, acc: 0.957771\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114564, acc: 0.957773\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114559, acc: 0.957775\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114554, acc: 0.957776\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114549, acc: 0.957778\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114544, acc: 0.957780\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114539, acc: 0.957782\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114534, acc: 0.957784\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114529, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114525, acc: 0.957787\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114520, acc: 0.957789\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114515, acc: 0.957791\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957793\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957795\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957796\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957798\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957800\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957806\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957807\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957809\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957811\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957813\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957815\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957802\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957804\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957806\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957808\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957811\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957813\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957815\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957818\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957820\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957824\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957826\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957828\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957835\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957837\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957819\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957821\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957822\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957824\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957826\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114447, acc: 0.957823\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957825\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957827\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957829\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957831\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957833\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957830\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957832\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957834\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957835\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957837\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957839\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957841\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957843\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957845\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957846\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957848\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957850\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957852\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957811\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957812\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957814\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957816\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957818\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957820\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957822\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957823\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957825\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957821\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957823\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957824\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957826\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957828\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957830\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957832\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957833\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957835\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957837\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957839\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957843\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957844\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957848\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957850\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957852\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957853\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957855\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957857\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957859\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957861\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957862\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957866\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957868\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957870\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957871\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957873\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957875\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957877\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957879\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957881\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957882\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957884\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.957886\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957888\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957890\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957891\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957893\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957895\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957897\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957899\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957900\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957902\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957904\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957908\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957909\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957911\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957913\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.957915\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957917\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957918\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957920\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.957922\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957924\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957904\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957906\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957903\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957905\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957907\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957908\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957910\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957912\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957914\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957905\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957907\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957909\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957912\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957914\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957916\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957919\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957921\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957921\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957922\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957924\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957926\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957928\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957908\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957910\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957912\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957914\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957915\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957917\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957919\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957921\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957923\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957924\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957926\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957928\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.957930\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.957932\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957912\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957871\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957830\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957832\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957834\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957835\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957837\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.957839\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957841\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957844\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114284, acc: 0.957842\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957843\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957845\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957847\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957849\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957851\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957852\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957854\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957856\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957836\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957834\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957836\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957837\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957839\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957837\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957817\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957819\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957821\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957823\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957824\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957826\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957828\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957830\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957832\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957833\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957835\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957837\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957839\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957819\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957821\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957823\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957825\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957805\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957807\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957809\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957811\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957812\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957814\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957816\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957775\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957777\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957779\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957781\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957782\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957784\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957786\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957788\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957790\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957787\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957788\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957790\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957792\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957790\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957791\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957800\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957802\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957804\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957806\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957803\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957805\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957806\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957808\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114432, acc: 0.957805\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114470, acc: 0.957764\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114465, acc: 0.957766\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114460, acc: 0.957768\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114455, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957772\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957773\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957775\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957777\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957779\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957781\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957782\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957780\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957782\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957784\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957785\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957783\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957785\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114510, acc: 0.957765\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114505, acc: 0.957767\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957769\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114495, acc: 0.957771\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114490, acc: 0.957772\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114485, acc: 0.957774\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957772\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957773\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957775\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957777\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957779\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957781\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957782\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957784\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957786\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114479, acc: 0.957788\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957791\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957793\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957752\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957754\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957756\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957758\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957760\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957761\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957763\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957765\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114458, acc: 0.957769\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114453, acc: 0.957770\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114449, acc: 0.957772\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957774\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114439, acc: 0.957776\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957778\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957779\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957781\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957783\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957785\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957787\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957788\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957786\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114438, acc: 0.957788\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957790\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957791\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114413, acc: 0.957797\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114408, acc: 0.957799\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957800\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957802\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114436, acc: 0.957783\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114431, acc: 0.957784\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114427, acc: 0.957786\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957788\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957790\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957792\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957793\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957795\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957797\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957799\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957801\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957788\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957790\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957792\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957794\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957795\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957801\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957803\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957804\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957806\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957807\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957809\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957811\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957813\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957814\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957816\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957818\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957820\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957822\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957823\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957825\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957827\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957829\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957832\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957834\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957795\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957797\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957799\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957801\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957802\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957804\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957808\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957810\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957811\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957813\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957815\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957806\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957808\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957810\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957812\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957813\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957815\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957817\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957819\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957816\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957818\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957822\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957823\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957825\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957827\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957829\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957831\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957832\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957834\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957836\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957838\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957839\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957841\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957845\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957847\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957850\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957847\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957848\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957846\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957847\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957849\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957851\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957848\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957850\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957852\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957854\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957856\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957857\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957859\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957861\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957863\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957865\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957866\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957868\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957870\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957872\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957873\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957875\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957877\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957879\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957881\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957882\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957884\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957882\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957883\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957885\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957887\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957889\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957891\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957892\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957894\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.957896\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957893\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957895\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957897\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957899\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957879\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957881\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957883\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957880\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957881\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957883\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957885\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957887\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957888\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957890\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957892\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957894\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957896\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957899\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957901\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957903\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957904\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957906\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957908\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957910\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957912\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957913\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957915\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957917\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957919\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957920\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957922\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957920\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957922\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957923\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957925\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957927\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957886\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957888\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957890\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957892\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.957894\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.957895\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957899\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957901\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957902\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957904\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957906\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957908\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957910\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957911\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957913\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957915\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957917\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957918\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957920\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.957922\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957924\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957918\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957920\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957922\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957919\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957920\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957922\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957924\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957926\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957927\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.957925\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957927\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957929\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957888\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957890\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957892\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957893\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957895\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.957897\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.957899\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.957901\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957902\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957904\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957906\n",
      "target: tensor([1.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957865\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957867\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957869\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957871\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957873\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957874\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957876\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957878\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957869\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957871\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957873\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957874\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957876\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957878\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957880\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957882\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957883\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.957843\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957845\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957847\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957848\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957850\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957852\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957854\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957855\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957857\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957859\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957847\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957848\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957829\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957831\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957833\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957835\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957836\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957838\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957840\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957842\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957843\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957845\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957847\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957845\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957846\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957848\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957850\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957852\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957853\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957855\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957857\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957859\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957860\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957862\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957864\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957866\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957868\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957869\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957871\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957868\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957870\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957872\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957869\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957829\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957831\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957833\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957834\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957836\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957838\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957840\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957841\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957843\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957845\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957847\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957849\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957850\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957810\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957812\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957793\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957794\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957792\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957794\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957795\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957797\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957799\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957801\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957803\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957800\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957802\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957804\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957805\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957807\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957813\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957793\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957795\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957799\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957800\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957802\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957804\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957806\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957808\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957809\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957811\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957813\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957815\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957816\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957797\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957799\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957801\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957802\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957804\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957806\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957808\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957810\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957811\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957813\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957815\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957817\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957818\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957820\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957822\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957824\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957805\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957806\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957804\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957806\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957807\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957788\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114461, acc: 0.957748\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114456, acc: 0.957750\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114451, acc: 0.957752\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114446, acc: 0.957753\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114441, acc: 0.957755\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957757\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114474, acc: 0.957738\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957739\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114506, acc: 0.957737\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114501, acc: 0.957739\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114496, acc: 0.957741\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114492, acc: 0.957742\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114487, acc: 0.957744\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114482, acc: 0.957746\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114477, acc: 0.957748\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114472, acc: 0.957749\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114468, acc: 0.957751\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114463, acc: 0.957753\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114500, acc: 0.957713\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114537, acc: 0.957710\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114532, acc: 0.957712\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114527, acc: 0.957714\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114523, acc: 0.957715\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114518, acc: 0.957717\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114513, acc: 0.957719\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114508, acc: 0.957721\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114503, acc: 0.957722\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114499, acc: 0.957724\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114494, acc: 0.957726\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114489, acc: 0.957728\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114484, acc: 0.957729\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114480, acc: 0.957731\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114475, acc: 0.957733\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114512, acc: 0.957730\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114507, acc: 0.957731\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114502, acc: 0.957733\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114497, acc: 0.957735\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114493, acc: 0.957737\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114488, acc: 0.957738\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114483, acc: 0.957740\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114478, acc: 0.957742\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114473, acc: 0.957744\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114469, acc: 0.957745\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114464, acc: 0.957747\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114459, acc: 0.957749\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114454, acc: 0.957751\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114450, acc: 0.957753\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114445, acc: 0.957754\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957756\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957760\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114426, acc: 0.957761\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957763\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957765\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957767\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957768\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957770\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957774\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957776\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114383, acc: 0.957777\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957765\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114415, acc: 0.957767\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957769\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957770\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114442, acc: 0.957751\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114437, acc: 0.957753\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114433, acc: 0.957755\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957757\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114423, acc: 0.957758\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957760\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957762\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114409, acc: 0.957764\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114404, acc: 0.957765\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957767\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957769\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957771\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957772\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957776\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957757\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957759\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114444, acc: 0.957746\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114440, acc: 0.957748\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114435, acc: 0.957750\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114430, acc: 0.957752\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957754\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114421, acc: 0.957755\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957757\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957759\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957761\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957762\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957764\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114392, acc: 0.957766\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957768\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957769\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957771\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957773\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957775\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957772\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114400, acc: 0.957774\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114395, acc: 0.957776\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957778\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957779\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957781\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957783\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957785\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957786\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957788\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957790\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957792\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957793\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957795\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957797\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957799\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957800\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957802\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957804\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957806\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957807\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957809\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957811\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957813\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957815\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957816\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957818\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957820\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957822\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957823\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957825\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957827\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957829\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957830\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957832\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957829\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957830\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957832\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.957834\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.957836\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.957837\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957839\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957841\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957843\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957844\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957846\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957848\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957850\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957852\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957853\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957850\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957852\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957854\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957856\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957857\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957859\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957861\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957863\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957864\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957866\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957868\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957870\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957871\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957873\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.957875\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957877\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957878\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957880\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.957882\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.957884\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.957885\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.957887\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.957889\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.957891\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114136, acc: 0.957893\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.957894\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.957896\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.957898\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957895\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.957897\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.957899\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.957901\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.957902\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957904\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.957906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.957908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114120, acc: 0.957909\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.957911\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.957913\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.957915\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.957916\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.957918\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.957920\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.957922\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.957923\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.957925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.957927\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957929\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957930\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957932\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.957934\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957936\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.957937\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.957939\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.957941\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.957944\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.957946\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.957927\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.957929\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.957931\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.957932\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957934\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.957936\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.957938\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.957939\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.957941\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957943\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114010, acc: 0.957945\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.957946\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.957948\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.957950\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.957947\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.957949\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.957951\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.957953\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.957954\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.957956\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.957958\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957960\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957957\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957958\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957960\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957962\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.957964\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.957965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.957967\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957969\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957972\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.957953\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.957955\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.957957\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.957959\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.957960\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.957962\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957964\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957967\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.957969\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.957973\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.957974\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957976\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.957978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.957980\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.957981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.957983\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.957985\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.957987\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.957990\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957971\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.957973\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.957970\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.957968\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.957970\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.957971\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.957932\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957934\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957935\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.957937\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.957939\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.957940\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.957937\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.957939\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.957941\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.957944\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.957946\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.957948\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.957949\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957951\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.957953\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.957954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.957956\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.957958\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.957960\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.957961\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957965\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113997, acc: 0.957967\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.957968\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.957970\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957972\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.957974\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.957975\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.957977\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.957972\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.957974\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.957975\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.957977\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.957979\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.957981\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.957982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.957984\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.957986\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.957988\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.957989\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.957991\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.957993\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957995\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.957996\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.957998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958000\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.958002\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.957999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.958000\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958002\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958004\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958006\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958007\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.958009\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.958011\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958013\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958016\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.958018\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.958020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.958021\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.958023\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.958025\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958006\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.958008\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.958009\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.958011\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958013\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958014\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.958012\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958013\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.958015\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958017\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113922, acc: 0.958019\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958020\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958022\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.958003\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958005\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958010\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958015\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.958019\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958020\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.958022\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958024\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.958026\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958027\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958029\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958031\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.958033\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958034\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.957995\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.957996\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.957978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.957979\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.957981\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.957983\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.957984\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.957986\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.957988\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.957990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.957991\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.957995\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.957997\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.957998\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957979\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.957981\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.957983\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.957985\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.957986\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.957988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.957990\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.957992\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.957993\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.957995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.957997\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.957999\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958000\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958004\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958005\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958007\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958009\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958011\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958018\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958019\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958021\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958023\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958020\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958024\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958026\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958027\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958029\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958026\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958024\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.958026\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958027\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958029\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958031\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.958028\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.957989\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.957991\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.957992\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113922, acc: 0.957994\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.957998\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.957999\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.957995\n",
      "target: tensor([3.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.957976\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.957978\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.957980\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.957981\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.957985\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.957987\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.957988\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.957990\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.957992\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.957994\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.957995\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.957990\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.957992\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.957994\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.957995\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.957997\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.957999\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.958000\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.957997\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.957999\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958001\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.958002\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958003\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958005\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957998\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958003\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.958005\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.958007\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958004\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958006\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958002\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.958004\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.958006\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114086, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958011\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.957992\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.957994\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.957996\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.957992\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.957994\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.957955\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957956\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.957958\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.957960\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.957961\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.957963\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957961\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957963\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957964\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957966\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957968\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.957969\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957952\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957954\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957951\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957953\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957955\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957954\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.957955\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957957\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957959\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957960\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957962\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957964\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957967\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957969\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957971\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957968\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957970\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114268, acc: 0.957951\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957953\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.957954\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.957956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957958\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957960\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957961\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957963\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957965\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957968\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957970\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957972\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957969\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957950\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957952\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957954\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.957956\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957957\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957939\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957940\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957944\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957947\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.957949\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957947\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957948\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957950\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957948\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957949\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957951\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957953\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957955\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957956\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957958\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957960\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957961\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957963\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957967\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957968\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.957970\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957972\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957973\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957975\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957973\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957975\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957972\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957973\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957975\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957977\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957979\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957980\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957982\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957984\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957981\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957983\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957980\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957982\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957984\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957985\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957987\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957989\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957991\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957992\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957994\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957997\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957999\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.958001\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.958003\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.958004\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.958001\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.958003\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.958004\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.958006\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.958008\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.958010\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.958011\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.958013\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.958016\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958018\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.958020\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.958017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.958016\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.958018\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.958019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.958021\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.958023\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.958024\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957985\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957987\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957989\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957990\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957992\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957994\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957996\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.958001\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.958002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.958004\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.958006\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957999\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.958001\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.958002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.958004\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.958006\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.958008\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.958009\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.958011\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.958013\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958015\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.958018\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.958020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.958021\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.958023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.958025\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.958027\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.958028\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.958030\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.958032\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958033\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958035\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958037\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.958034\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.958036\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114255, acc: 0.958038\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.958040\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.958041\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.958043\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.958045\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.958046\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958048\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958050\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.958047\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.958049\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.958050\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.958052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.958054\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.958051\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.958053\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958055\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.958016\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.958019\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958021\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.958018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.958020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.958022\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.958023\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.958025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.958027\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.958028\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.958030\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.958032\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.958034\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.958035\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.958037\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.958040\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.958038\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.958040\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.958036\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.958037\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.958039\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.958041\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.958042\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.958044\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.958046\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.958048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.958049\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.958051\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.958053\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.958054\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.958056\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.958058\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958060\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.958061\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.958063\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.958065\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.958066\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.958068\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.958070\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958072\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.958073\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.958075\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.958077\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958078\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958080\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958082\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958083\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.958085\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.958087\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958089\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958070\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958072\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958073\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.958075\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.958077\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958078\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.958080\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.958082\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.958084\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.958085\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.958087\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.958089\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.958090\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.958092\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.958094\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.958096\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.958097\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.958099\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.958101\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114136, acc: 0.958102\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958104\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.958106\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.958107\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958109\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958111\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958113\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.958114\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.958116\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958118\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958119\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958121\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958123\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958125\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958126\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958128\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958089\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.958091\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958092\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958094\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958096\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958097\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.958099\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958101\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958103\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958104\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958106\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958108\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958109\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.958070\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958072\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.958074\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958062\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.958064\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958065\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.958067\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.958069\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114086, acc: 0.958071\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958072\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958074\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958035\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958037\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958038\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.958040\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958042\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.958044\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958045\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958049\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958050\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958052\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958054\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958055\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958057\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958022\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958025\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958027\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958029\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958030\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958032\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958034\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958035\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958037\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958039\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958040\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958030\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958032\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958034\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958035\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114082, acc: 0.958033\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958035\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958036\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958033\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958035\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.958026\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.958028\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.958030\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.958031\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958033\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958035\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.958037\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.958038\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.958040\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.958042\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958043\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958045\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958047\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958048\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958050\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958052\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958054\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958055\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958057\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958060\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958062\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958044\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.958045\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958047\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958049\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958054\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958055\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958057\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.958059\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.958061\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958022\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958023\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.958025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.958027\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958032\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.958034\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958035\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958032\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958034\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958035\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958037\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958040\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.958042\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958003\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.958001\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.958002\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958004\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.958006\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958007\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958009\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114124, acc: 0.958006\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.958004\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.958006\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114150, acc: 0.958007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.958009\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.958011\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.958012\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.958014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.958011\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.958012\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114154, acc: 0.958014\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.958016\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.958018\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114140, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.958021\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958023\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.958024\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.958006\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.958009\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.958011\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114144, acc: 0.958013\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957994\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957996\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957993\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957995\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957997\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957999\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958000\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.958004\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.958005\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.958007\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.958010\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.958012\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.958014\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.958016\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.958017\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957979\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.957980\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.957982\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.957984\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957965\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957967\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.957969\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957970\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957972\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957974\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957975\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957977\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114167, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957976\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957980\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957981\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957963\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957968\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957970\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957971\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957973\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957975\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957977\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957974\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957978\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957979\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957941\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957942\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957944\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957946\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957947\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.957949\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957951\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957952\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957954\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.957956\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957937\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957935\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957936\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957938\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957940\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957941\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957946\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957948\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957950\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957952\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957953\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957955\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957957\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957954\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114287, acc: 0.957951\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957953\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957954\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957916\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957917\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957919\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957921\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957902\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957904\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957906\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957907\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114317, acc: 0.957909\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114312, acc: 0.957911\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114308, acc: 0.957913\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957896\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957898\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957899\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957901\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957903\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957904\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957906\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957908\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957909\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957911\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957913\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957914\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957916\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957914\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957916\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957913\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957915\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957917\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957918\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957920\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.957922\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957923\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957925\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957927\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957908\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957906\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957908\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957909\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957911\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957913\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957914\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957896\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957898\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114428, acc: 0.957895\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114424, acc: 0.957897\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114419, acc: 0.957898\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957900\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957902\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114405, acc: 0.957903\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114401, acc: 0.957905\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957907\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957908\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957910\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957912\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957914\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957915\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957917\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957919\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957920\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957922\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957904\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957905\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957907\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114417, acc: 0.957904\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114412, acc: 0.957906\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114407, acc: 0.957908\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957909\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957911\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957913\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114389, acc: 0.957914\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957916\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957918\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957919\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957921\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957923\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957925\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957926\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957928\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957930\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957931\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957933\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957935\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957936\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957938\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957926\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114396, acc: 0.957918\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957920\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114387, acc: 0.957921\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957923\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957925\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957926\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957928\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957930\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114359, acc: 0.957932\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957933\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957935\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957937\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957938\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957940\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.957943\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114322, acc: 0.957945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114318, acc: 0.957947\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114313, acc: 0.957948\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114309, acc: 0.957950\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114304, acc: 0.957952\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957953\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957955\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957957\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.957959\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957960\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.957962\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957964\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957965\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957967\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957969\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957950\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957952\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957954\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.957955\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957957\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957959\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114266, acc: 0.957960\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.957962\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957964\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957965\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957967\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957969\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957971\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957972\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957974\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957976\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957977\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957979\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957981\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957982\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957984\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957981\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957983\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957985\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957986\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957988\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957990\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957991\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957993\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957995\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957996\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957998\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.958000\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.958001\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.958003\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.958005\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.958006\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.958008\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114160, acc: 0.958010\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.958012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.958013\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.958005\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.958007\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114172, acc: 0.958010\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.958007\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957989\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957990\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.957992\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957994\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957996\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957997\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957999\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958001\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958002\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958004\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.958006\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957987\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957991\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957992\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957994\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.957976\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.957977\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957979\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957981\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957982\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957984\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957986\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.957987\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957989\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.957991\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957992\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957994\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.957996\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957998\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957999\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.958001\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957963\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.957964\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957966\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957968\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957969\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957971\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957973\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957974\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.957976\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.957978\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957977\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957979\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.957967\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957969\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957970\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957972\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.957974\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.957976\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957977\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957981\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957982\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.957984\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957986\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.957987\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957989\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957991\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.957992\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.957990\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.957992\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.957995\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957998\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.958000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.958002\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114168, acc: 0.958003\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114164, acc: 0.958005\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957995\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957977\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.957979\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957980\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957982\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957984\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957985\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957987\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957989\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957990\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957992\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957994\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.957995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114171, acc: 0.957999\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957996\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.957998\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957980\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.957976\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.957977\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.957979\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.957981\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.957982\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957980\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.957982\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957979\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957981\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957963\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957965\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957966\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957958\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957960\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957961\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114403, acc: 0.957943\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114399, acc: 0.957945\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114434, acc: 0.957926\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114429, acc: 0.957928\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114425, acc: 0.957930\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114420, acc: 0.957931\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114416, acc: 0.957933\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114411, acc: 0.957935\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957936\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957938\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114397, acc: 0.957940\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114393, acc: 0.957942\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114388, acc: 0.957943\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114384, acc: 0.957945\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114379, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957948\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957950\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957952\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957953\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957955\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957957\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957958\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957960\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957962\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957965\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957964\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957966\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957968\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114346, acc: 0.957969\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957971\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957973\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957976\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957978\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957979\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957981\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957983\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957984\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114301, acc: 0.957986\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957988\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957989\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114287, acc: 0.957991\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114282, acc: 0.957993\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114278, acc: 0.957994\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957996\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.957998\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.957999\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.958001\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.957998\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.958000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.958001\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.958003\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.958005\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957986\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957988\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114298, acc: 0.957990\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.957991\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.957993\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957975\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957976\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957980\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957981\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957983\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957985\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957986\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957968\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957970\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957972\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114314, acc: 0.957973\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114310, acc: 0.957975\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114305, acc: 0.957977\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957978\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957980\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957982\n",
      "target: tensor([7.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957973\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957975\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957977\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957978\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957980\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957982\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957985\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957987\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114325, acc: 0.957988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114321, acc: 0.957990\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114316, acc: 0.957992\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957974\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957975\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114342, acc: 0.957977\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114338, acc: 0.957979\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957980\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957982\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957984\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114320, acc: 0.957985\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957987\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957989\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114306, acc: 0.957990\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114302, acc: 0.957992\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114297, acc: 0.957994\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114293, acc: 0.957995\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114288, acc: 0.957997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957999\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.958000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.958002\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.958004\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.958005\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.958007\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.958009\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.958010\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.958012\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.958014\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958017\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958019\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958020\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958022\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958024\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958025\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.958027\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.958032\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958034\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.958035\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958024\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.958025\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.958027\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.958029\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.958032\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958034\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.958035\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.958037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.958039\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958001\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.958002\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.958004\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957966\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957968\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.957969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.957973\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.957974\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957976\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.957978\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957979\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957981\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957984\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.957986\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957984\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957985\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.957987\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957989\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957990\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957992\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957994\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.957995\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.957997\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.957999\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114175, acc: 0.958000\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.958002\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.958004\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.958005\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958007\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114152, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.958010\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.958012\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.958014\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958020\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.958022\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114111, acc: 0.958024\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.958025\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958027\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.958029\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958030\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958032\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958034\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958035\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958040\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.958038\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.958039\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.958041\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.958039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114123, acc: 0.958040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.958042\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.958044\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958045\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.958047\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958048\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.958011\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.958014\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.958016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958017\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958021\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.958017\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.958019\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.958017\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.958020\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.958018\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.958019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.958021\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.958019\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.958020\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.958022\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.958024\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.958025\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.958027\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958020\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958022\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.958024\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958025\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958027\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958029\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958030\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958032\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.958014\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958016\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.958017\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958019\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958021\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.958024\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.958026\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958027\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.958029\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.958011\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958012\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958014\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.958016\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958017\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.958016\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.958018\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.958020\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.958021\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.958020\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.958022\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.958024\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.958025\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.958027\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958029\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958030\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958032\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.958033\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958035\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958037\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958038\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.958035\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.958037\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.958038\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.958042\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.958041\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114294, acc: 0.958037\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114289, acc: 0.958038\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114285, acc: 0.958040\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.958042\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.958043\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.958045\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.958048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.958050\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.958052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.958053\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.958055\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.958053\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114274, acc: 0.958054\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114270, acc: 0.958056\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114265, acc: 0.958058\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.958059\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114256, acc: 0.958061\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.958063\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.958064\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.958066\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958068\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.958065\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114269, acc: 0.958067\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114264, acc: 0.958069\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114260, acc: 0.958070\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.958052\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114290, acc: 0.958054\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114286, acc: 0.958055\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.958057\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114272, acc: 0.958060\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114268, acc: 0.958062\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.958064\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114259, acc: 0.958065\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.958067\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.958069\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.958070\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114280, acc: 0.958068\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114276, acc: 0.958070\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.958071\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958073\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114262, acc: 0.958075\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.958076\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.958078\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.958080\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.958081\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.958083\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.958085\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.958086\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.958088\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.958090\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.958091\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958093\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.958094\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.958096\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.958098\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.958099\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.958101\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.958103\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958091\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958093\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.958091\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.958092\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.958094\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.958096\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958097\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.958099\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958100\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958102\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958104\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.958105\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.958107\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958109\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.958110\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958112\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958114\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.958115\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114179, acc: 0.958117\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.958119\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.958120\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.958122\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.958124\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958125\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114152, acc: 0.958127\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114148, acc: 0.958129\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.958130\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.958132\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.958133\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.958135\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.958137\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958138\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.958140\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.958142\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.958143\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.958145\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.958147\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958148\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958150\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958152\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958153\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958155\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958156\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958158\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958160\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958161\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958163\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958165\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958166\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958168\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958170\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958132\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958134\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958135\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958137\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.958099\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.958101\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.958103\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958104\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.958106\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958108\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958109\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958111\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958112\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958114\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958116\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958117\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958119\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958121\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.958122\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958124\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958126\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958127\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958129\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958131\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958132\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958134\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.958135\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.958137\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958134\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958136\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.958134\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958135\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958137\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958139\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.958140\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958142\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958144\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.958145\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.958147\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.958149\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.958150\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958152\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958153\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958155\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.958152\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.958154\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.958156\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.958157\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958159\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958161\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958123\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958125\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.958126\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.958128\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.958130\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958131\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.958133\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958135\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958136\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958138\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958139\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.958141\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.958143\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.958144\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.958146\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.958148\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113971, acc: 0.958149\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.958151\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.958153\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.958154\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.958156\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.958157\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958159\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958161\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.958162\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.958164\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958166\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.958163\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.958165\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958166\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958168\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958170\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958171\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958173\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958174\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958178\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958179\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958181\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958183\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958184\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958186\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958188\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958189\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.958191\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.958192\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958194\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958196\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958199\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958201\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958202\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958199\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.958201\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958202\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958204\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958206\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958207\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958209\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958210\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958212\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958214\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958215\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958217\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958214\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958216\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958218\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958219\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958221\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958222\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958224\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958222\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958225\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.958227\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958228\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958232\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958233\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958235\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958236\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958238\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958240\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958241\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958243\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958245\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958246\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958248\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958250\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958253\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958254\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958256\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958258\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958259\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958261\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958263\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958264\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958266\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958269\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958271\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958272\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958274\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958276\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958277\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958279\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958280\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958282\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958284\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958285\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958283\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958286\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958290\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958291\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958293\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958275\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958277\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958280\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958281\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958283\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958285\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958286\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958288\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958291\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958273\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958275\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958272\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958273\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958269\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958271\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958260\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958261\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958263\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958265\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958266\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958268\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958269\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958271\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958273\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958274\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958276\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958279\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958281\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958282\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958280\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958282\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958283\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958285\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958287\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958288\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958290\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958292\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958293\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958291\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958293\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958275\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958258\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958260\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958262\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958263\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958267\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958268\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958270\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958271\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958273\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958255\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958257\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958220\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.958221\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958223\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958226\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958228\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.958225\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.958227\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.958229\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.958230\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958232\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958234\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958235\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958237\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958238\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958240\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958242\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.958224\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.958225\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958227\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958229\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.958230\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958229\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958230\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958232\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958234\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958235\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.958237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958240\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958242\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958243\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958245\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958247\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958248\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.958246\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958249\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958251\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.958249\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.958250\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958252\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958249\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.958245\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958208\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958210\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958211\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.958213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958215\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.958216\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.958218\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.958219\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958221\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.958223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958224\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.958226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958228\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958229\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958231\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.958232\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.958215\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.958177\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958179\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958181\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958169\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958171\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.958173\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958174\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958172\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958173\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958175\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958176\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958178\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958180\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958181\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958183\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.958185\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958186\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958188\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958189\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958191\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.958193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.958194\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114017, acc: 0.958196\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.958185\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958186\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958188\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958189\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.958191\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958173\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958175\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958177\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958178\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958180\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958177\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958178\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958180\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958181\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958183\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958185\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.958182\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958184\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958185\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958187\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958188\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958190\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958192\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958195\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958197\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958198\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958200\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958201\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958203\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958205\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958206\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958208\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.958210\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.958211\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.958213\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.958214\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958216\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958218\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958219\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958221\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958222\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.958224\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958226\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958222\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958224\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.958226\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.958227\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958229\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.958231\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.958232\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.958234\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.958235\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.958237\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.958240\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.958242\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.958244\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.958245\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.958247\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113942, acc: 0.958248\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113937, acc: 0.958250\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.958252\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958253\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958255\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958256\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958258\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958260\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.958257\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.958259\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958260\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958262\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.958252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.958254\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.958255\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958238\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958241\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.958243\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.958244\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.958246\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.958247\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.958249\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958251\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958252\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958254\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958255\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958257\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958259\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958260\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958262\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958263\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958265\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958268\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958270\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958272\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958273\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958275\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.958276\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958239\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958241\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.958243\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958244\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958246\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958238\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958239\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958241\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958243\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958244\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.958246\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958249\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.958251\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.958254\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.958256\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958257\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958259\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958260\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958262\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958264\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958265\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958267\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958268\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958270\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958272\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958273\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958275\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958276\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958280\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958243\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958244\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958246\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958248\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958249\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958251\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958249\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958250\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958252\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958253\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958255\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958258\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958261\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958263\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958265\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958266\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958268\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958269\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958271\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958273\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958274\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958276\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958258\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958249\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.958231\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113884, acc: 0.958233\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958234\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958232\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958233\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958231\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958233\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958234\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958236\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958238\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958243\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958244\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958246\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958247\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958249\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958252\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958254\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.958255\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.958257\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958259\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958262\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958263\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958265\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958267\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958268\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958270\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958271\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958273\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958275\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958276\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958278\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958280\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958281\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958244\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958246\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958247\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958230\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958231\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958233\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958235\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958236\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958219\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958220\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958217\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958219\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958220\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.958222\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.958225\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.958227\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958228\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.958231\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.958233\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958235\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958236\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958238\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958239\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958243\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958244\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958246\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958248\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958249\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958251\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958252\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958254\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958256\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958257\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958259\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958262\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958264\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958265\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958267\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958268\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958270\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958273\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958275\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958272\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958273\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958275\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958276\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958278\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958280\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958276\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958240\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958241\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958243\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958244\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958246\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958248\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958249\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958251\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958252\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958254\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958256\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958257\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958259\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958262\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958260\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958261\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958263\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958265\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958266\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958268\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958231\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958233\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958234\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958236\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958237\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958239\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958241\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958242\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958244\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958245\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958249\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958250\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958252\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958253\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958255\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958257\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958258\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958260\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958223\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958225\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958226\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958228\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958231\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958233\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958234\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958236\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958238\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958239\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958241\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958242\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958244\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958246\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958234\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958236\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958238\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958241\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958230\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958231\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958233\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958234\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958236\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958238\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958220\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958222\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958210\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958212\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958214\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958215\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958217\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958218\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958216\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958218\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958219\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958221\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958223\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958224\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958207\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958208\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958210\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958211\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958213\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958215\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958216\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958218\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958219\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958221\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958223\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958226\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958227\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958229\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958229\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958231\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958232\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958234\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958235\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958237\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958239\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958240\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958242\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958243\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958245\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958247\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958248\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958250\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958251\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958253\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958255\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958256\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958258\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958259\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958261\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958263\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958264\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958266\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958267\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958269\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958271\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958272\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958274\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958275\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958277\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958279\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958280\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.958282\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958283\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.958285\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.958287\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113635, acc: 0.958288\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113630, acc: 0.958290\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958291\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958293\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958295\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958296\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958298\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.958299\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958301\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958299\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958300\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958283\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958280\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958281\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958283\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958284\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958286\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958288\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958270\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958267\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958269\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958271\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958272\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958274\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958275\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958277\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958279\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958280\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958282\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958283\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958287\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958288\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958290\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958253\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958255\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958256\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958258\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958260\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958261\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113690, acc: 0.958263\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958264\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958266\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958267\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958269\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958271\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958272\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958274\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958275\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958277\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958275\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958276\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958278\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958280\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958281\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958283\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958284\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958286\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113650, acc: 0.958288\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.958289\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958286\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958288\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958291\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.958293\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.958294\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958292\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958294\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958295\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958297\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958299\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958300\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958294\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958296\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958297\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958299\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958297\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958298\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958300\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958301\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958286\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958287\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958289\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958290\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958292\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958294\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958295\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958298\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958300\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.958301\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958303\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958305\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958306\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958308\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958309\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958311\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958314\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958316\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113650, acc: 0.958319\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958317\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958318\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958321\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958323\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958320\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958321\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958323\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958325\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958326\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958328\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958329\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958293\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958295\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958296\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958298\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958299\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958302\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958304\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958306\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958307\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958309\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958307\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958308\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113690, acc: 0.958310\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958311\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958313\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958315\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958316\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958318\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958319\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958321\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958322\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958324\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958315\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958312\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958313\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958277\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958279\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958280\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958282\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958283\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958285\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958287\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958288\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958290\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958291\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958293\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958296\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958294\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958295\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958297\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958299\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958300\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958302\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958300\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958303\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958304\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958306\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958307\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958309\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958312\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958314\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958315\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113690, acc: 0.958317\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958319\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958322\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958323\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958325\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958326\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958328\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958330\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958334\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113638, acc: 0.958336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958338\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958339\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113625, acc: 0.958341\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958344\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958345\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113608, acc: 0.958347\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.958349\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958350\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113633, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958350\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958353\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958354\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958356\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958357\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958359\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958362\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958364\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958365\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958367\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958369\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958370\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958372\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.958373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.958375\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958376\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958359\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958362\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958364\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958365\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958367\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958368\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958370\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958372\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.958373\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958356\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958357\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958354\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958356\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113638, acc: 0.958321\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958323\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958324\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113625, acc: 0.958326\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958327\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113608, acc: 0.958332\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113604, acc: 0.958334\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958335\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.958337\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.958338\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958335\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958337\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958338\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958340\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958342\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958343\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958345\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958346\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958348\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.958350\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958346\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958350\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958351\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958354\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958357\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958359\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958361\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958362\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958364\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958367\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958368\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958370\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958368\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958369\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958367\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958370\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.958372\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958373\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113596, acc: 0.958375\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958377\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113587, acc: 0.958378\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958380\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958381\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958383\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958384\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958386\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958388\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958391\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958392\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113540, acc: 0.958396\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958399\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113527, acc: 0.958400\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113523, acc: 0.958402\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113519, acc: 0.958403\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113514, acc: 0.958405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113510, acc: 0.958407\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113506, acc: 0.958408\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958410\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113497, acc: 0.958411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.958413\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113489, acc: 0.958414\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113484, acc: 0.958416\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.958418\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113476, acc: 0.958419\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.958421\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113467, acc: 0.958422\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113463, acc: 0.958424\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113459, acc: 0.958425\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113454, acc: 0.958427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113450, acc: 0.958429\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113446, acc: 0.958430\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113441, acc: 0.958432\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113437, acc: 0.958433\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113433, acc: 0.958435\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113466, acc: 0.958430\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113462, acc: 0.958432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.958433\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.958397\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113487, acc: 0.958399\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113483, acc: 0.958400\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.958402\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113474, acc: 0.958403\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113470, acc: 0.958405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113466, acc: 0.958406\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113461, acc: 0.958408\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113457, acc: 0.958410\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113453, acc: 0.958411\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.958413\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113444, acc: 0.958414\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113440, acc: 0.958416\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113436, acc: 0.958417\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113431, acc: 0.958419\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113427, acc: 0.958421\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113423, acc: 0.958422\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113418, acc: 0.958424\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113414, acc: 0.958425\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113410, acc: 0.958427\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113406, acc: 0.958428\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113401, acc: 0.958430\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113397, acc: 0.958432\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113393, acc: 0.958433\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113388, acc: 0.958435\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113422, acc: 0.958433\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113418, acc: 0.958434\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113413, acc: 0.958436\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113409, acc: 0.958437\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113405, acc: 0.958439\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113401, acc: 0.958440\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113396, acc: 0.958442\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113392, acc: 0.958443\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113388, acc: 0.958445\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113383, acc: 0.958447\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113379, acc: 0.958448\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113375, acc: 0.958450\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113371, acc: 0.958451\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113366, acc: 0.958453\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113362, acc: 0.958454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113358, acc: 0.958456\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113353, acc: 0.958458\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113349, acc: 0.958459\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113383, acc: 0.958451\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113378, acc: 0.958453\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113374, acc: 0.958454\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113408, acc: 0.958437\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113403, acc: 0.958439\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113399, acc: 0.958440\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113395, acc: 0.958442\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113390, acc: 0.958443\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113424, acc: 0.958432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113420, acc: 0.958434\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113415, acc: 0.958436\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113449, acc: 0.958418\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113444, acc: 0.958420\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113440, acc: 0.958421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113436, acc: 0.958423\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113432, acc: 0.958425\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113427, acc: 0.958426\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113423, acc: 0.958428\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113419, acc: 0.958429\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113414, acc: 0.958431\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113448, acc: 0.958429\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958426\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113477, acc: 0.958428\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113510, acc: 0.958426\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113506, acc: 0.958427\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113502, acc: 0.958429\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113498, acc: 0.958430\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.958432\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113489, acc: 0.958434\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.958435\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958437\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113476, acc: 0.958438\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113472, acc: 0.958440\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113468, acc: 0.958441\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958424\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113497, acc: 0.958426\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.958427\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113488, acc: 0.958429\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113484, acc: 0.958430\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.958432\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113475, acc: 0.958434\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.958435\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.958399\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113500, acc: 0.958401\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.958390\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958391\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958393\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.958394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113517, acc: 0.958396\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113512, acc: 0.958397\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113546, acc: 0.958380\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958382\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958364\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958366\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958368\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958369\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.958371\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958372\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958374\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958375\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958379\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958380\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958382\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958346\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958349\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113548, acc: 0.958350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958352\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958353\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113565, acc: 0.958354\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.958356\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958354\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958357\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958360\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958362\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958363\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.958365\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958363\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958364\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958366\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958367\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958369\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.958336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958337\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113596, acc: 0.958339\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958341\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958345\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958348\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958350\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958352\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.958353\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113554, acc: 0.958355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958358\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958359\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958361\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958363\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958364\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958363\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958365\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113548, acc: 0.958366\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958330\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958332\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958333\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958330\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958333\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958334\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958317\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958319\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958320\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958322\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958323\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958325\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958328\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958330\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958333\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958334\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958336\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958340\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958342\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113554, acc: 0.958344\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.958345\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113546, acc: 0.958347\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.958348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113537, acc: 0.958350\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113533, acc: 0.958351\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958345\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.958347\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113554, acc: 0.958348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958350\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113578, acc: 0.958349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958351\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958352\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958348\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958349\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.958351\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113628, acc: 0.958340\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958341\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958343\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958345\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958342\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958345\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958347\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958350\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958353\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958354\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958356\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958358\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958359\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958362\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958364\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958347\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958348\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958350\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958351\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.958353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958354\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113596, acc: 0.958356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958359\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958361\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958362\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958364\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958365\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958367\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958368\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.958370\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113554, acc: 0.958372\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.958373\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958375\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958376\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958374\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958371\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958373\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958375\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.958376\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958378\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.958379\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958381\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958345\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958348\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958353\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958354\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958357\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958355\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958360\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113635, acc: 0.958343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113630, acc: 0.958344\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958346\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958329\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958330\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958332\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958335\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958333\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958334\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958336\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958337\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.958339\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.958341\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113650, acc: 0.958342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.958344\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958345\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113637, acc: 0.958347\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113633, acc: 0.958348\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958351\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958355\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958358\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958341\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958344\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113628, acc: 0.958345\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958348\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958350\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958314\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958316\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958317\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958319\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958320\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958323\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958325\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958327\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958328\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958331\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958333\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958334\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958336\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958337\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958339\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958341\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958342\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958344\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958347\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113551, acc: 0.958350\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113546, acc: 0.958351\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.958353\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113538, acc: 0.958355\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.958356\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958359\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.958361\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113517, acc: 0.958362\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113513, acc: 0.958364\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113508, acc: 0.958365\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113504, acc: 0.958367\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113500, acc: 0.958369\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113496, acc: 0.958370\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.958372\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.958371\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113516, acc: 0.958372\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958355\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958357\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958358\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958360\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958361\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958363\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958364\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113519, acc: 0.958366\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.958367\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113511, acc: 0.958369\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113507, acc: 0.958371\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113502, acc: 0.958372\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113498, acc: 0.958374\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113494, acc: 0.958375\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.958377\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.958378\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958380\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113477, acc: 0.958381\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113473, acc: 0.958383\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.958385\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113464, acc: 0.958386\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113460, acc: 0.958388\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113456, acc: 0.958389\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113452, acc: 0.958391\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113447, acc: 0.958392\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113480, acc: 0.958389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113476, acc: 0.958391\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113472, acc: 0.958392\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113468, acc: 0.958394\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113464, acc: 0.958395\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113459, acc: 0.958397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113455, acc: 0.958398\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113451, acc: 0.958400\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113447, acc: 0.958402\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113442, acc: 0.958403\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113438, acc: 0.958405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113434, acc: 0.958406\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113430, acc: 0.958408\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113425, acc: 0.958409\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113421, acc: 0.958411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113417, acc: 0.958412\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113450, acc: 0.958410\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113483, acc: 0.958393\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113479, acc: 0.958394\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113475, acc: 0.958396\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113470, acc: 0.958397\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113466, acc: 0.958399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113462, acc: 0.958401\n",
      "target: tensor([0.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958328\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958329\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.958332\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.958334\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113511, acc: 0.958335\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958332\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113540, acc: 0.958334\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958335\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958318\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.958320\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.958321\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958286\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958287\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958289\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958290\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958292\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958293\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958295\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958292\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958294\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958296\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958297\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958299\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958300\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958302\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958303\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958305\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113600, acc: 0.958288\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113596, acc: 0.958289\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958291\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113587, acc: 0.958292\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958294\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958296\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958297\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958299\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958300\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958265\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958266\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958264\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958266\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958267\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.958269\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958270\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958273\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958275\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958276\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958278\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958261\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958262\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958260\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958261\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958259\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958261\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958262\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958264\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958265\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958267\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958269\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958233\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958234\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958236\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958234\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958232\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958230\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958214\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958216\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958217\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958219\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958211\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958214\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958216\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958217\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958219\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958220\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958218\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958219\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958221\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958222\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958224\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958225\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958229\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958230\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958232\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958233\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958235\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958236\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958238\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958239\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958241\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958243\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958244\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958246\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958247\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958249\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958250\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958252\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958253\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958255\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958238\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958239\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958241\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958243\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958244\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958246\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958249\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958250\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958252\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958253\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958255\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958257\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958258\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958260\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958261\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958263\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958264\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958266\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958267\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958269\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958270\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958274\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958275\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.958277\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958278\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958280\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958281\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958283\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958284\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958286\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958287\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958289\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958281\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958246\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958247\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958249\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958250\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958252\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958254\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958255\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958257\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958258\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958260\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958261\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958263\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958264\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958229\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958230\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958232\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958233\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958230\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958231\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958233\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958234\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958236\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958237\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958239\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958241\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958242\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958244\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113690, acc: 0.958245\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958247\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958248\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958250\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958251\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958253\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958254\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958256\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958258\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.958259\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958261\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958262\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958264\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958267\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958268\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958270\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958271\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958268\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958269\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.958271\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958269\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958270\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958272\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958273\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958275\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958276\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958278\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.958279\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113642, acc: 0.958281\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113638, acc: 0.958283\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958284\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958282\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.958283\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.958285\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.958286\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958282\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958284\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958287\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958289\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958290\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.958292\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958293\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958295\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.958296\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.958298\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958299\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958301\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958302\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958286\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958287\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958286\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958288\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958277\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958279\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958280\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958282\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958283\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958285\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958268\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958270\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958253\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958236\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958237\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958239\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958222\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958223\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958225\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958226\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958228\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958230\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958231\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958233\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958234\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958236\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958239\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958240\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958242\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958243\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958245\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958250\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958251\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958247\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958248\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958250\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958247\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958249\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958251\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958234\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958231\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958233\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958235\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958236\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958238\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958239\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958241\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958242\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958244\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958245\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958247\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958248\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958250\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958252\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958253\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958255\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958256\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958258\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958259\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958261\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958262\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958264\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958265\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958267\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958268\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958270\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958272\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958273\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958275\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958276\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958278\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958279\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958281\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958270\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958272\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958273\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958275\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958276\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958278\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958279\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958281\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958282\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958284\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958285\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958287\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958252\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958253\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958255\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958256\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958258\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958259\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958261\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958258\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958256\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958259\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958260\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958262\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958263\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958265\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958266\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958268\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958269\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958271\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958273\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958274\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958276\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958277\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958279\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958280\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958282\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958283\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958285\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958286\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958288\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958289\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958291\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958292\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958294\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958296\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958297\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958299\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958300\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.958302\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.958303\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958301\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958299\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958300\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958302\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958303\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958305\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958306\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958308\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958309\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958311\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958314\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958315\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958317\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958318\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958320\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.958323\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.958325\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113635, acc: 0.958326\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958328\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958317\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958318\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958320\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958322\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958305\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958306\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958309\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958311\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.958312\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.958314\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958303\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958305\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958306\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958308\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958309\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958311\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958312\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958314\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958315\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.958318\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.958320\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958322\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958323\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958325\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958322\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958324\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958325\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958315\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958316\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958319\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958321\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113668, acc: 0.958322\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113664, acc: 0.958324\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958325\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958327\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958329\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113647, acc: 0.958330\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.958332\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.958333\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113635, acc: 0.958335\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958338\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958339\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958341\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958342\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958344\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958345\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958347\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113634, acc: 0.958330\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113630, acc: 0.958332\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958335\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958336\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113613, acc: 0.958338\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113609, acc: 0.958339\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113605, acc: 0.958341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958342\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113596, acc: 0.958344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113592, acc: 0.958345\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958347\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113584, acc: 0.958348\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113580, acc: 0.958350\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958351\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958353\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113567, acc: 0.958355\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113563, acc: 0.958356\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113559, acc: 0.958358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958359\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113551, acc: 0.958361\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113546, acc: 0.958362\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113542, acc: 0.958364\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113575, acc: 0.958347\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113571, acc: 0.958348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958350\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113558, acc: 0.958353\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113554, acc: 0.958355\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113550, acc: 0.958356\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113546, acc: 0.958358\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958359\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113537, acc: 0.958361\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113533, acc: 0.958362\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958364\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958362\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958363\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958365\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958366\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958368\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113541, acc: 0.958369\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958371\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958360\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958358\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958359\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958361\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958362\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113585, acc: 0.958364\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958365\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113576, acc: 0.958367\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113572, acc: 0.958369\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958370\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958335\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113633, acc: 0.958318\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958320\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113661, acc: 0.958303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113657, acc: 0.958304\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958306\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.958308\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.958309\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958311\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958312\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958314\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113628, acc: 0.958315\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958317\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958320\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958321\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958323\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958324\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958326\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.958327\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.958329\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.958330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958332\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113578, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958335\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958336\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958338\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958340\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958341\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958343\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958344\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958344\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958345\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958342\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958344\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958346\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113589, acc: 0.958349\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958346\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958348\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113650, acc: 0.958345\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113646, acc: 0.958347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113641, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113637, acc: 0.958350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113633, acc: 0.958352\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113629, acc: 0.958353\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113625, acc: 0.958355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958356\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113653, acc: 0.958321\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113649, acc: 0.958323\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958320\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958324\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958327\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958328\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958330\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.958331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958333\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958336\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958337\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958339\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958340\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958342\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958343\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958345\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958346\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958348\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958349\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958351\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958352\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958354\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.958355\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958357\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958358\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958362\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113565, acc: 0.958363\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958366\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958368\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113548, acc: 0.958369\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958371\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113540, acc: 0.958372\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958374\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113532, acc: 0.958375\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113528, acc: 0.958377\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113524, acc: 0.958378\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113519, acc: 0.958380\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113515, acc: 0.958381\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113511, acc: 0.958383\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113507, acc: 0.958384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113503, acc: 0.958386\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113499, acc: 0.958387\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113495, acc: 0.958389\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.958390\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113486, acc: 0.958392\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113482, acc: 0.958393\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.958395\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113474, acc: 0.958396\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113470, acc: 0.958398\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113502, acc: 0.958390\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113498, acc: 0.958392\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113494, acc: 0.958393\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.958395\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113486, acc: 0.958396\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958398\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113514, acc: 0.958396\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113510, acc: 0.958397\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.958399\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958400\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113497, acc: 0.958402\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113493, acc: 0.958403\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113489, acc: 0.958405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113485, acc: 0.958406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113481, acc: 0.958408\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113513, acc: 0.958406\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113509, acc: 0.958407\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.958409\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958410\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113533, acc: 0.958408\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958410\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958411\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113520, acc: 0.958413\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113516, acc: 0.958414\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113512, acc: 0.958416\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113508, acc: 0.958417\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113504, acc: 0.958419\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113500, acc: 0.958420\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113496, acc: 0.958422\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113491, acc: 0.958423\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113487, acc: 0.958425\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113483, acc: 0.958426\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113479, acc: 0.958428\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113475, acc: 0.958429\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113471, acc: 0.958431\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113467, acc: 0.958433\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113463, acc: 0.958434\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.958436\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113454, acc: 0.958437\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113450, acc: 0.958439\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113446, acc: 0.958440\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113442, acc: 0.958442\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113438, acc: 0.958443\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113434, acc: 0.958445\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113429, acc: 0.958446\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113462, acc: 0.958444\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113458, acc: 0.958445\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113490, acc: 0.958443\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113486, acc: 0.958444\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113482, acc: 0.958446\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113478, acc: 0.958447\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113473, acc: 0.958449\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113469, acc: 0.958450\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113502, acc: 0.958433\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113534, acc: 0.958426\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113530, acc: 0.958427\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113526, acc: 0.958429\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.958430\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113517, acc: 0.958432\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113513, acc: 0.958433\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113509, acc: 0.958435\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113505, acc: 0.958436\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113501, acc: 0.958438\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113497, acc: 0.958439\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113529, acc: 0.958437\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113525, acc: 0.958439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113521, acc: 0.958440\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113516, acc: 0.958442\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958407\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113545, acc: 0.958409\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958374\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958375\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958377\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.958378\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.958380\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113556, acc: 0.958381\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113552, acc: 0.958383\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113548, acc: 0.958384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113544, acc: 0.958386\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113540, acc: 0.958387\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113536, acc: 0.958389\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113568, acc: 0.958387\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113564, acc: 0.958388\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113560, acc: 0.958390\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113555, acc: 0.958391\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958389\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958354\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958356\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958357\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113607, acc: 0.958359\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113603, acc: 0.958360\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113599, acc: 0.958362\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113595, acc: 0.958363\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113591, acc: 0.958365\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113587, acc: 0.958366\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113583, acc: 0.958368\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113579, acc: 0.958369\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958371\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113570, acc: 0.958372\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113566, acc: 0.958374\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113562, acc: 0.958375\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958341\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113590, acc: 0.958342\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113586, acc: 0.958344\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113582, acc: 0.958345\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113578, acc: 0.958347\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113574, acc: 0.958348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958350\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113565, acc: 0.958351\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113561, acc: 0.958353\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113557, acc: 0.958354\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113553, acc: 0.958356\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113549, acc: 0.958357\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113581, acc: 0.958341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113577, acc: 0.958342\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113573, acc: 0.958344\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113569, acc: 0.958345\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113601, acc: 0.958310\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113597, acc: 0.958312\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113593, acc: 0.958313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113588, acc: 0.958315\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113621, acc: 0.958298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958300\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958301\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113645, acc: 0.958299\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958300\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958302\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113632, acc: 0.958303\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113628, acc: 0.958305\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113624, acc: 0.958306\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113620, acc: 0.958308\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113616, acc: 0.958309\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113612, acc: 0.958311\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958309\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958310\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113636, acc: 0.958312\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958313\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113627, acc: 0.958315\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113623, acc: 0.958316\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113619, acc: 0.958318\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113615, acc: 0.958319\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113611, acc: 0.958321\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113643, acc: 0.958286\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113639, acc: 0.958288\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113635, acc: 0.958289\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113631, acc: 0.958291\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958292\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958294\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113618, acc: 0.958295\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113614, acc: 0.958297\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113610, acc: 0.958298\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113606, acc: 0.958300\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113602, acc: 0.958301\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113598, acc: 0.958303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113594, acc: 0.958304\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113626, acc: 0.958297\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113622, acc: 0.958298\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113617, acc: 0.958300\n",
      "target: tensor([1.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113686, acc: 0.958265\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958266\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958268\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958269\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958271\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958273\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958256\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958257\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958223\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958224\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958226\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958225\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958227\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958228\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958230\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958231\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958233\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958234\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958236\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958237\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958239\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958240\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958242\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958243\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958245\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958246\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958248\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958249\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958251\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958252\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958236\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958239\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958240\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958242\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958243\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958245\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958243\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958244\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958246\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958211\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958213\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958214\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958216\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958181\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958182\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958184\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958167\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958169\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958170\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958172\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958173\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958175\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958172\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958165\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958166\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958168\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958169\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958171\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958168\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958170\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958171\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958173\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958174\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958176\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958177\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958179\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958144\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958146\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958147\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958149\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958150\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958152\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958150\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958151\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958153\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958154\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958156\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958157\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958123\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958124\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958126\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958127\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958129\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958130\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958132\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958133\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958135\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958132\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958130\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958132\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958133\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958135\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958136\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958138\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958139\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958137\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958139\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958140\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958142\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.958143\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.958145\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113877, acc: 0.958146\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.958148\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.958149\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958151\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958152\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958154\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958155\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958157\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958158\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958160\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958158\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958159\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958161\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958162\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958164\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958165\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958167\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958168\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958170\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958171\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958173\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958174\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958176\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958177\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958179\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958180\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958182\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958183\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958185\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958186\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958188\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958189\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958191\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958193\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958194\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958196\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958179\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958181\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958182\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958184\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958185\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958187\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958188\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958190\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958191\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958193\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958190\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958192\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958193\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958195\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958198\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958200\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958201\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958203\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958186\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958188\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958189\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958191\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958192\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958194\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958195\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958179\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958180\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958182\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958183\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958185\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958186\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958188\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958189\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958191\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958192\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958194\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958159\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958125\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958126\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958128\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958129\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958131\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958132\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958116\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958117\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958119\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958084\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958087\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958089\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958087\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958088\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958090\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958091\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958093\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958094\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958096\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958097\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958099\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958064\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958066\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958067\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958069\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958070\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958072\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958073\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958075\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958076\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958078\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958079\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958081\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958082\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958084\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958087\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958086\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958088\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958089\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958091\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958074\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958077\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958079\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958080\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958082\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958083\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958085\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958086\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958088\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958089\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958091\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958092\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958094\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958059\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958061\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958062\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958064\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958065\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958067\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958068\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958070\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958071\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958073\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958074\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958076\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958077\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958079\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958080\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958082\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958083\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958085\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958086\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958088\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958090\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958091\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958093\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958094\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958096\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958097\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958099\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958100\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958102\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958103\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958105\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958106\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958108\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958109\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958111\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958112\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958110\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958111\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958113\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958096\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958094\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958096\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958079\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958077\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958075\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958078\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958079\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958081\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958082\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958084\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958085\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958087\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958088\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958090\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958091\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958093\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958094\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958096\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958097\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958099\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958091\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958093\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958094\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958096\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958097\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958099\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958082\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958084\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958085\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958087\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958088\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958090\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958091\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958093\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958094\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958096\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958097\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958099\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958100\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958102\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958103\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958105\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958106\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958108\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958109\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958111\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958112\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958110\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958112\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958113\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958115\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958116\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958118\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958119\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958121\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958122\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958124\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958121\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958122\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958124\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958125\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958127\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958128\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958130\n",
      "target: tensor([4.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958124\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958122\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958123\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958125\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958123\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958124\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958090\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958092\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113885, acc: 0.958093\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113881, acc: 0.958095\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958093\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958090\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958092\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.958057\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958059\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.958060\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.958062\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958059\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958056\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958058\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958061\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958062\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958064\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958065\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.958067\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.958068\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.958070\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.958053\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114014, acc: 0.958055\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114010, acc: 0.958056\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958058\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113997, acc: 0.958061\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113993, acc: 0.958062\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113989, acc: 0.958064\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113985, acc: 0.958065\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.958067\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.958068\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.958070\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.958071\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113965, acc: 0.958073\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.958074\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.958076\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.958077\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958079\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958080\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.958078\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.958079\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958081\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958079\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958080\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958082\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958083\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.958085\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.958086\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.958088\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958071\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.958073\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.958074\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958076\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958077\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.958079\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.958080\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958082\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114010, acc: 0.958080\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958081\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958083\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958084\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.958082\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.958084\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114021, acc: 0.958085\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958051\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958053\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958050\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958052\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958053\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958055\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114096, acc: 0.958040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.958042\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958043\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958045\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958046\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958048\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958049\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958051\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958052\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.958054\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958055\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958058\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958056\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958042\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.958039\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114130, acc: 0.958041\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114126, acc: 0.958042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.958044\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958045\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.958050\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.958051\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.958048\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.958049\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.958051\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114116, acc: 0.958054\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.958055\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958057\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958058\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958060\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114132, acc: 0.958043\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.958041\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.958042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.958044\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.958045\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114147, acc: 0.958047\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114143, acc: 0.958048\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114139, acc: 0.958050\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.958051\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958053\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114127, acc: 0.958054\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114123, acc: 0.958056\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114119, acc: 0.958057\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114115, acc: 0.958059\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114110, acc: 0.958060\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.958056\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.958040\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.958041\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.958043\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.958044\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958046\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.958047\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.958049\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.958051\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.958053\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.958054\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.958056\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.958057\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958059\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958060\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958062\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958063\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958065\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114100, acc: 0.958066\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114096, acc: 0.958068\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114092, acc: 0.958069\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958071\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958072\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958074\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.958072\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958073\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958075\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.958076\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.958078\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.958079\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.958081\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.958082\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958084\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958085\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114107, acc: 0.958069\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114103, acc: 0.958070\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114099, acc: 0.958072\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114094, acc: 0.958073\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114090, acc: 0.958075\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114122, acc: 0.958073\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114118, acc: 0.958074\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114114, acc: 0.958076\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114110, acc: 0.958077\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114106, acc: 0.958079\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114102, acc: 0.958080\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114098, acc: 0.958082\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958083\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958085\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958086\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958088\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958089\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.958091\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958092\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958094\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958095\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958097\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958098\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958100\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958096\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114112, acc: 0.958080\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114108, acc: 0.958081\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114104, acc: 0.958083\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114135, acc: 0.958067\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114131, acc: 0.958068\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114163, acc: 0.958052\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114159, acc: 0.958053\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114155, acc: 0.958055\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114151, acc: 0.958056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114146, acc: 0.958058\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114142, acc: 0.958059\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114138, acc: 0.958061\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114134, acc: 0.958062\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.958052\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114162, acc: 0.958053\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114158, acc: 0.958055\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.958038\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.958040\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.958041\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.958043\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.958044\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.958046\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.958047\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.958031\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958033\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958034\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.958036\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.958027\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957993\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957994\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957996\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957997\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957999\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958000\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958002\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958003\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958005\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958006\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958008\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.958009\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.958007\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.958008\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957992\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957993\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957995\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957996\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957999\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958001\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958002\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.958004\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.958005\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.958007\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.958010\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.958008\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.958009\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.958011\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.958012\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958014\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958015\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.958017\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.958018\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.958020\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.958021\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.958005\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.958006\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.958008\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.958009\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958011\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958012\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958014\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958015\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.958018\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.958016\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.958018\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114253, acc: 0.957984\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957985\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114281, acc: 0.957983\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114277, acc: 0.957985\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114273, acc: 0.957986\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114268, acc: 0.957988\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114300, acc: 0.957971\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114296, acc: 0.957973\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114292, acc: 0.957974\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957972\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957973\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957975\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957976\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957978\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957979\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114334, acc: 0.957945\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114330, acc: 0.957947\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114326, acc: 0.957948\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957946\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957947\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114350, acc: 0.957949\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957950\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957952\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957953\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957955\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114329, acc: 0.957956\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957954\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957956\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957957\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957959\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957959\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957960\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957962\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957963\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114391, acc: 0.957929\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114422, acc: 0.957926\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114418, acc: 0.957928\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114414, acc: 0.957929\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114410, acc: 0.957931\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114406, acc: 0.957932\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114402, acc: 0.957934\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114398, acc: 0.957935\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957937\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957938\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957940\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957941\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957943\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957944\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957946\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957947\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957949\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114358, acc: 0.957950\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114354, acc: 0.957952\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114349, acc: 0.957953\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114345, acc: 0.957955\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114341, acc: 0.957956\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114337, acc: 0.957958\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114333, acc: 0.957959\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957957\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114361, acc: 0.957958\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957960\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957961\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957963\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114380, acc: 0.957960\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114376, acc: 0.957962\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114372, acc: 0.957963\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114368, acc: 0.957965\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114364, acc: 0.957966\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957968\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957969\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957971\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957972\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957974\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114375, acc: 0.957972\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114371, acc: 0.957973\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114367, acc: 0.957975\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114363, acc: 0.957976\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114394, acc: 0.957942\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114390, acc: 0.957944\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114386, acc: 0.957945\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114382, acc: 0.957947\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114378, acc: 0.957948\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114374, acc: 0.957950\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114370, acc: 0.957951\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114366, acc: 0.957953\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114362, acc: 0.957954\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114357, acc: 0.957956\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114353, acc: 0.957957\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114385, acc: 0.957955\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114381, acc: 0.957956\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114377, acc: 0.957958\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114373, acc: 0.957959\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114369, acc: 0.957961\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114365, acc: 0.957962\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114360, acc: 0.957964\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114356, acc: 0.957965\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114352, acc: 0.957967\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114348, acc: 0.957968\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114344, acc: 0.957970\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114340, acc: 0.957971\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114336, acc: 0.957973\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114332, acc: 0.957974\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114328, acc: 0.957976\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114324, acc: 0.957977\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114355, acc: 0.957967\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114351, acc: 0.957968\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114347, acc: 0.957970\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114343, acc: 0.957971\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114339, acc: 0.957973\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114335, acc: 0.957974\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114331, acc: 0.957976\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114327, acc: 0.957977\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114323, acc: 0.957979\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114319, acc: 0.957980\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114315, acc: 0.957982\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114311, acc: 0.957983\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114307, acc: 0.957985\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114303, acc: 0.957986\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114299, acc: 0.957988\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114295, acc: 0.957989\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114291, acc: 0.957991\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114287, acc: 0.957992\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114283, acc: 0.957994\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114279, acc: 0.957995\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114275, acc: 0.957997\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114271, acc: 0.957998\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114267, acc: 0.958000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114263, acc: 0.958001\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114258, acc: 0.958003\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114254, acc: 0.958004\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114250, acc: 0.958006\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114246, acc: 0.958007\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114242, acc: 0.958009\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114238, acc: 0.958010\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114234, acc: 0.958011\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114230, acc: 0.958013\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114226, acc: 0.958014\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.958016\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.958017\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.958019\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.958020\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.958023\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.958013\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114225, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114221, acc: 0.958018\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114217, acc: 0.958019\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114213, acc: 0.958020\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114209, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114205, acc: 0.958023\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114201, acc: 0.958025\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.957991\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.957993\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.957994\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.957996\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114216, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114212, acc: 0.957999\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114208, acc: 0.958000\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114204, acc: 0.958001\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114200, acc: 0.958003\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114196, acc: 0.958004\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114192, acc: 0.958006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114188, acc: 0.958007\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114184, acc: 0.958009\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114180, acc: 0.958010\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114176, acc: 0.958012\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.957996\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.957997\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.957999\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.958000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.958002\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114222, acc: 0.958000\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114218, acc: 0.958001\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114249, acc: 0.957985\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114245, acc: 0.957986\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114241, acc: 0.957988\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114237, acc: 0.957989\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114233, acc: 0.957991\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114229, acc: 0.957992\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114261, acc: 0.957990\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114257, acc: 0.957991\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114252, acc: 0.957993\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114248, acc: 0.957994\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114244, acc: 0.957996\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114240, acc: 0.957997\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114236, acc: 0.957999\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114232, acc: 0.958000\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114228, acc: 0.958002\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114224, acc: 0.958003\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114220, acc: 0.958005\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114251, acc: 0.957989\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114247, acc: 0.957990\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114243, acc: 0.957992\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114239, acc: 0.957993\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114235, acc: 0.957994\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114231, acc: 0.957996\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114227, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114223, acc: 0.957999\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114219, acc: 0.958000\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114215, acc: 0.958002\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114211, acc: 0.958003\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114207, acc: 0.958005\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114203, acc: 0.958006\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114199, acc: 0.958008\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114195, acc: 0.958009\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114191, acc: 0.958011\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114187, acc: 0.958012\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114183, acc: 0.958014\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114214, acc: 0.957980\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114210, acc: 0.957981\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114206, acc: 0.957983\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114202, acc: 0.957984\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114198, acc: 0.957986\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114194, acc: 0.957987\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114190, acc: 0.957989\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114186, acc: 0.957990\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114182, acc: 0.957992\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114178, acc: 0.957993\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114174, acc: 0.957995\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114170, acc: 0.957996\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114166, acc: 0.957998\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114197, acc: 0.957996\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114193, acc: 0.957997\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114189, acc: 0.957999\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114185, acc: 0.958000\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114181, acc: 0.958002\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114177, acc: 0.958003\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114173, acc: 0.958005\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114169, acc: 0.958006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114165, acc: 0.958007\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114161, acc: 0.958009\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114157, acc: 0.958010\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114153, acc: 0.958012\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114149, acc: 0.958013\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114145, acc: 0.958015\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114141, acc: 0.958016\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114137, acc: 0.958018\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114133, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114129, acc: 0.958021\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114125, acc: 0.958022\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114121, acc: 0.958024\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114117, acc: 0.958025\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114113, acc: 0.958027\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114109, acc: 0.958028\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114105, acc: 0.958030\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114101, acc: 0.958031\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114097, acc: 0.958033\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114093, acc: 0.958034\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114089, acc: 0.958036\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114085, acc: 0.958037\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958039\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958040\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.958041\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958043\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958044\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958046\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958047\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958049\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958050\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958052\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958053\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.958055\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958056\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.958058\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.958059\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958057\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.958059\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.958060\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958062\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958063\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.958064\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.958066\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958067\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958069\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958070\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.958072\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958073\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958075\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958076\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958074\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958075\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958077\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958078\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958080\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958081\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958083\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958084\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.958086\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.958087\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958089\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958090\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958092\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.958093\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113979, acc: 0.958094\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113975, acc: 0.958096\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958062\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958064\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958065\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.958067\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.958068\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.958070\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.958071\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.958037\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.958039\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.958040\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114001, acc: 0.958042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113997, acc: 0.958043\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958041\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958043\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958044\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.958046\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958049\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958050\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958052\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958053\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.958055\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.958056\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958058\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.958059\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.958060\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.958062\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958063\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.958065\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958066\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958050\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.958034\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.958036\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958020\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958021\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958023\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958020\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958023\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958025\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958026\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.958028\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958030\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958032\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.958033\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114032, acc: 0.958035\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114028, acc: 0.958036\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958038\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958035\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958036\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958038\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958039\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.958041\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958042\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958044\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958045\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958047\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958048\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958032\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958034\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958035\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958036\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.958038\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.958039\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114026, acc: 0.958041\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114022, acc: 0.958042\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114018, acc: 0.958044\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958028\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958025\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958027\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.958028\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958030\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958031\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958033\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958034\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958036\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958020\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958021\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958023\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958024\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958026\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958027\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114095, acc: 0.958017\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114091, acc: 0.958018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114087, acc: 0.958020\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.958021\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958023\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958024\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958026\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958027\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958029\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.958030\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958031\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.958033\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.958034\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.958036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.958037\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958039\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958040\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958042\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958043\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.958041\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958043\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114046, acc: 0.958044\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958046\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.958049\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958015\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958016\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958018\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114088, acc: 0.958011\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114084, acc: 0.958012\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114080, acc: 0.958014\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114076, acc: 0.958015\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114072, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114068, acc: 0.958018\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114064, acc: 0.958019\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114060, acc: 0.958021\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.958022\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.958024\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114083, acc: 0.958022\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114079, acc: 0.958023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114075, acc: 0.958025\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114071, acc: 0.958026\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.958028\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.958029\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.958031\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.958032\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114086, acc: 0.958030\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114082, acc: 0.958032\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114078, acc: 0.958033\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114074, acc: 0.958035\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114070, acc: 0.958036\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114066, acc: 0.958037\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114062, acc: 0.958039\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114058, acc: 0.958040\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114054, acc: 0.958042\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114050, acc: 0.958043\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114081, acc: 0.958010\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114077, acc: 0.958011\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114073, acc: 0.958013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114069, acc: 0.958014\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114065, acc: 0.958016\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114061, acc: 0.958017\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114057, acc: 0.958019\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114053, acc: 0.958020\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114049, acc: 0.958022\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114045, acc: 0.958023\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114041, acc: 0.958024\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114037, acc: 0.958026\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114033, acc: 0.958027\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114029, acc: 0.958029\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114025, acc: 0.958030\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114056, acc: 0.957997\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114052, acc: 0.957998\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114048, acc: 0.958000\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114044, acc: 0.958001\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114040, acc: 0.958003\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114036, acc: 0.958004\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114067, acc: 0.957988\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114063, acc: 0.957990\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114059, acc: 0.957991\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114055, acc: 0.957993\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114051, acc: 0.957994\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114047, acc: 0.957996\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114043, acc: 0.957997\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114039, acc: 0.957999\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114035, acc: 0.958000\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114031, acc: 0.958001\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958003\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958004\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958006\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958007\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958009\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958010\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958012\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958013\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113996, acc: 0.958015\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113992, acc: 0.958016\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113988, acc: 0.958018\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958019\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.958020\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113972, acc: 0.958023\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113968, acc: 0.958025\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113964, acc: 0.958026\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958028\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.958029\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958031\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958032\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958034\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958035\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958037\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958038\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958040\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958041\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958042\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958044\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958045\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958047\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958048\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.958045\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.958046\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958048\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.958049\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958051\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958052\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958054\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.958055\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958056\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958054\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958056\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113961, acc: 0.958052\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113957, acc: 0.958054\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113953, acc: 0.958055\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.958057\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.958058\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.958060\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113937, acc: 0.958061\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.958063\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.958064\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.958066\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958067\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958068\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958035\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958037\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958038\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958039\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958041\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958042\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958044\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958045\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958047\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.958013\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958015\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958018\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958019\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958021\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958022\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.958020\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.958022\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.958023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958025\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958026\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.958027\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113931, acc: 0.958029\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958030\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113923, acc: 0.958032\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.958030\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958028\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958012\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114007, acc: 0.958015\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114003, acc: 0.958016\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.958018\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958019\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958021\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958022\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958024\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113980, acc: 0.958025\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113976, acc: 0.958026\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958024\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958026\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958027\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958028\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958030\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958031\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113983, acc: 0.958033\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.958000\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.958001\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114005, acc: 0.958002\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958004\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958005\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113994, acc: 0.958007\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113990, acc: 0.958008\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113986, acc: 0.958010\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.958011\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958013\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.958014\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.958016\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.958017\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.958019\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.958020\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.958021\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113950, acc: 0.958023\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113946, acc: 0.958024\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113942, acc: 0.958026\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958027\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958029\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958030\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958032\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113922, acc: 0.958033\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.958035\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.958036\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.958037\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958039\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958040\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958042\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958043\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.958045\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.958046\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958048\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958049\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.958047\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958049\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958050\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958051\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958053\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958054\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958056\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958057\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958059\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958060\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958062\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958063\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958065\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958066\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958068\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958069\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958070\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958072\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958073\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958075\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958078\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958075\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958076\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958078\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958079\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958081\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958082\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958084\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958085\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958086\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958088\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958089\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958091\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958092\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958094\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958095\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958097\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958098\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958065\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958066\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958064\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958065\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958067\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958068\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958070\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958071\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958073\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958074\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958076\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958074\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958075\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958076\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958078\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958079\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958081\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958082\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958084\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958085\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958087\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958088\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958090\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958091\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958092\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958094\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958087\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958077\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958078\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958079\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958081\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958082\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958084\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958085\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958087\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958088\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958090\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958091\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958093\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958094\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958092\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958093\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958095\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958096\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958098\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958099\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958101\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958102\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958104\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958105\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958106\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958108\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958102\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958104\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958105\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958107\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958108\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958110\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958111\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958113\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958114\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958112\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958114\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958115\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958116\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958118\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958119\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958121\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958119\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958120\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958122\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958123\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958125\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958126\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958127\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958129\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958130\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958132\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958133\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958135\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958136\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958138\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958139\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958140\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958142\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958143\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958145\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958142\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958143\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958145\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958138\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958139\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958141\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958142\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958143\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958145\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958146\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958148\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958149\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958151\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958152\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958154\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958155\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958156\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958158\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958159\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958161\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958162\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958164\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958165\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958167\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958168\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958169\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958171\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958172\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958174\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958175\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958177\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958178\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958180\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.958181\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958182\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958184\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958185\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958187\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958188\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958190\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958191\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958194\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958195\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958197\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113651, acc: 0.958198\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113682, acc: 0.958196\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113678, acc: 0.958198\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958199\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958201\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958202\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958200\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958202\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958203\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958187\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958189\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958190\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958192\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958193\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958194\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958196\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958193\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958194\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958196\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958197\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958195\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958196\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958198\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958199\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958201\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958168\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958169\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958171\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958168\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958170\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958171\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958173\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958174\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958176\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958177\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958179\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958180\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958181\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958183\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958184\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958186\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958187\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958189\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958190\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958192\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958193\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958194\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958196\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958197\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958194\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958195\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958197\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958194\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958196\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958197\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958199\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958200\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958202\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958203\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958204\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958202\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958203\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958205\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958206\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958208\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958204\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958205\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958206\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958208\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958209\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958211\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958208\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958210\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958207\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958209\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958210\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958212\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958213\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958215\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958216\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958217\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958219\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958220\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958222\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958220\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958221\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958223\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958221\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958222\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958224\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958225\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958223\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958224\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958226\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958229\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958230\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958232\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958233\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958234\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958236\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958237\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958239\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958240\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958242\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958243\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958245\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958246\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958247\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958237\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958239\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958240\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958242\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958243\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958245\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958246\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958247\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958249\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958250\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958252\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958253\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958255\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958252\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958219\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958221\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958222\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958224\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958225\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958228\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958229\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958231\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958232\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958234\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958235\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958236\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958238\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958239\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958241\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958242\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958244\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958245\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958247\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958249\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958251\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958249\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958250\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958252\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958253\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958255\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958256\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958257\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958259\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958260\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958262\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958263\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958265\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958266\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958250\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958252\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958253\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958255\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958256\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958254\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958255\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958257\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958258\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958260\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958261\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958263\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958261\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958262\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958260\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958261\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958263\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958264\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958249\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958250\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958248\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958249\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958251\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113834, acc: 0.958252\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958254\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958255\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958257\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958258\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958260\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958227\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958228\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958230\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958227\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958229\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958230\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958231\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958233\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958234\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958236\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958237\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958239\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958236\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958237\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958204\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958206\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958203\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958205\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958206\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958208\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958175\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958176\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958178\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113926, acc: 0.958179\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113922, acc: 0.958181\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.958182\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958183\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958185\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.958186\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958188\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958189\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958191\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113891, acc: 0.958192\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113887, acc: 0.958193\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958195\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958196\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958198\n",
      "target: tensor([4.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958192\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958194\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958195\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958197\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958198\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958188\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958189\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958191\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958192\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.958194\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.958195\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.958197\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113893, acc: 0.958198\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958196\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958198\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958199\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958200\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113908, acc: 0.958202\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113904, acc: 0.958203\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113900, acc: 0.958205\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113896, acc: 0.958206\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.958208\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113889, acc: 0.958209\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958207\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113915, acc: 0.958208\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113911, acc: 0.958210\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113907, acc: 0.958211\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113903, acc: 0.958213\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113899, acc: 0.958214\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113895, acc: 0.958216\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113892, acc: 0.958217\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113888, acc: 0.958218\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113918, acc: 0.958186\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113914, acc: 0.958187\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113910, acc: 0.958188\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.958186\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113937, acc: 0.958188\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113933, acc: 0.958189\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113929, acc: 0.958190\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113925, acc: 0.958192\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113921, acc: 0.958193\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113917, acc: 0.958195\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958196\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958198\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.958199\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958200\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958202\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958203\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958205\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958206\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113882, acc: 0.958208\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113878, acc: 0.958209\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113874, acc: 0.958210\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958212\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958213\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958215\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958216\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958218\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958219\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958220\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958222\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958223\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958225\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958223\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958224\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958226\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958227\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958228\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113880, acc: 0.958226\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958228\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113873, acc: 0.958229\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113869, acc: 0.958230\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113865, acc: 0.958232\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113861, acc: 0.958233\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113857, acc: 0.958235\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113853, acc: 0.958236\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958237\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958234\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113876, acc: 0.958235\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113906, acc: 0.958233\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958235\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958236\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958238\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958239\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113886, acc: 0.958241\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113883, acc: 0.958242\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113879, acc: 0.958243\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113875, acc: 0.958245\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958246\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113867, acc: 0.958248\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113863, acc: 0.958249\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958251\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958252\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958253\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958255\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958256\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958258\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958259\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958261\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958262\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958263\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958265\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958266\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958268\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958269\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958271\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958272\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958273\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958275\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958276\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958278\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958279\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958277\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958278\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958280\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958281\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958282\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958284\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958285\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958287\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958288\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958290\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958291\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958292\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958294\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958295\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958297\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958298\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958300\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958301\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958302\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958304\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958305\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958307\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958308\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958309\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958311\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958314\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958315\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113703, acc: 0.958317\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113699, acc: 0.958318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958319\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958321\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958322\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958324\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958325\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958323\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958324\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958326\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.958327\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958328\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113691, acc: 0.958330\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958331\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113683, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113679, acc: 0.958334\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113675, acc: 0.958336\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113671, acc: 0.958337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958338\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958340\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113660, acc: 0.958341\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113656, acc: 0.958343\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113652, acc: 0.958344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113648, acc: 0.958345\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113644, acc: 0.958347\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113640, acc: 0.958348\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958333\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113667, acc: 0.958334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113663, acc: 0.958336\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113659, acc: 0.958337\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113655, acc: 0.958338\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958333\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958334\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113674, acc: 0.958336\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113670, acc: 0.958337\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113666, acc: 0.958338\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113662, acc: 0.958340\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113658, acc: 0.958341\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113654, acc: 0.958343\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958341\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958342\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958343\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958341\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958339\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958341\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958342\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958344\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958345\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958347\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958348\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958351\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958348\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958350\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958351\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958353\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958354\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958356\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958357\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958358\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958360\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958361\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958363\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958356\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958357\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958358\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958360\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958361\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958363\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958364\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113701, acc: 0.958365\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113697, acc: 0.958367\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113693, acc: 0.958368\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113689, acc: 0.958370\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958371\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958373\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958357\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113707, acc: 0.958358\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958356\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958357\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958358\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113726, acc: 0.958360\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113722, acc: 0.958361\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113718, acc: 0.958363\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958364\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958331\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958333\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958334\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958336\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958337\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958339\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958340\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958341\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113714, acc: 0.958343\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958344\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958346\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958347\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113698, acc: 0.958348\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113694, acc: 0.958350\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113690, acc: 0.958351\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113687, acc: 0.958353\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958337\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958340\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113705, acc: 0.958341\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958309\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958310\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958312\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958313\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958314\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958317\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958319\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958320\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958322\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958320\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958321\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958322\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958324\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958325\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958327\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958324\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958326\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958323\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958325\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958328\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958329\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958330\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958332\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958316\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958318\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958319\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958321\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958322\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958323\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958325\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958326\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958328\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958329\n",
      "target: tensor([0.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958296\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958298\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958299\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958301\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958302\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958304\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958305\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958306\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958308\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958309\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958311\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958312\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958313\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958315\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958316\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958318\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958316\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958317\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958319\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958303\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958304\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958272\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958273\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958275\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958259\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958261\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958262\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958263\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958265\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958266\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958268\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958269\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958271\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958272\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958270\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958271\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958272\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958274\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958275\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958277\n",
      "target: tensor([2.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958267\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958268\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958270\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958271\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958272\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958274\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958275\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958277\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958278\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958280\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958281\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958282\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958284\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958282\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958283\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958285\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958283\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958284\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113813, acc: 0.958287\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958285\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958286\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958288\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958289\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958287\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958288\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958290\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958291\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958292\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958294\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958295\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958297\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958298\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958299\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958301\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958302\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958305\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958307\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958308\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958309\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958311\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958312\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958314\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958315\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958313\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958314\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958316\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958317\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958319\n",
      "target: tensor([1.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958303\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958305\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958306\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958303\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113848, acc: 0.958304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113844, acc: 0.958305\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113840, acc: 0.958307\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113870, acc: 0.958304\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113866, acc: 0.958305\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113862, acc: 0.958307\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113859, acc: 0.958308\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113855, acc: 0.958310\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113851, acc: 0.958311\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113847, acc: 0.958312\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113843, acc: 0.958314\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113839, acc: 0.958315\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113832, acc: 0.958318\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113828, acc: 0.958319\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113858, acc: 0.958287\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113854, acc: 0.958289\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113850, acc: 0.958290\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958291\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958293\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113838, acc: 0.958294\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113835, acc: 0.958296\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113831, acc: 0.958297\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113827, acc: 0.958298\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113823, acc: 0.958300\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113819, acc: 0.958301\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113815, acc: 0.958303\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113812, acc: 0.958304\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113808, acc: 0.958305\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113804, acc: 0.958307\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113800, acc: 0.958308\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958310\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958311\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958312\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958314\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958315\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958317\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958318\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958320\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958321\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958322\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958324\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958325\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958327\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958328\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958329\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958331\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958332\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958334\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958335\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958336\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958338\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958335\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958337\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958338\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958336\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113768, acc: 0.958338\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113764, acc: 0.958339\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958341\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958342\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958343\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958345\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958346\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958348\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958345\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958347\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958349\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958352\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958351\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958352\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958354\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113766, acc: 0.958355\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113762, acc: 0.958356\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958358\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958359\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958361\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958362\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113777, acc: 0.958347\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113773, acc: 0.958348\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958349\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958351\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958352\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958354\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958355\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958356\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958358\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958359\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958361\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958362\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958363\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958365\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958366\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958368\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958369\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958370\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958372\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958373\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113700, acc: 0.958375\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113696, acc: 0.958376\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958378\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958379\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113685, acc: 0.958380\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113681, acc: 0.958382\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113677, acc: 0.958383\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113673, acc: 0.958385\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958386\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958387\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958385\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113692, acc: 0.958386\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113688, acc: 0.958388\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113684, acc: 0.958389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113680, acc: 0.958391\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113676, acc: 0.958392\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113672, acc: 0.958393\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113669, acc: 0.958395\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113665, acc: 0.958396\n",
      "target: tensor([2.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113695, acc: 0.958386\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958384\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958382\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113750, acc: 0.958384\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958385\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958387\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958388\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958391\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958392\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958394\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958395\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113716, acc: 0.958396\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113712, acc: 0.958398\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958399\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113704, acc: 0.958401\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958398\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958400\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958401\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958403\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113719, acc: 0.958404\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113715, acc: 0.958405\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113711, acc: 0.958407\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113708, acc: 0.958408\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958406\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958404\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958405\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958407\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958408\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958409\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958411\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958412\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958414\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958416\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958418\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958419\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958417\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958419\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958420\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958421\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958423\n",
      "target: tensor([2.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958402\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958403\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958405\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113792, acc: 0.958406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113788, acc: 0.958407\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958409\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958410\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958412\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113772, acc: 0.958413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113769, acc: 0.958414\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113765, acc: 0.958416\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958417\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113757, acc: 0.958419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958420\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958421\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958423\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958424\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958426\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958427\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958428\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958426\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958427\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113753, acc: 0.958429\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958430\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113745, acc: 0.958432\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113741, acc: 0.958433\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113737, acc: 0.958434\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958436\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958434\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958435\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958436\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958438\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958439\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958440\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958442\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958443\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113733, acc: 0.958445\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958446\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958447\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958449\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958450\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958452\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113710, acc: 0.958453\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113706, acc: 0.958454\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113702, acc: 0.958456\n",
      "target: tensor([3.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958449\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113728, acc: 0.958450\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113724, acc: 0.958452\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958453\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113717, acc: 0.958454\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113713, acc: 0.958456\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113709, acc: 0.958457\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958455\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958456\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113731, acc: 0.958458\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113727, acc: 0.958459\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113723, acc: 0.958460\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113720, acc: 0.958462\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958430\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113746, acc: 0.958431\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113742, acc: 0.958432\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113738, acc: 0.958434\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113734, acc: 0.958435\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113730, acc: 0.958437\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113760, acc: 0.958435\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113756, acc: 0.958436\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958437\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113749, acc: 0.958439\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958437\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958438\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113771, acc: 0.958440\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958441\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113763, acc: 0.958442\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113759, acc: 0.958444\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113755, acc: 0.958445\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113752, acc: 0.958447\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113748, acc: 0.958448\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113744, acc: 0.958449\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113740, acc: 0.958451\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113736, acc: 0.958452\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958454\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113729, acc: 0.958455\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113725, acc: 0.958456\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113721, acc: 0.958458\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113751, acc: 0.958442\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113747, acc: 0.958444\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113743, acc: 0.958445\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113739, acc: 0.958447\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113735, acc: 0.958448\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113732, acc: 0.958449\n",
      "target: tensor([6.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113761, acc: 0.958446\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113758, acc: 0.958447\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113754, acc: 0.958449\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113784, acc: 0.958417\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113780, acc: 0.958418\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113776, acc: 0.958419\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958412\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958414\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958417\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958418\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958419\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958422\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113775, acc: 0.958424\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958422\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113801, acc: 0.958423\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113797, acc: 0.958424\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958426\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113790, acc: 0.958427\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113786, acc: 0.958429\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113782, acc: 0.958430\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113778, acc: 0.958431\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113774, acc: 0.958433\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113770, acc: 0.958434\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113767, acc: 0.958436\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113796, acc: 0.958434\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113793, acc: 0.958435\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113789, acc: 0.958436\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113785, acc: 0.958438\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113781, acc: 0.958439\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958407\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958408\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113803, acc: 0.958410\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958411\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958409\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958410\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958412\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958413\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958415\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958416\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958417\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958419\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113799, acc: 0.958420\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113795, acc: 0.958421\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113791, acc: 0.958423\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113787, acc: 0.958424\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113783, acc: 0.958426\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113779, acc: 0.958427\n",
      "target: tensor([6.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113809, acc: 0.958424\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113805, acc: 0.958425\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958426\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958428\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958429\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958414\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958415\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113816, acc: 0.958417\n",
      "target: tensor([1.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958401\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958403\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113871, acc: 0.958400\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113868, acc: 0.958402\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113864, acc: 0.958403\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113860, acc: 0.958405\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113856, acc: 0.958406\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113852, acc: 0.958407\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113849, acc: 0.958409\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113845, acc: 0.958410\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113841, acc: 0.958412\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113837, acc: 0.958413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958414\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113830, acc: 0.958416\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113826, acc: 0.958417\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113822, acc: 0.958418\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113818, acc: 0.958420\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958421\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113811, acc: 0.958423\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113807, acc: 0.958424\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113836, acc: 0.958422\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113833, acc: 0.958423\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113829, acc: 0.958425\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113825, acc: 0.958426\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113821, acc: 0.958428\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958429\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113814, acc: 0.958430\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113810, acc: 0.958432\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113806, acc: 0.958433\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113802, acc: 0.958435\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113798, acc: 0.958436\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113794, acc: 0.958437\n",
      "target: tensor([7.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113824, acc: 0.958435\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113820, acc: 0.958436\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113817, acc: 0.958437\n",
      "target: tensor([0.]), logits: tensor([-1.], grad_fn=<RoundBackward0>), loss: 0.113846, acc: 0.958405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113842, acc: 0.958407\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113872, acc: 0.958391\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113902, acc: 0.958389\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113898, acc: 0.958391\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.113927, acc: 0.958389\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958390\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958392\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958393\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113912, acc: 0.958394\n",
      "target: tensor([8.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113942, acc: 0.958392\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958393\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113934, acc: 0.958395\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113930, acc: 0.958396\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958381\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.958382\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958384\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113982, acc: 0.958382\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113978, acc: 0.958383\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.958385\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113971, acc: 0.958386\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.958387\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.958389\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.958390\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.958392\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958393\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958394\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958396\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113940, acc: 0.958397\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113936, acc: 0.958398\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.958397\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.958398\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113958, acc: 0.958399\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113954, acc: 0.958401\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113951, acc: 0.958402\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113947, acc: 0.958403\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113943, acc: 0.958405\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113939, acc: 0.958406\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113935, acc: 0.958408\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113932, acc: 0.958409\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113928, acc: 0.958410\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113924, acc: 0.958412\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113920, acc: 0.958413\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113916, acc: 0.958415\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113913, acc: 0.958416\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113909, acc: 0.958417\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113905, acc: 0.958419\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113901, acc: 0.958420\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113897, acc: 0.958422\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.113894, acc: 0.958423\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113890, acc: 0.958424\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113919, acc: 0.958422\n",
      "target: tensor([7.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113949, acc: 0.958420\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113945, acc: 0.958421\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113941, acc: 0.958422\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.113938, acc: 0.958424\n",
      "target: tensor([8.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113967, acc: 0.958421\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113963, acc: 0.958423\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113960, acc: 0.958424\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113956, acc: 0.958426\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113952, acc: 0.958427\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113948, acc: 0.958428\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113944, acc: 0.958430\n",
      "target: tensor([5.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113974, acc: 0.958426\n",
      "target: tensor([2.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.113970, acc: 0.958427\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.113966, acc: 0.958428\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113962, acc: 0.958430\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113959, acc: 0.958431\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113955, acc: 0.958432\n",
      "target: tensor([9.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958431\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113981, acc: 0.958432\n",
      "target: tensor([7.]), logits: tensor([7.], grad_fn=<RoundBackward0>), loss: 0.113977, acc: 0.958433\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.113973, acc: 0.958435\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113969, acc: 0.958436\n",
      "target: tensor([1.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.113999, acc: 0.958421\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.113995, acc: 0.958422\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113991, acc: 0.958424\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113987, acc: 0.958425\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.113984, acc: 0.958426\n",
      "target: tensor([9.]), logits: tensor([10.], grad_fn=<RoundBackward0>), loss: 0.114013, acc: 0.958424\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114009, acc: 0.958426\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114006, acc: 0.958427\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114002, acc: 0.958429\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.113998, acc: 0.958430\n",
      "target: tensor([5.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958426\n",
      "target: tensor([5.]), logits: tensor([5.], grad_fn=<RoundBackward0>), loss: 0.114024, acc: 0.958427\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114020, acc: 0.958429\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114016, acc: 0.958430\n",
      "target: tensor([0.]), logits: tensor([-0.], grad_fn=<RoundBackward0>), loss: 0.114012, acc: 0.958431\n",
      "target: tensor([3.]), logits: tensor([2.], grad_fn=<RoundBackward0>), loss: 0.114042, acc: 0.958424\n",
      "target: tensor([4.]), logits: tensor([4.], grad_fn=<RoundBackward0>), loss: 0.114038, acc: 0.958426\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114034, acc: 0.958427\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114030, acc: 0.958429\n",
      "target: tensor([8.]), logits: tensor([8.], grad_fn=<RoundBackward0>), loss: 0.114027, acc: 0.958430\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114023, acc: 0.958431\n",
      "target: tensor([1.]), logits: tensor([1.], grad_fn=<RoundBackward0>), loss: 0.114019, acc: 0.958433\n",
      "target: tensor([0.]), logits: tensor([0.], grad_fn=<RoundBackward0>), loss: 0.114015, acc: 0.958434\n",
      "target: tensor([6.]), logits: tensor([6.], grad_fn=<RoundBackward0>), loss: 0.114011, acc: 0.958435\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114008, acc: 0.958437\n",
      "target: tensor([3.]), logits: tensor([3.], grad_fn=<RoundBackward0>), loss: 0.114004, acc: 0.958438\n",
      "target: tensor([9.]), logits: tensor([9.], grad_fn=<RoundBackward0>), loss: 0.114000, acc: 0.958440\n"
     ]
    }
   ],
   "source": [
    "train_running_loss, train_acc = 0., 0.\n",
    "random.shuffle(dataArray)\n",
    "dataIterator = iter(dataArray)\n",
    "## training step\n",
    "for i, patient in enumerate(dataIterator):\n",
    "    labels = torch.tensor([patient.num], dtype=torch.float32)\n",
    "    # labels = torch.tensor([0 if i != patient.num else 1 for i in range(10)], dtype=torch.float32)\n",
    "\n",
    "    Pxx = patient.getTensor(foldername)\n",
    "\n",
    "    ## forward + backprop + loss\n",
    "    logits = model(Pxx)\n",
    "    logits = torch.round(logits)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    train_running_loss += loss.detach().item()\n",
    "    train_acc += get_acc_2(logits, labels)\n",
    "    print('target: %s, logits: %s, loss: %f, acc: %f' %(labels, logits, train_running_loss / (i + 1), train_acc / (i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join('.', 'models', 'audMNISTmodel', 'model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
